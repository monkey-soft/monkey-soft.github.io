{"pages":[],"posts":[{"title":"深入理解HTTP","text":"HTTP是什么 HTTP全称是HyperText Transfer Protocal，即：超文本传输协议。它主要规定了客户端和服务器之间的通信格式。HTTP还是一个基于请求/响应模式的、无状态的协议；即我们通常所说的Request/Response。 HTTP与TCP的关系TCP协议是位于TCP/IP参考模型中的网络互连层，而HTTP协议属于应用层。因此，HTTP协议是基于TCP协议。 HTTP请求(HTTP Request)HTTP请求由三部分组成，分别是： 请求行 HTTP头 请求体 下面是请求示例： 123456789GET /?tn=90058352_hao_pg HTTP/1.1Host: www.hao123.comConnection: keep-aliveCache-Control: max-age=0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Upgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/5.7.16400.16 Safari/537.36Accept-Encoding: gzip, deflateAccept-Language: zh-CN,zh;q=0.8 请求行同样也是由请求方法（POST/GET）方式、请求的主机、协议版本号三部分组成。下面为请求行的示例：GET /?tn=90058352_hao_pg HTTP/1.1 HTTP头HTTP头又细分为请求头(request header)、普通头(general header)、实体头(entity header)而HTTP头主要关注点是其字段 Accept作用: 浏览器可以接受的媒体类型例如： Accept: text/html 代表浏览器可以接受服务器回发的类型为 text/html 也就是我们常说的html文档通配符 * 代表任意类型例如： Accept: */* 代表浏览器可以处理所有类型,(一般浏览器发给服务器都是发这个) Accept-Language作用： 浏览器申明自己接收的语言。语言跟字符集的区别：中文是语言，中文有多种字符集，比如big5，gb2312，gbk等等；例如： Accept-Language: zh-CN,zh Accept-Encoding作用： 浏览器申明自己接收的编码方法，通常指定压缩方法（gzip，deflate）例如：Accept-Encoding: gzip, Accept-Encoding: deflate User-Agent作用： 告诉HTTP服务器， 客户端使用的操作系统的名称和版本以及浏览器的名称和版本.例如： User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/5.7.16400.16 Safari/537.36 Content-Type作用： 告诉服务器，请求的内容的类型常见的字段： 假设使用POST方式请求 text/xml [请求体为文本] application/json [请求体为JSON数据] application/xml [请求体为xml数据] image/jpeg [请求体为jpeg图片] multipart/form-data [请求体为表单] Cookie作用： 最重要的header，将cookie的值发送给HTTP服务器 Connection例如： Connection: keep-alive 当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接例如： Connection: close 代表一个Request完成后，客户端和服务器之间用于传输HTTP数据的TCP连接会关闭， 当客户端再次发送Request，需要重新建立TCP连接。 Content-Length作用：发送给HTTP服务器数据的长度。例如： Content-Length: 18 Referer:作用： 提供了Request的上下文信息的服务器，告诉服务器我是从哪个链接过来的。 请求体这个只有post方式请求才有，get方式请求没有。 HTTP响应(HTTP Response)HTTP Response的结构跟Request的结构基本一样。同样分为三部分： 响应行 响应头 响应体 下面是响应示例： 1234567891011HTTP/1.1 200 OKCache-Control: max-age=0Content-Encoding: gzipContent-Length: 156474Content-Type: text/html;charset=UTF-8Cxy_all: 90058352_hao_pg+d4fa7f28cefb9b120f868558e440bafaDate: Sun, 20 Nov 2016 05:09:51 GMTExpires: Sun, 20 Nov 2016 05:09:51 GMTLfy: nj02.11Server: BWS/1.0Set-Cookie: __bsi=11619936655404239050_00_60_N_R_126_0303_c02f_Y; max-age=3600; domain=www.hao123.com; path=/ 响应行响应行由协议版本、响应状态构成下面为响应行的示例：HTTP/1.1 200 OK 响应头响应头关注点是字段，常见的字段如下： Cache-Control作用: 非常重要的规则。 这个用来指定Response-Request遵循的缓存机制。例如：Cache-Control:Public 可以被任何缓存所缓存Cache-Control:Private 内容只缓存到私有缓存中Cache-Control:no-cache 所有内容都不会被缓存 Content-Type作用：服务器告诉浏览器，自己响应的对象的类型和字符集例如:Content-Type: text/html; charset=utf-8Content-Type: image/jpeg Expires作用: 浏览器会在指定过期时间内使用本地缓存例如: Expires:Sun, 20 Nov 2016 05:09:51 GMT Connection跟HTTP头中的Connection是同样的原理 Content-Encoding跟HTTP中头的Content-Encoding是同样的原理 Content-Length作用：指明实体正文的长度，以字节方式存储的十进制数字来表示。例如: Content-Length: 156474 Date作用: 生成消息的具体时间和日期例如: Date: Sun, 20 Nov 2016 05:09:51 GMT 响应体响应体包含的内容是网页的内容信息，主要是html代码等","link":"/111.html"},{"title":"常用 Python 标准库","text":"众所周知，Python有庞大的库资源，有官方标准库以及第三方的扩展库。每个库都一把利器，能帮助我们快速处理某方面的问题。作为一名python的初学者，当把基本的语法、列表和元组、字典、迭代器、异常处理、I/O操作、抽象等知识点学完之后。我建议把官方常用的标准库也随便学下来。讲真的，你知道这些库之后，你会有种相见恨晚的感觉。 接下来带大家走进python标准库的世界。PS： 使用Python的版本为Python3 字符串 re: 正则表达式。用来判断字符串是否是你指定的特定字符串。在爬虫项目中，经常能捕获到它的身影。 StringIO: 提供以文件为保存形式来读和写字符串。还有个性能更加好的cStringIO版本 struct: 以二进制字节序列来解释字符串。可以通过格式化参数，指定类型、长度、字节序（大小端）、内存对齐等。 数据类型 bisect: 数组二分算法。提供支持按顺序对列表进行排序，而不必每次在列表中插入后再去排序。 heapq: 堆队列算法。最小堆：完全平衡二叉树， 所有节点都小于字节点。 datetime: 提供操作日期和时间的类。其中有两种日期和时间类型： naive和aware collections: 高性能容器数据类型。实现了Python的通用内置容器、字典、列表、集合，和元组专门的数据类型提供替代品 pprint: 提供”整洁低打印”任意Python数据结构的能力。 数学运算 random: 各种分布的伪随机数的生成器 math: 数学函数。提供了由C标准的数学函数访问。该库的函数不适用于复数。 cmath: 为复数提供的数学函数。 operator: 提供了重载操作符 文件和目录 os.path: 常用路径名操作。提供了操作路径名的常用的函数。 filecmp: 文件和目录的比较。提供了比较文件和目录的函数。 shutil: 高级的文件操作。提供了许多文件和文件集上的操作操作。尤其是提供支持文件复制和删除的函数。 数据存储 serialization: Python专用的序列化算法，通常不建议用来存储自定义数据。 pickle: Python对象序列化。提供了一个基本但功能强大的Python对象序列化和反序列化算法。 cPickle: 比pickle快1000倍的对象序列化库， 和pickle可互相替换。 shevle: 将对象pickle序列化，然后保存到anydbm格式文件。anydbm是KV结构的数据库，可以保存多个序列化对象。 sqlite3: SQLite数据库DB-API 2.0接口。 数据压缩 zipfile: 提供了ZIP文件个创建、读取、写入、最佳和列出zip文件的函数。 tarfile: 提供了tar文件的压缩和解压的函数。 文件格式 csv: 提供对CSV文件的读取和写入的函数。 加密 hashlib: 安全哈希和消息摘要。实现了一个通用的接口来实现多个不同的安全哈希和消息摘要算法。包括 FIPS 安全哈希算法 SHA1、SHA224、SHA256、SHA384和 SHA512（定义在 FIPS 180-2），以及 RSA 的 MD5 算法（在互联网 RFC 1321中定义)。 hmac: 用于消息认证的加密哈希算法。实现了RFC 2104 中描述的HMAC 算法。 md5: 实现了MD5加密算法。 sha: 实现了sha1加密算法。 操作系统 time: 时间获取和转换。提供了各种与时间相关的函数。 argparse: 命令行选项、参数和子命令的解析器。使用该库使得编码用户友好的命令行接口非常容易。取代了之前的optparse io: 提供接口处理IO流。 logging: Python的日志工具。提供了日志记录的API。 logging.config: Python日志配置。用于配置日志模块的API。 os: 提供丰富的雨MAC，NT，Posix等操作系统进行交互的能力。这个模块允许程序独立的于操作系统环境。文件系统，用户数据库和权限进行交互。 _thread: 多线程控制。提供了一个底层、原始的操作 —— 多个控制线程共享全局数据空间。 threading: 高级线程接口。是基于_thread模块的，但是比_thread更加容易使用、更高层次的线程API。 sys: 提供访问和维护python解释器的能力。这包括了提示信息，版本，整数的最大值，可用模块，路径钩子，标准错误，标准输入输出的定位和解释器调用的命令行参数。 进程通信 subprocess: 管理子进程。允许用户产生新的进程，然后连接他们的输入/输出/错误/管道，并获取返回值。 socket: 底层网络接口。 signal: 设置异步时间处理handlers。信号是软中断，提供了一种异步事件通知机制。 网络数据处理 json: JSON格式的编码器和解码器。 base64: 提供依据RFC 3548的规定（Base16, Base32, Base64 ）进行数据编码和解码。 htmllib: 提供了一个HTML语法解析器。 mimetypes: 提供了判断给定的URL的MIME类型。 操作因特网网络协议 urllib: 提供了用于获取万维网数据的高层接口。这个是Python2.7版本的，Python3已经将其拆分成多个模块urllib.request，urllib.parse和urllib.error。 urlparse: 提供了用于处理URL的函数，可以在URL和平台特定的文件名间相互转换。 http.client: HTTP协议客户端。 telnetlib: 提供了实现Telnet协议的Telnet类。 poplib: POP3协议客户端。 ftplib: FTP协议客户端。 smtplib: SMTP协议客户端。 webbrowser: 提供控制浏览器行为的函数。","link":"/123.html"},{"title":"每逢佳节倍思亲","text":"回忆童年每逢冬至来临外面寒风凛凛屋内热气腾腾全家人围在一起吃汤圆团团圆圆 如今身为游子在外漂流更思念家更思念父母亲只能发QQ说说、微博、朋友圈给远方家人送上祝福？不！没有比给家人打一电话来得更加直接来的更加亲切拿起你手中的手机给家里打一通电话为爸妈送上节日祝福！祝：冬至快乐~","link":"/124.html"},{"title":"你为何要那么拼命？","text":"很多人往往有这样的状态，当完成一个目标之后，就守着这收获的成果沾沾自喜。你觉得考上了大学，就可以整天逃课沉迷于游戏？你觉得找到工作了，就可以准时下班走人，天天潇潇洒洒？答案是否定的。如果你是现在身处这状态，说明你对自己未来人生没有什么规划，是对自己极其不负责任的表示。你试问你自己，是否有在为自己拼命？ 01.何炅, 这个名字已经家喻户晓了。大家都知道他是大名鼎鼎的湖南电视台主持人。平时我们都在享受何老师给我们带来快乐，可知背后辛酸的汗水呢？何老师在读大学三年级时，每天需要应对高难度的阿拉伯语的学习，还担任着学生会的工作，兼职文艺部和宣传部的“要职”。除此之外，他还在央视担任支持，平日要撰写台本以及录影，有时还要出差去外地录制。每天他都很晚才回到学校，同学们可能已经下了晚自习，甚至都已经入睡了。而他只能先在学生会里将自己学生干部的事情都做完后，再回到宿舍开始预习第二天上课要准备的内容。 02.彭宇年轻的时还是街头一个小混混。当他树立人生中第一个梦想————进入电视台工作，生活从此跟之前是天壤之别。他时常对着电视机练习如何应对突发，还报名参加骗子的演员培训班。到了后来，他听说北京机会多，毅然决定北漂，每天在北京电影学院门口等着接活，做群众演员。 为何现在要拼命？只为自己，只为自己生活得更好，只为青春无悔。 你是否感到很震撼？看下你目前的生活状态，如果整天这么懒散。那么你该制定短期目标，并为之拼命一把。","link":"/15.html"},{"title":"学爬虫之道","text":"近来在阅读 《轻量级 Django》,虽然还没有读完，但我已经收益颇多。我不得不称赞 Django 框架的开发人员，他们把 Web 开发降低门槛。Django 让我从对 Web 开发是一无所知到现在可以编写小型 web 应用，这很舒服。 Django 已经算是入门，所以自己把学习目标转到爬虫。自己接下来会利用三个月的时间来专攻 Python 爬虫。这几天，我使用“主题阅读方法”阅读 Python 爬虫入门的文档。制定 Python 爬虫的学习路线。 第一阶段：夯实入门要就是在打基础，所以要从最基础的库学起。下面是几个库是入门最经典的库 urllib它属于 Python 标准库。该库的作用是请求网页并下载数据。在学习该库之前，最好把 HTTP 协议了解下。这会大大提高后面的学习效率。 先学会如何使用 urllib 请求到数据，再学习一些高级用法。例如： 设置 Headers: 某些网站反感爬虫的到访，于是对爬虫一律拒绝请求。设置 Headers 可以把请求伪装成浏览器访问网站。 Proxy 的设置: 某些站点做了反倒链的设置，会将高频繁访问的 IP 地址封掉。所以我们需要用到代理池。 错误解析：根据 URLError 与 HTTPError 返回的错误码进行解析。 Cookie 的使用：可以模拟网站登录，需要结合 cookielib 一起使用。 rere 是正则表达式库。同时也是 Python 标准库之一。它的作用是匹配我们需要爬取的内容。所以我们需要掌握正则表达式常用符号以及常用方法的用法。 BeautifulSoupBeautifulSoup 是解析网页的一款神器。它可以从 HTML 或者 XML 文件中提取数据。配合 urllib 可以编写出各种小巧精干的爬虫脚本。 第二阶段：进阶当把基础打牢固之后，我们需要更进一步学习。使用更加完善的库来提高爬取效率 使用多线程使用多线程抓取数据，提高爬取数据效率。 学习 RequestsRequests 作为 urlilb 的替代品。它是更加人性化、更加成熟的第三方库。使用 Requests 来处理各种类型的请求，重复抓取问题、cookies 跟随问题、多线程多进程、多节点抓取、抓取调度、资源压缩等一系列问题。 学习 XpathXpath 也算是一款神器。它是一款高效的、表达清晰简单的分析语言。掌握它以后介意弃用正则表达式了。一般是使用浏览器的开发者工具 加 lxml 库。 学习 Selenium使用 Selenium，模拟浏览器提交类似用户的操作，处理js动态产生的网页。因为一些网站的数据是动态加载的。类似这样的网站，当你使用鼠标往下滚动时，会自动加载新的网站。 第三阶段：突破学习 ScrapyScrapy 是一个功能非常强大的分布式爬虫框架。我们学会它，就可以不用重复造轮子。 数据存储如果爬取的数据条数较多，我们可以考虑将其存储到数据库中。因此，我们需要学会 MySqlMongoDB、SqlLite的用法。更加深入的，可以学习数据库的查询优化。 第四阶段：为我所用当爬虫完成工作，我们已经拿到数据。我们可以利用这些数据做数据分析、数据可视化、做创业项目原始启动数据等。我们可以学习 NumPy、Pandas、 Matplotlib 这三个库。 NumPy ：它是高性能科学计算和数据分析的基础包。 Pandas : 基于 NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。它可以算得上作弊工具。 Matplotlib：Python中最著名的绘图系统Python中最著名的绘图系统。它可以制作出散点图，折线图，条形图，直方图，饼状图，箱形图散点图，折线图，条形图，直方图，饼状图，箱形图等。","link":"/62.html"},{"title":"详解 python3 urllib","text":"本文是爬虫系列文章的第一篇，主要讲解 Python 3 中的 urllib 库的用法。urllib 是 Python 标准库中用于网络请求的库。该库有四个模块，分别是urllib.request，urllib.error，urllib.parse，urllib.robotparser。其中urllib.request，urllib.error两个库在爬虫程序中应用比较频繁。那我们就开门见山，直接讲解这两个模块的用法。 发起请求模拟浏览器发起一个 HTTP 请求，我们需要用到 urllib.request 模块。urllib.request 的作用不仅仅是发起请求， 还能获取请求返回结果。发起请求，单靠 urlopen() 方法就可以叱咤风云。我们先看下 urlopen() 的 API 1urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) 第一个参数 String 类型的地址或者 data 是 bytes 类型的内容，可通过 bytes()函数转为化字节流。它也是可选参数。使用 data 参数，请求方式变成以 POST 方式提交表单。使用标准格式是application/x-www-form-urlencoded timeout 参数是用于设置请求超时时间。单位是秒。 cafile和capath代表 CA 证书和 CA 证书的路径。如果使用HTTPS则需要用到。 context参数必须是ssl.SSLContext类型，用来指定SSL设置 cadefault参数已经被弃用，可以不用管了。 该方法也可以单独传入urllib.request.Request对象 该函数返回结果是一个http.client.HTTPResponse对象。 简单抓取网页我们使用 urllib.request.urlopen() 去请求百度贴吧，并获取到它页面的源代码。 123456import urllib.requesturl = \"http://tieba.baidu.com\"response = urllib.request.urlopen(url)html = response.read() # 获取到页面的源代码print(html.decode('utf-8')) # 转化为 utf-8 编码 设置请求超时有些请求可能因为网络原因无法得到响应。因此，我们可以手动设置超时时间。当请求超时，我们可以采取进一步措施，例如选择直接丢弃该请求或者再请求一次。 12345import urllib.requesturl = \"http://tieba.baidu.com\"response = urllib.request.urlopen(url, timeout=1)print(response.read().decode('utf-8')) 使用 data 参数提交数据在请求某些网页时需要携带一些数据，我们就需要使用到 data 参数。 123456789101112import urilib.parseimport urllib.requesturl = \"http://127.0.0.1:8000/book\"params = { 'name':'浮生六记', 'author':'沈复'}data = bytes(urllib.parse.urlencode(params), encoding='utf8')response = urllib.request.urlopen(url, data=data)print(response.read().decode('utf-8')) params 需要被转码成字节流。而 params 是一个字典。我们需要使用 urllib.parse.urlencode() 将字典转化为字符串。再使用 bytes() 转为字节流。最后使用 urlopen() 发起请求，请求是模拟用 POST 方式提交表单数据。 使用 Request由上我们知道利用 urlopen() 方法可以发起简单的请求。但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入headers（请求头）、指定请求方式等信息，我们就可以利用更强大的Request类来构建一个请求。按照国际惯例，先看下 Request 的构造方法： 1urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None) url 参数是请求链接，这个是必传参数，其他的都是可选参数。 data 参数跟 urlopen() 中的 data 参数用法相同。 headers 参数是指定发起的 HTTP 请求的头部信息。headers 是一个字典。它除了在 Request 中添加，还可以通过调用 Reques t实例的 add_header() 方法来添加请求头。 origin_req_host 参数指的是请求方的 host 名称或者 IP 地址。 unverifiable 参数表示这个请求是否是无法验证的，默认值是False。意思就是说用户没有足够权限来选择接收这个请求的结果。例如我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，我们就要将 unverifiable 的值设置成 True。 method 参数指的是发起的 HTTP 请求的方式，有 GET、POST、DELETE、PUT等 简单使用 Request使用 Request 伪装成浏览器发起 HTTP 请求。如果不设置 headers 中的 User-Agent，默认的User-Agent是Python-urllib/3.5。可能一些网站会将该请求拦截，所以需要伪装成浏览器发起请求。我使用的 User-Agent 是 Chrome 浏览器。 123456789import urllib.requesturl = \"http://tieba.baidu.com/\"headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}request = urllib.request.Request(url=url, headers=headers)response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) Request 高级用法如果我们需要在请求中添加代理、处理请求的 Cookies，我们需要用到Handler和OpenerDirector。 1） HandlerHandler 的中文意思是处理者、处理器。 Handler 能处理请求（HTTP、HTTPS、FTP等）中的各种事情。它的具体实现是这个类 urllib.request.BaseHandler。它是所有的 Handler 的基类，其提供了最基本的Handler的方法，例如default_open()、protocol_request()等。继承 BaseHandler 有很多个，我就列举几个比较常见的类： ProxyHandler：为请求设置代理 HTTPCookieProcessor：处理 HTTP 请求中的 Cookies HTTPDefaultErrorHandler：处理 HTTP 响应错误。 HTTPRedirectHandler：处理 HTTP 重定向。 HTTPPasswordMgr：用于管理密码，它维护了用户名密码的表。 HTTPBasicAuthHandler：用于登录认证，一般和 HTTPPasswordMgr 结合使用。 2） OpenerDirector对于 OpenerDirector，我们可以称之为 Opener。我们之前用过 urlopen() 这个方法，实际上它就是 urllib 为我们提供的一个Opener。那 Opener 和 Handler 又有什么关系？opener 对象是由 build_opener(handler) 方法来创建出来 。我们需要创建自定义的 opener，就需要使用 install_opener(opener)方法。值得注意的是，install_opener 实例化会得到一个全局的 OpenerDirector 对象。 使用代理我们已经了解了 opener 和 handler，接下来我们就通过示例来深入学习。第一个例子是为 HTTP 请求设置代理有些网站做了浏览频率限制。如果我们请求该网站频率过高。该网站会被封 IP，禁止我们的访问。所以我们需要使用代理来突破这“枷锁”。 1234567891011121314151617import urllib.requesturl = \"http://tieba.baidu.com/\"headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}proxy_handler = urllib.request.ProxyHandler({ 'http': 'web-proxy.oa.com:8080', 'https': 'web-proxy.oa.com:8080'})opener = urllib.request.build_opener(proxy_handler)urllib.request.install_opener(opener)request = urllib.request.Request(url=url, headers=headers)response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) 认证登录有些网站需要携带账号和密码进行登录之后才能继续浏览网页。碰到这样的网站，我们需要用到认证登录。我们首先需要使用 HTTPPasswordMgrWithDefaultRealm() 实例化一个账号密码管理对象；然后使用 add_password() 函数添加账号和密码；接着使用 HTTPBasicAuthHandler() 得到 hander；再使用 build_opener() 获取 opener 对象；最后使用 opener 的 open() 函数发起请求。 第二个例子是携带账号和密码请求登录百度贴吧，代码如下： 123456789101112import urllib.requesturl = \"http://tieba.baidu.com/\"user = 'user'password = 'password'pwdmgr = urllib.request.HTTPPasswordMgrWithDefaultRealm()pwdmgr.add_password(None，url ，user ，password)auth_handler = urllib.request.HTTPBasicAuthHandler(pwdmgr)opener = urllib.request.build_opener(auth_handler)response = opener.open(url)print(response.read().decode('utf-8')) Cookies设置如果请求的页面每次需要身份验证，我们可以使用 Cookies 来自动登录，免去重复登录验证的操作。获取 Cookies 需要使用 http.cookiejar.CookieJar() 实例化一个 Cookies 对象。再用 urllib.request.HTTPCookieProcessor 构建出 handler 对象。最后使用 opener 的 open() 函数即可。 第三个例子是获取请求百度贴吧的 Cookies 并保存到文件中，代码如下： 123456789101112131415import http.cookiejarimport urllib.requesturl = \"http://tieba.baidu.com/\"fileName = 'cookie.txt'cookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open(url)f = open(fileName,'a')for item in cookie: f.write(item.name+\" = \"+item.value+'\\n')f.close() HTTPResponse从上面的例子可知， 使用 urllib.request.urlopen() 或者 opener.open(url) 返回结果是一个 http.client.HTTPResponse 对象。它具有 msg、version、status、reason、debuglevel、closed等属性以及read()、readinto()、getheader(name)、getheaders()、fileno()等函数。 错误解析发起请求难免会出现各种异常，我们需要对异常进行处理，这样会使得程序比较人性化。异常处理主要用到两个类，urllib.error.URLError和urllib.error.HTTPError。 URLErrorURLError 是 urllib.error 异常类的基类, 可以捕获由urllib.request 产生的异常。它具有一个属性reason，即返回错误的原因。 捕获 URL 异常的示例代码： 12345678import urllib.requestimport urllib.errorurl = \"http://www.google.com\"try: response = request.urlopen(url)except error.URLError as e: print(e.reason) HTTPError HTTPError 是 UEKRrror 的子类，专门处理 HTTP 和 HTTPS 请求的错误。它具有三个属性。 1)code：HTTP 请求返回的状态码。 1)renson：与父类用法一样，表示返回错误的原因。 1)headers`：HTTP 请求返回的响应头信息。 获取 HTTP 异常的示例代码, 输出了错误状态码、错误原因、服务器响应头 12345678910import urllib.requestimport urllib.errorurl = \"http://www.google.com\"try: response = request.urlopen(url)except error.HTTPError as e: print('code: ' + e.code + '\\n') print('reason: ' + e.reason + '\\n') print('headers: ' + e.headers + '\\n')","link":"/66.html"},{"title":"Python 正则表达式","text":"我们能够使用 urllib 向网页请求并获取其网页数据。但是抓取信息数据量比较大，我们可能需要其中一小部分数据。对付刚才的难题，就需要正则表达式出马了。正则表达式能帮助我们匹配过滤到我们需要的数据，但它学习起来非常枯燥无味。你可能会说，我还没有开始想学习正则表达式，你就来打击我？ 莫慌！层层递进地学习，一步一个脚印地学习，很快就会学会了。对于爬虫，我觉得学会最基本的符号就差不多了。 正则表达式下面是一张关于正则表达式字符的图，图片资料来自CSDN。先把图中字符了解清楚，基本上算是入门。 re 模块Python 自 1.5 版本起通过新增 re （Regular Expression 正则表达式）模块来提供对正则表达式的支持。使用 re 模块先将正则表达式填充到 Pattern 对象中，再把 Pattern 对象作为参数使用 match 方法去匹配的字符串文本。match 方法会返回一个 Match 对象，再通过 Match 对象会得到我们的信息并进行操作。下面介绍几个 re 常用的函数。 compile 函数compile 是把正则表达式的模式和标识转化成正则表达式对象，供 match() 和 search() 这两个函数使用。它的函数语法如下： 1re.compile(pattern[, flags]) 第一个参数是pattern，指的正则表达式。 第二个参数flags是匹配模式，是个可选参数。可以使用按位或’|’表示同时生效，也可以在正则表达式字符串中指定。匹配模式有以下几种： flag 描述 re.I(全拼：IGNORECASE) 忽略大小写（括号内是完整写法，下同） re.M(全拼：MULTILINE) 多行模式，改变’^’和’$’的行为（参见上图） re.S(全拼：DOTALL) 点任意匹配模式，改变’.’的行为 re.L(全拼：LOCALE) 使预定字符类 \\w \\W \\b \\B \\s \\S 取决于当前区域设定 re.U(全拼：UNICODE) 使预定字符类 \\w \\W \\b \\B \\s \\S \\d \\D 取决于unicode定义的字符属性 re.X(全拼：VERBOSE) 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。 该方法返回的结果是一个 Pattern 对象。 match 函数match()函数只在字符串的开始位置尝试匹配正则表达式，也就是说只有在 0 位置匹配成功的话才有返回。如果不是开始位置匹配成功的话，match() 就返回 none。它的函数语法如下： 1re.match(pattern, string[, flags]) 第一个参数：匹配的正则表达式 第二个参数：要被匹配的字符串 flags 是可选参数，跟 compile 用法相似 匹配成功 re.match 方法返回一个匹配的对象，否则返回None。要想获得匹配结果，既可以使用groups()函数获取一个包含所有字符串的元组（从 1 到 所含的小组号），也可以使用group(组号)函数获取某个组号的字符串。 match 函数用法的示例代码： 123456789import repattern = re.compile('Python')text = 'Python python pythonn'match = re.search(pattern, text)if match: print(match.group())else: print('没有匹配') search 函数 search() 函数是扫描整个字符串来查找匹配，它返回结果是第一个成功匹配的字符串。 1re.search(pattern, string[, flags]) 参数用法以及返回结果跟match函数用法相同。 search 函数用法的示例代码： 123456789import repattern = re.compile('Python')text = 'welcome to Python world!'match = re.search(pattern, text)if match: print(match.group())else: print('没有匹配') findall 函数findall函数在字符串中搜索子串，并以列表形式返回全部能匹配的所有子串。 1re.findall(pattern, string[, flags]) 参数用法以及返回结果跟match函数用法相同。 findall 函数用法的示例代码： 123456789import repattern = re.compile('\\d+')text = 'one1two2three3four4'list = re.findall(pattern, text)if list: print(list)else: print('没有匹配')","link":"/67.html"},{"title":"内容提取神器 beautifulSoup 的用法","text":"上篇文章只是简单讲述正则表达式如何读懂以及 re 常见的函数的用法。我们可能读懂别人的正则表达式，但是要自己写起正则表达式的话，可能会陷入如何写的困境。正则表达式写起来费劲又出错率高，那么有没有替代方案呢？俗话说得好，条条道路通罗马。目前还两种代替其的办法，一种是使用 Xpath 神器，另一种就是本文要讲的 BeautifulSoup。 BeautifulSoup 简介引用 BeautifulSoup 官网的说明： Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work. 大致意思如下: BeautifulSoup 是一个能从 HTML 或 XML 文件中提取数据的 Python 库。它能通过自己定义的解析器来提供导航、搜索，甚至改变解析树。它的出现，会大大节省开发者的时间。 安装 BeautifulSoup目前 BeautifulSoup 最新版本是 4.6.0，它是支持 Python3的。所以可以大胆去升级安装使用。 安装方法有两种： 使用pip比较推荐使用这种方式，既简单又方便管理。 1234pip install beautifulsoup4# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install beautifulsoup4 使用easy_install 1easy_install beautifulsoup4 使用系统包管理 12sudo apt-get install Python-bs4# 适用于 ubuntu 系统以及 Debian 系统 初始 BeautifulSoup首先导入 BeautifulSoup 库，然后创建一个 BeautifulSoup 对象，再利用对象做文章。具体参考示例代码： 1234from bs4 import BeautifulSoupsoup = BeautifulSoup(response)print(soup.prettify()) 上面代码中，response 可以urlllib或者request请求返回的内容，也可以是本地 HTML 文本。如果要打开本地，代码需要改为 12soup = BeautifulSoup(open(\"index.html\"))# 打开当前目录下 index.html 文件 soup.prettify()函数的作用是打印整个 html 文件的 dom 树，例如上面执行结果如下： 123456789101112&lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse's story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt; &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt; &lt;/body&gt;&lt;/html&gt; 解析 BeautifulSoup 对象想从 html 中获取到自己所想要的内容，我归纳出三种办法： 利用 Tag 对象从上文得知，BeautifulSoup 将复杂 HTML 文档转换成一个复杂的树形结构,每个节点都是Python对象。跟安卓中的Gson库有异曲同工之妙。节点对象可以分为 4 种：Tag, NavigableString, BeautifulSoup, Comment。 Tag 对象可以看成 HTML 中的标签。这样说，你大概明白具体是怎么回事。我们再通过例子来更加深入了解 Tag 对象。以下代码是以 prettify() 打印的结果为前提。 例子1 获取head标签内容 123print(soup.head)# 输出结果如下：&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt; 例子2 获取title标签内容 123print(soup.title)# 输出结果如下：&lt;title&gt;The Dormouse's story&lt;/title&gt; 例子3 获取p标签内容 123print(soup.p)# 输出结果如下：&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; 如果 Tag 对象要获取的标签有多个的话，它只会返回所以内容中第一个符合要求的标签。 对象一般含有属性，Tag 对象也不例外。它具有两个非常重要的属性， name 和 attrs。 namename 属性是 Tag 对象的标签名。不过也有特殊的，soup 对象的 name 是 [document] 12345print(soup.name)print(soup.head.name)# 输出结果如下：[document]head attrsattrs 属性是 Tag 对象所包含的属性值，它是一个字典类型。 123print(soup.p.attrs）# 输出结果如下：{'class': ['title'], 'name': 'dromouse'} 其他三个属性也顺带介绍下: NavigableString 说白了就是：Tag 对象里面的内容 123print(soup.title.string) # 输出结果如下：The Dormouse's story BeautifulSoup BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象。它是一个特殊的 Tag。 1234567print(type(soup.name))print(soup.name)print(soup.attrs)# 输出结果如下：&lt;type 'unicode'&gt;[document]{} 空字典 Comment Comment 对象是一个特殊类型的 NavigableString 对象。如果 HTML 页面中含有注释及特殊字符串的内容。而那些内容不是我们想要的，所以我们在使用前最好做下类型判断。例如： 12if type(soup.a.string) == bs4.element.Comment: ... # 执行其他操作，例如打印内容 利用过滤器过滤器其实是一个find_all()函数， 它会将所有符合条件的内容以列表形式返回。它的构造方法如下： 1find_all(name, attrs, recursive, text, **kwargs ) name 参数可以有多种写法： （1）节点名 123print(soup.find_all('p'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] （2）正则表达式 123print(soup.find_all(re.compile('^p')))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] （3）列表如果参数为列表，过滤标准为列表中的所有元素。看下具体代码，你就会一目了然了。 123print(soup.find_all(['p', 'a']))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;, &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 另外 attrs 参数可以也作为过滤条件来获取内容，而 limit 参数是限制返回的条数。 利用 CSS 选择器以 CSS 语法为匹配标准找到 Tag。同样也是使用到一个函数，该函数为select()，返回类型也是 list。它的具体用法如下, 同样以 prettify() 打印的结果为前提： （1）通过 tag 标签查找 123print(soup.select(head))# 输出结果如下：[&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;] （2）通过 id 查找 123print(soup.select('#link1'))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] （3）通过 class 查找 123print(soup.select('.sister'))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] （4）通过属性查找 123print(soup.select('p[name=dromouse]'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;] 123print(soup.select('p[class=title]'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;] （5）组合查找 1234print(soup.select(\"body p\"))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;,&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] 123print(soup.select(\"p &gt; a\"))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 123print(soup.select(\"p &gt; .sister\"))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 处理上下关系从上文可知，我们已经能获取到节点对象，但有时候需要获取其父节点或者子节点的内容，我们要怎么做了？这就需要对parse tree进行遍历 （1）获取子节点利用.children属性，该属性会返回当前节点所以的子节点。但是它返回的类型不是列表，而是迭代器 （2）获取所有子孙节点使用.descendants属性，它会返回所有子孙节点的迭代器 （3）获取父节点通过.parent属性可以获得所有子孙节点的迭代器 （4）获取所有父节点.parents属性，也是返回所有子孙节点的迭代器 （5）获取兄弟节点兄弟节点可以理解为和本节点处在统一级的节点，.next_sibling属性获取了该节点的下一个兄弟节点，.previous_sibling则与之相反，如果节点不存在，则返回 None 注意：实际 HTML 中的 tag 的.next_sibling和 .previous_sibling属性通常是字符串或空白，因为空白或者换行也可以被视作一个节点，所以得到的结果可能是空白或者换行 （5）获取所有兄弟节点通过.next_siblings和.previous_siblings属性可以对当前节点的兄弟节点迭代输出","link":"/78.html"},{"title":"爬虫实战一：爬取当当网所有 Python 书籍","text":"我们已经学习 urllib、re、BeautifulSoup 这三个库的用法。但只是停留在理论层面上，还需实践来检验学习成果。因此，本文主要讲解如何利用我们刚才的几个库去实战。 确定爬取目标任何网站皆可爬取，就看你要不要爬取而已。本次选取的爬取目标是当当网，爬取内容是 以 Python 为关键字搜索出来的页面中所有书籍的信息。具体如下图所示： 本次爬取结果有三项： 图书的封面图片 图书的书名 图书的链接页面最后把这三项内容保存到 csv 文件中。 爬取过程总所周知，每个站点的页面 DOM 树是不一样的。所以我们需要先对爬取页面进行分析，再确定自己要获取的内容，再定义程序爬取内容的规则。 确定 URL 地址我们可以通过利用浏览器来确定URL 地址，为 urllib 发起请求提供入口地址。接下来，我们就一步步来确定请求地址。搜索结果页面为 1 时，URL 地址如下： 搜索结果页面为 3 时，URL 地址如下： 搜索结果页面为 21 时，即最后一页，URL 地址如下： 从上面的图片中，我们发现 URL 地址的差异就在于 page_index 的值，所以 URL 地址最终为 http://search.dangdang.com/?key=python&amp;act=input&amp;show=big&amp;page_index=。而 page_index 的值，我们可以通过循环依次在地址后面添加。因此， urllib 请求代码可以这样写： 123456789101112# 爬取地址, 当当所有 Python 的书籍, 一共是 21 页url = \"http://search.dangdang.com/?key=python&amp;act=input&amp;show=big&amp;page_index=\"index = 1while index &lt;= 21: # 发起请求 request = urllib.request.Request(url=url+str(index), headers=headers) response = urllib.request.urlopen(request) index = index + 1 # 解析爬取内容 parseContent(response) time.sleep(1) # 休眠1秒 确定爬取节点有了 URL 地址，就能使用 urllib 获取到页面的 html 内容。到了这步，我们就需要找到爬取的节点的规则，以便于 BeautifulSoup 地解析。为了搞定这个问题，就要祭出大招 —— Chrome 浏览器的开发者功能（按下 F12 键就能启动）。我们按下 F12 键盘，依次对每本书进行元素检查（在页面使用鼠标右键，点击“检查”即可），具体结果如下： 从上图可以得知解析规则：每本书的节点是一个 a 标签，a 标签具有 title，href，子标签 img 的 src 三个属性，这三者分别对应书名、书的链接页面、书的封图。看到这里也需你不会小激动，感叹这不就是我们要感兴趣的内容吗？得到解析规则，编写BeautifulSoup 解析代码就有了思路，具体代码如下： 1234567891011121314151617# 提取爬取内容中的 a 标签, 例如：# &lt;a# class=\"pic\" dd_name=\"单品图片\"# ddclick=\"act=normalResult_picture&amp;amp;pos=23648843_53_2_q\"# href=\"http://product.dangdang.com/23648843.html\"# name=\"itemlist-picture\"# target=\"_blank\" title=\"# 趣学Python――教孩子学编程 \"&gt;## &lt;img# alt=\" 趣学Python――教孩子学编程 \"# data-original=\"http://img3x3.ddimg.cn/20/34/23648843-1_b_0.jpg\"# src=\"images/model/guan/url_none.png\"/&gt;# &lt;/a&gt;soup = BeautifulSoup(response)books = soup.find_all('a', class_='pic')print(books) 运行结果如下： 这证明刚才制定规则是正确爬取我们所需的内容。 保存爬取信息我写爬虫程序有个习惯，就是每次都会爬取内容持久化到文件中。这样方便以后查看使用。如果爬取数据量比较大，我们可以用其做数据分析。我这里为了方便，就将数据保存到 csv 文件中。用 Python 将数据写到文件中，我们经常中文乱码问题所烦恼。如果单纯使用 csv 库，可能摆脱不了这烦恼。所以我们将 csv 和 codecs 结合一起使用。在写数据到 csv 文件的时候，我们可以通过指定文件编码。这样中文乱码问题就迎刃而解。具体代码如下: 12345678910111213141516171819fileName = 'PythonBook.csv'# 指定编码为 utf-8, 避免写 csv 文件出现中文乱码with codecs.open(fileName, 'w', 'utf-8') as csvfile: filednames = ['书名', '页面地址', '图片地址'] writer = csv.DictWriter(csvfile, fieldnames=filednames) writer.writeheader() for book in books: # print(book) # print(book.attrs) # 获取子节点&lt;img&gt; # (book.children)[0] if len(list(book.children)[0].attrs) == 3: img = list(book.children)[0].attrs['data-original'] else: img = list(book.children)[0].attrs['src'] writer.writerow({'书名': book.attrs['title'], '页面地址': book.attrs['href'], '图片地址': img}) 看到这里，你可能会问为什么不把编码指定为 gb2312 呢，这样用 ecxel 打开就不会乱码了？原因是当书名全部为英文单词时，使用 gb2312 编码，writer.writerow()会出现编码错误的问题。 如果你要用 excel 打开 PythonBook.csv文件, 你则需多执行下面几步： 1) 打开 Excel 2) 执行“数据”-&gt;“自文本” 3) 选择 CSV 文件，出现文本导入向导 4) 选择“分隔符号”，下一步 5) 勾选“逗号”，去掉“ Tab 键”，下一步，完成 6）在“导入数据”对话框里，直接点确定 爬取结果最后，我们将上面代码整合起来即可。这里就不把代码贴出来了，具体阅读原文即可查看源代码。我就把爬取结果截下图： 写在最后这次实战算是结束了，但是我们不能简单地满足，看下程序是否有优化的地方。我把该程序不足的地方写出来。 该程序是单线程，没有使用多线程，执行效率不够高。 没有应用面向对象编程思想，程序的可扩展性不高。 没有使用随机 User-Agent 和 代理，容易被封 IP。 如果你想获取网站的源码, 点击☞☞☞Github 仓库地址","link":"/79.html"},{"title":"Python 多进程与多线程","text":"前言：为什么有人说 Python 的多线程是鸡肋，不是真正意义上的多线程？ 看到这里，也许你会疑惑。这很正常，所以让我们带着问题来阅读本文章吧。 问题：1、Python 多线程为什么耗时更长？2、为什么在 Python 里面推荐使用多进程而不是多线程？ 基础知识现在的 PC 都是多核的，使用多线程能充分利用 CPU 来提供程序的执行效率。 线程线程是一个基本的 CPU 执行单元。它必须依托于进程存活。一个线程是一个execution context（执行上下文），即一个 CPU 执行时所需要的一串指令。 进程进程是指一个程序在给定数据集合上的一次执行过程，是系统进行资源分配和运行调用的独立单位。可以简单地理解为操作系统中正在执行的程序。也就说，每个应用程序都有一个自己的进程。 每一个进程启动时都会最先产生一个线程，即主线程。然后主线程会再创建其他的子线程。 两者的区别 线程必须在某个进行中执行。 一个进程可包含多个线程，其中有且只有一个主线程。 多线程共享同个地址空间、打开的文件以及其他资源。 多进程共享物理内存、磁盘、打印机以及其他资源。 线程的类型线程的因作用可以划分为不同的类型。大致可分为： 主线程 子线程 守护线程（后台线程） 前台线程 Python 多线程GIL其他语言，CPU 是多核时是支持多个线程同时执行。但在 Python 中，无论是单核还是多核，同时只能由一个线程在执行。其根源是 GIL 的存在。 GIL 的全称是 Global Interpreter Lock(全局解释器锁)，来源是 Python 设计之初的考虑，为了数据安全所做的决定。某个线程想要执行，必须先拿到 GIL，我们可以把 GIL 看作是“通行证”，并且在一个 Python 进程中，GIL 只有一个。拿不到通行证的线程，就不允许进入 CPU 执行。 而目前 Python 的解释器有多种，例如： CPython：CPython 是用C语言实现的 Python 解释器。 作为官方实现，它是最广泛使用的 Python 解释器。 PyPy：PyPy 是用RPython实现的解释器。RPython 是 Python 的子集， 具有静态类型。这个解释器的特点是即时编译，支持多重后端（C, CLI, JVM）。PyPy 旨在提高性能，同时保持最大兼容性（参考 CPython 的实现）。 Jython：Jython 是一个将 Python 代码编译成 Java 字节码的实现，运行在JVM (Java Virtual Machine) 上。另外，它可以像是用 Python 模块一样，导入 并使用任何Java类。 IronPython：IronPython 是一个针对 .NET 框架的 Python 实现。它 可以用 Python 和 .NET framewor k的库，也能将 Python 代码暴露给 .NET 框架中的其他语言。 GIL 只在 CPython 中才有，而在 PyPy 和 Jython 中是没有 GIL 的。 每次释放 GIL锁，线程进行锁竞争、切换线程，会消耗资源。这就导致打印线程执行时长，会发现耗时更长的原因。 并且由于 GIL 锁存在，Python 里一个进程永远只能同时执行一个线程(拿到 GIL 的线程才能执行)，这就是为什么在多核CPU上，Python 的多线程效率并不高的根本原因。 创建多线程Python提供两个模块进行多线程的操作，分别是thread和threading，前者是比较低级的模块，用于更底层的操作，一般应用级别的开发不常用。 方法1：直接使用threading.Thread() 1234567891011import threading# 这个函数名可随便定义def run(n): print(\"current task：\", n)if __name__ == \"__main__\": t1 = threading.Thread(target=run, args=(\"thread 1\",)) t2 = threading.Thread(target=run, args=(\"thread 2\",)) t1.start() t2.start() 方法2：继承threading.Thread来自定义线程类，重写run方法 12345678910111213141516import threadingclass MyThread(threading.Thread): def __init__(self, n): super(MyThread, self).__init__() # 重构run函数必须要写 self.n = n def run(self): print(\"current task：\", n)if __name__ == \"__main__\": t1 = MyThread(\"thread 1\") t2 = MyThread(\"thread 2\") t1.start() t2.start() 线程合并Join函数执行顺序是逐个执行每个线程，执行完毕后继续往下执行。主线程结束后，子线程还在运行，join函数使得主线程等到子线程结束时才退出。 1234567891011121314import threadingdef count(n): while n &gt; 0: n -= 1if __name__ == \"__main__\": t1 = threading.Thread(target=count, args=(\"100000\",)) t2 = threading.Thread(target=count, args=(\"100000\",)) t1.start() t2.start() # 将 t1 和 t2 加入到主线程中 t1.join() t2.join() 线程同步与互斥锁线程之间数据共享的。当多个线程对某一个共享数据进行操作时，就需要考虑到线程安全问题。threading模块中定义了Lock 类，提供了互斥锁的功能来保证多线程情况下数据的正确性。 用法的基本步骤： 123456#创建锁mutex = threading.Lock()#锁定mutex.acquire([timeout])#释放mutex.release() 其中，锁定方法acquire可以有一个超时时间的可选参数timeout。如果设定了timeout，则在超时后通过返回值可以判断是否得到了锁，从而可以进行一些其他的处理。 具体用法见示例代码： 123456789101112131415161718192021import threadingimport timenum = 0mutex = threading.Lock()class MyThread(threading.Thread): def run(self): global num time.sleep(1) if mutex.acquire(1): num = num + 1 msg = self.name + ': num value is ' + str(num) print(msg) mutex.release()if __name__ == '__main__': for i in range(5): t = MyThread() t.start() 可重入锁（递归锁）为了满足在同一线程中多次请求同一资源的需求，Python 提供了可重入锁（RLock）。RLock内部维护着一个Lock和一个counter变量，counter 记录了 acquire 的次数，从而使得资源可以被多次 require。直到一个线程所有的 acquire 都被 release，其他的线程才能获得资源。 具体用法如下： 1234567891011#创建 RLockmutex = threading.RLock()class MyThread(threading.Thread): def run(self): if mutex.acquire(1): print(\"thread \" + self.name + \" get mutex\") time.sleep(1) mutex.acquire() mutex.release() mutex.release() 守护线程如果希望主线程执行完毕之后，不管子线程是否执行完毕都随着主线程一起结束。我们可以使用setDaemon(bool)函数，它跟join函数是相反的。它的作用是设置子线程是否随主线程一起结束，必须在start() 之前调用，默认为False。 定时器如果需要规定函数在多少秒后执行某个操作，需要用到Timer类。具体用法如下： 12345678from threading import Timer def show(): print(\"Pyhton\")# 指定一秒钟之后执行 show 函数t = Timer(1, hello)t.start() Python 多进程创建多进程Python 要进行多进程操作，需要用到muiltprocessing库，其中的Process类跟threading模块的Thread类很相似。所以直接看代码熟悉多进程。 方法1：直接使用Process, 代码如下： 123456789from multiprocessing import Process def show(name): print(\"Process name is \" + name)if __name__ == \"__main__\": proc = Process(target=show, args=('subprocess',)) proc.start() proc.join() 方法2：继承Process来自定义进程类，重写run方法, 代码如下： 123456789101112131415161718from multiprocessing import Processimport timeclass MyProcess(Process): def __init__(self, name): super(MyProcess, self).__init__() self.name = name def run(self): print('process name :' + str(self.name)) time.sleep(1)if __name__ == '__main__': for i in range(3): p = MyProcess(i) p.start() for i in range(3): p.join() 多进程通信进程之间不共享数据的。如果进程之间需要进行通信，则要用到Queue模块或者Pipi模块来实现。 Queue Queue 是多进程安全的队列，可以实现多进程之间的数据传递。它主要有两个函数,put和get。 put() 用以插入数据到队列中，put 还有两个可选参数：blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，该方法会阻塞 timeout 指定的时间，直到该队列有剩余的空间。如果超时，会抛出 Queue.Full 异常。如果 blocked 为 False，但该 Queue 已满，会立即抛出 Queue.Full 异常。 get()可以从队列读取并且删除一个元素。同样，get 有两个可选参数：blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，那么在等待时间内没有取到任何元素，会抛出 Queue.Empty 异常。如果blocked 为 False，有两种情况存在，如果 Queue 有一个值可用，则立即返回该值，否则，如果队列为空，则立即抛出 Queue.Empty 异常。 具体用法如下： 1234567891011from multiprocessing import Process, Queue def put(queue): queue.put('Queue 用法') if __name__ == '__main__': queue = Queue() pro = Process(target=put, args=(queue,)) pro.start() print(queue.get()) pro.join() Pipe Pipe的本质是进程之间的用管道数据传递，而不是数据共享，这和socket有点像。pipe() 返回两个连接对象分别表示管道的两端，每端都有send() 和recv()函数。 如果两个进程试图在同一时间的同一端进行读取和写入那么，这可能会损坏管道中的数据。 具体用法如下： 123456789101112from multiprocessing import Process, Pipe def show(conn): conn.send('Pipe 用法') conn.close() if __name__ == '__main__': parent_conn, child_conn = Pipe() pro = Process(target=show, args=(child_conn,)) pro.start() print(parent_conn.recv()) pro.join() 进程池创建多个进程，我们不用傻傻地一个个去创建。我们可以使用Pool模块来搞定。 Pool 常用的方法如下： 方法 含义 apply() 同步执行（串行） apply_async() 异步执行（并行） terminate() 立刻关闭进程池 join() 主进程等待所有子进程执行完毕。必须在close或terminate()之后使用 close() 等待所有进程结束后，才关闭进程池 具体用法见示例代码： 12345678910111213from multiprocessing import Pooldef show(num): print('num : ' + str(num))if __name__==\"__main__\": pool = Pool(processes = 3) for i in xrange(6): # 维持执行的进程总数为processes，当一个进程执行完毕后会添加新的进程进去 pool.apply_async(show, args=(i, )) print('====== apply_async ======') pool.close() #调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束 pool.join() 选择多线程还是多进程？在这个问题上，首先要看下你的程序是属于哪种类型的。一般分为两种 CPU 密集型 和 I/O 密集型。 CPU 密集型：程序比较偏重于计算，需要经常使用 CPU 来运算。例如科学计算的程序，机器学习的程序等。 I/O 密集型：顾名思义就是程序需要频繁进行输入输出操作。爬虫程序就是典型的 I/O 密集型程序。 如果程序是属于 CPU 密集型，建议使用多进程。而多线程就更适合应用于 I/O 密集型程序。","link":"/710.html"},{"title":"详解 Requests 库的用法","text":"如果你把上篇多线程和多进程的文章搞定了，那么要恭喜你了 。你编写爬虫的能力上了一个崭新的台阶。不过，我们还不能沾沾自喜，因为任重而道远。那么接下来就关注下本文的主要内容。本文主要介绍 urllib 库的代替品 —— Requests。 Requests 简介引用 Requests 官网的说明： Requests is the only Non-GMO HTTP library for Python, safe for human consumption. Requests 官方的介绍语是多么霸气。之所以能够这么霸气，是因为 Requests 相比 urllib 在使用方面上会让开发者感到更加人性化、更加简洁、更加舒服。并且国外一些知名的公司也在使用该库，例如 Google、Microsoft、Amazon、Twitter 等。因此，我们就更加有必要来学习 Request 库了。在学习之前，我们来看下它究竟具有哪些特性？ 具体如下： Keep-Alive &amp; 连接池 国际化域名和 URL 带持久 Cookie 的会话 浏览器式的 SSL 认证 自动内容解码 基本/摘要式的身份认证 优雅的 key/value Cookie 自动解压 Unicode 响应体 HTTP(S) 代理支持 文件分块上传 流下载 连接超时 分块请求 支持 .netrc 安装 Requests古人云：“工欲善其事，必先利其器”。在学习 Requests 之前，我们应先将库安装好。安装它有两种办法。 方法1：通过 pip 安装 比较推荐使用这种方式，既简单又方便管理。 1234pip install requests# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install requests 方法2：通过源码安装 先通过 git 克隆源码库： 1git clone git://github.com/kennethreitz/requests.git 或者直接到 github 网页上下载源码压缩包 接着进入到 requests 目录中执行以下命令： 1python setup.py install 发起请求有了前面学习 urllib 库的经验，现在我们学习 Requests 应该会更加容易上手。 简单抓取网页我们使用 Requests 向百度贴吧发起一个 HTTP 请求，并获取到它页面的源代码。 12345import requests# 使用 get 方式请求response = requests.get('https://tieba.baidu.com/')print(response.text) 那么使用 POST 请求网页，代码又该怎么写呢？相信答案已经浮现在你脑海中了。没错，就是将 get 换成 post 即可。 12345import requests# 使用 post 方式请求response = requests.post('https://tieba.baidu.com/')print(response.text) 传递 URL 参数我们在请求网页时，经常需要携带一些参数。Requests 提供了params关键字参数来满足我们的需求。params 是一个字符串字典，我们只要将字典构建并赋给 params 即可。我们也无须关心参数的编码问题，因为 Requests 很人性化，会将我们需要传递的参数正确编码。它的具体用法如下： 123456789import requestsurl = 'http://httpbin.org/get'params = {'name': 'Numb', 'author': 'Linkin Park'}# params 支持列表作为值# params = {'name': 'Good Time', 'author': ['Owl City', 'Carly Rae Jepsen']}response = requests.get(url, params=params)print(response.url)print(response.text) 如果字典为空是不会被拼接到 URL中的。另外，params 的拼接顺序是随机的，而不是写在前面就优先拼接。 123#运行结果如下：http://httpbin.org/get?name=Numb&amp;author=Linkin+Parkhttp://httpbin.org/get?name=Good+Time&amp;author=Owl+City&amp;author=Carly+Rae+Jepsen 你也许会疑问，为什么会有多了个”+”号呢？这个是 Requests 为了替代空格，它在请求时会自动转化为空格的。 构造请求头为了将 Requests 发起的 HTTP 请求伪装成浏览器，我们通常是使用headers关键字参数。headers 参数同样也是一个字典类型。具体用法见以下代码： 12345678910111213141516171819202122232425262728import requestsurl = 'https://tieba.baidu.com/'headers = { 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36' # 还可以设置其他字段。}response = requests.get(url, headers=headers)print(response.url)print(response.text)``` ### 使用 data 参数提交数据data 参数通常结合 POST 请求方式一起使用。如果我们需要用 POST 方式提交表单数据或者JSON数据，我们只需要传递一个字典给 data 参数。- 提交表单数据我们使用测试网页`http://httpbin.org/post`来提交表单数据作为例子进行展示。```pythonimport requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, data=data) print(response.text) 运行结果如下：我们会看到http://httpbin.org/post页面打印我们的请求内容中，有form字段。 12345678910111213141516171819202122{ \"args\": {}, \"data\": \"\", \"files\": {}, \"form\": { \"pass\": \"admin\", \"user\": \"admin\" }, \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Cache-Control\": \"max-age=259200\", \"Connection\": \"close\", \"Content-Length\": \"21\", \"Content-Type\": \"application/x-www-form-urlencoded\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.11.1\" }, \"json\": null, \"origin\": \"14.*.*.*\", \"url\": \"http://httpbin.org/post\"} 提交 JSON 数据 在HTTP 请求中，JSON 数据是被当作字符串文本。所以，我们使用 data 参数的传递 JSON 数据时，需要将其转为为字符串。我们继续使用上文的代码做演示。 12345678910import jsonimport requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, data=json.dumps(data))print(response.text) 你可以拿下面的运行结果和提交表单数据的运行结果做下对比，你会了解更加清楚两者的差异。 123456789101112131415161718192021{ \"args\": {}, \"data\": \"{\\\"pass\\\": \\\"admin\\\", \\\"user\\\": \\\"admin\\\"}\", \"files\": {}, \"form\": {}, \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Cache-Control\": \"max-age=259200\", \"Connection\": \"close\", \"Content-Length\": \"34\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.11.1\" }, \"json\": { \"pass\": \"admin\", \"user\": \"admin\" }, \"origin\": \"14.*.*.*\", \"url\": \"http://httpbin.org/post\"} 那是否有更加简便的方法来传递 JSON 数据？Requests 在 2.4.2 版本新增该功能。我们可以使用 json 参数直接传递，然后它会被自动编码。 123456789import requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, json=data)print(response.text) 使用代理有些网站做了浏览频率限制。如果我们请求该网站频率过高，该网站会被封掉我们的 IP，禁止我们的访问。所以我们需要使用代理来突破这“枷锁”。这里需要用到proxies参数，proxies 也是一个字典类型。具体用法如下： 123456789101112import requestsurl = 'https://tieba.baidu.com/'proxies = { 'http':\"web-proxy.oa.com:8080\", 'https':\"web-proxy.oa.com:8080\" # 若你的代理需要使用HTTP Basic Auth，可以使用 http://user:password@host/ 语法： # \"http\": \"http://user:pass@27.154.181.34:43353/\"}response = requests.get(url, proxies=proxies)print(response.url)print(response.text) 除了支持 HTTP 代理，Requests 在 2.10 版本新增支持 SOCKS 协议的代理。也就是说，Requests 能够使用 ShadowSocks 代理。看到这里，你的内心是不是有点小激动？使用 SOCKS 代理，需要额外安装一个第三方库，我们就使用 pip 来安装。 1pip install requests[socks] 安装成功之后，就可以正常使用了，用法跟 HTTP 代理相关。具体见代码： 1234proxies = { 'http': 'socks5://user:pass@host:port', 'https': 'socks5://user:pass@host:port'} 设置请求超时我们使用代理发起请求，经常会碰到因代理失效导致请求失败的情况。因此，我们对请求超时做下设置。当发现请求超时，更换代理再重连。 1response = requests.get(url, timeout=3) 如果你要同时设置 connect 和 read 的超时时间，可以传入一个元组进行设置。 1response = requests.get(url, timeout=(3, 30)) 使用 Cookie想在响应结果中获取 cookie 的一些值，可以直接访问。 1response.cookies['key'] # key 为 Cookie 字典中键 想发送 cookies 到服务器，可以使用cookies参数。同样该参数也是字典类型 1234567url = 'http://httpbin.org/cookies'# cookies = dict(domain='httpbin.org')cookies = { 'domain':'httpbin.org',}response = requests.get(url, cookies=cookies)print(response.text) 响应结果我们跟Python 打交道，摆脱不了编码的问题。使用 Requests 请求，我们无需担心编码问题。感觉 Requests 真的是太人性化了。请求发出后，Requests 会基于 HTTP 头部对响应的编码作出有根据的推测。当你访问 response .text 之时，Requests 会使用其推测的文本编码。 12response = requests.get(url)print(response.text) 如果你想改变 response 的编码格式，可以这么做： 1response.encoding = 'UTF-8' 二进制响应内容对于非文本请求， 我们能以字节的方式访问请求响应体。Requests 会自动为我们解码 gzip 和 deflate 传输编码的响应数据。例如，以请求返回的二进制数据创建一张图片，你可以使用如下代码： 1234from PIL import Imagefrom io import BytesIOi = Image.open(BytesIO(response.content)) JSON 响应内容Requests 中也有一个内置的 JSON 解码器，助我们处理 JSON 数据： 12345import requestsurl = 'https://github.com/timeline.json'response = requests.get(url)print(response.json()) 如果 JSON 解码失败， response .json() 就会抛出一个异常。例如，响应内容是 401 (Unauthorized)，尝试访问 response .json() 将会抛出 ValueError: No JSON object could be decoded 异常。 响应状态码我们需要根据响应码来判断请求的结果，具体是这样获取状态码： 1response.status_code Requests 内部提供了一个状态表，如果有需要对状态码进行判断，可以看下requests.codes的源码。 高级用法重定向与请求历史有些页面会做一些重定向的处理。Requests 又发挥人性化的特性。它在默认情况下，会帮我们自动处理所有重定向，包括 301 和 302 两种状态码。我们可以使用response .history来追踪重定向。 Response.history是一个 Response 对象的列表，为了完成请求而创建了这些对象。这个对象列表按照从最老到最近的请求进行排序。 如果我们要禁用重定向处理，可以使用allow_redirects参数： 1response = requests.get(url, allow_redirects=False) 会话Requests 支持 session 来跟踪用户的连接。例如我们要来跨请求保持一些 cookie，我们可以这么做： 1234567s = requests.Session()s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')r = s.get(\"http://httpbin.org/cookies\")print(r.text)# '{\"cookies\": {\"sessioncookie\": \"123456789\"}}' 身份认证有些 web 站点都需要身份认证成功之后才能访问。urllib 具备这样的功能，Requests 也不例外。Requests 支持基本身份认证（HTTP Basic Auth）、netrc 认证、摘要式身份认证、OAuth 1 认证等。 基本身份认证 许多要求身份认证的web服务都接受 HTTP Basic Auth。这是最简单的一种身份认证，并且 Requests 对这种认证方式的支持是直接开箱即可用。HTTP Basic Auth 用法如下： 123456from requests.auth import HTTPBasicAuthurl = 'http://httpbin.org/basic-auth/user/passwd'requests.get(url, auth=HTTPBasicAuth('user', 'pass'))# 简写的使用方式requests.get(url, auth=('user', 'pass')) 摘要式身份认证 摘要式是 HTTP 1.1 必需的第二种身份验证机制。这种身份验证由用户名和密码组成。随后将用 MD5（一种单向哈希算法）对摘要式身份验证进行哈希运算，并将其发送到服务器。具体用法如下： 1234from requests.auth import HTTPDigestAuthurl = 'http://httpbin.org/digest-auth/auth/user/passwd/MD5'requests.get(url, auth=HTTPDigestAuth('user', 'pass')) OAuth 认证 OAuth（开放授权）认证在我们的生活中随处可见。Requests 同样也支持这中认证方式，其中包括 OAuth 1.0 和 OAuth 2.0。如果你需要用到该认证，你需要安装一个支持库requests-oauthlib。我以 OAuth 1.0 认证作为例子进行讲解： 123456789import requestsfrom requests_oauthlib import OAuth1url = 'https://api.twitter.com/1.1/account/verify_credentials.json'auth = OAuth1('YOUR_APP_KEY', 'YOUR_APP_SECRET', 'USER_OAUTH_TOKEN', 'USER_OAUTH_TOKEN_SECRET')requests.get(url, auth=auth)","link":"/711.html"},{"title":"应该如何阅读？","text":"我最近阅读民主与建设出版社出版的《如何阅读一本书》，自己收获颇多。这本书算是经典之作。以通俗的语言告诉我们如何选择书籍？究竟要以什么方法来阅读一本书？我将自己学到的分享出来。希望能帮助大家提高阅读速度，把书籍读得更加明白，记得更牢固。 为什么要阅读？在进入本文主题之前，我们先思考一个问题 —— 我们为什么要阅读？你可以不必急着回答这个问题，带着问题来往下读。 古人有很多名句鼓励学子多读书，例如宋真宗赵恒的《劝学诗》，其中有我们耳熟能详的语句： 书中自有千种粟书中自有黄金屋书中车马多如簇书中自有颜如玉 由此可见，阅读是手段。我们可以通过读书来获得赖以谋能的技能。那么问题来了？我们要阅读什么书？随便阅读一本书就能获取知识吗？答案是否定的。所以我们要读好书，还要掌握些阅读的技巧。 阅读什么书？市面上书籍种类琳琅满目。我们该如何选择书籍？先来看看书籍的分类 第一类：如同主食 能够解决职业、生活、生理、心理等方面的现实问题的书籍都可以称为“主食”。”主食”是我们的刚需。所以我们就应该花大量时间去阅读。举个栗子，假如你是一名 Android 粉丝，想通过学习 Android 开发来谋生。那么你应该阅读 Android 开发的书籍，例如《第一行代码》、《Android 源码设计模式解析与实战》等。 第二类：如同美食 这类书籍是不求针对人生的现实问题，却可以满足思想要求。对于这些书籍，我们应该重“质”不重“量”。我们不知道怎么选择这类书籍时，可以根据一些名家推荐或者订阅一些名家的微信公众号。例如，张哥的 stomzhang 公众号。张哥的每篇推文，我基本上都有仔细阅读。我自己订阅张哥的公众号一年多了，提高不仅仅是专业技能，更是视野。 第三类：如同蔬菜、水果 可以理解为工具书。这类书籍不仅可以帮助我们查找不了解的字词、概念、数据等信息，也可以帮助我们掌握通用的方法与技巧。 第四类：如同甜点、零食 这类书籍是用于娱乐、消遣、满足休闲需求。一些网络小说和娱乐性图书，包括一部分畅销书都属于此列。对于这类书籍，我们只可偶尔阅读，但不能过。 阅读方法阅读可以分为四个层次，不同的阅读层次适用不同的阅读方法。具体分类如下：1）基础阅读（Elementray Reading）2）检视阅读（Inspectional Reading）3）分析阅读（Analytical Reading）4）主题阅读（Syntopical Reading） 第一层：基础阅读基础阅读，即处于四肢阶段的阅读。所以又被称为初级阅读。我们在小学阶段已经学会了，因为这一层次的阅读要求是认识文字并了解文字的意思。 第二层：检视阅读检视阅读，即系统化略读。类似我们初高中做语文的阅读理解题目。 检视阅读是非常有价值的阅读方式。通过检视阅读，我们可以了解一本书“主要讲什么内容”、“书的结构如何”、“各章重点讲什么”，进而判断这本书是否值得分析阅读或主题阅读。 对一本书进行检视阅读，可以按照以下步骤：A、看书名、副题、作者简介、序言，大致清楚作者的写作风格、作者在什么背景下著作本书的。B、研究目录，了解作者的写作路线。C、粗略地阅读一下各章的内容，遇到不懂的部分就跳过去。 看到这里，你可能有这样的疑问：我没有将一本书读完，只阅读其中几章，这算是阅读吗？算。诸葛孔明的“略其大意”，陶渊明的“不求甚解”都算是这种阅读方法。 第三层：分析阅读分析阅读，即完整阅读。也就是精读。通过分析阅读，我们可以对全书有更精准的把握，复述全书各部分的大意及重要细节，然后使之成为自己的知识。 这种阅读方法更适合“主食”和“美食”类图书。使用分析阅读就是带着四个问题去阅读： 1）这本书究竟讲了什么？ 回到这个问题，事实上就是做到以下三步：A、对书的体裁和主题进行确认。B、自己组织语言表述整本书的内容C、按照全书的结构顺序或逻辑顺序列举全书的大纲，并将各个部分的大纲也列出来。思维导图就可以派上用场了。 2）作者通过这本书解决了什么问题？ 回答这个问题，也需要一步一步找出作者的主要想法、声明与论点，并形成自己的判断。首先，找出可以表达作者观点的关键字，与作者达成共识。然后，在最重要的语句中，提炼关键字，抓住作者的主旨。最后，根据书中的内容确定作者已经解决了哪些问题，还有哪些是没有解决的。 3）这本书说得有道理吗？ 对本书进行评论就是这个问题的答案。但在没有对问题 2 进行完整的解答之前，不要轻易去尝试。评论一本书不要带有个人感情色彩。 我个人觉得评论一本书类似写议论文，要有理有据，求同存异。 4）这本书与我有什么关系？ 这个问题与阅读效用有密切相关，简单地回答，就是“有用”或者“部分有用”。 如果一本书告诉我们一些咨询，我们一定要问一问这些咨询有什么意义；如果一本书不仅提供咨询，还对我们有所启发，就更应该找出书中更深的含意或其他相关的建议，以获得更多启示。 第四层：主题阅读 主题阅读是主动的、专一的、大量的阅读。 主题阅读，顾名思义就是定个主题，然后使用检视阅读来筛选与主题有关的书籍，再对每本书中与主题极为相关的具体章节进行精读来建立自己的主旨（论点）。 这种阅读方法带有很强的阅读性，不能短时间能掌握的，需要长期的阅读积累以及阅读训练。","link":"/812.html"},{"title":"干将莫邪” —— Xpath 与 lxml 库","text":"前面的文章，我们已经学会正则表达式以及 BeautifulSoup库的用法。我们领教了正则表达式的便捷，感受 beautifulSoup 的高效。本文介绍也是内容提取的工具 —— Xpath，它一般和 lxml 库搭配使用。所以，我称这两者为“干将莫邪”。 Xpath 和 lxml Xpath XPath即为XML路径语言，它是一种用来确定XML（标准通用标记语言的子集）文档中某部分位置的语言。XPath 基于 XML 的树状结构，提供在数据结构树中找寻节点的能力。 Xpath 原本是用于选取 XML 文档节点信息。XPath 是于 1999 年 11 月 16 日 成为 W3C 标准。因其既简单方便又容易，所以它逐渐被人说熟知。 lxml lxml 是功能丰富又简单易用的，专门处理 XML 和 HTML 的 Python 官网标准库。 Xpath 的语法正则表达式的枯燥无味又学习成本高，Xpath 可以说是不及其万分之一。所以只要花上 10 分钟，掌握 Xpath 不在话下。Xpath 的语言以及如何从 HTML dom 树中提取信息，我将其归纳为“主干 - 树支 - 绿叶”。 “主干” —— 选取节点抓取信息，我们需知道要从哪里开始抓取。因此，需要找个起始节点。Xpath 选择起始节点有以下可选： 表达式 描述 nodename 选取标签节点的所有子节点。 / 从根节点选取，html DOM 树的节点就是 html。 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。 . 选择当前节点，一般用于二级提取。 .. 选取当前节点的父节点，在二级提取时用到。 @ 选取属性。 我们通过以下实例来了解其用法： 路径表达式 描述 xpath(‘//div’) 选取 div 元素的所有子节点。 xpath(‘/div’) 选取 div 元素作为根节点。如果同级有多个 div ，那么所有 div 都会被选为根节点。 xpath(‘/div/span’) 选取属于 div 元素下所有 span 元素节点。如果 span 有多个，也会被选中。 xpath(‘//div’) 选取所有 div 元素节点，不管它们在文档的位置。 xpath(‘//div/span’) 选取 div 元素下的所有 span 元素节点，不管位于 div 之下的什么位置 xpath(“//@[class=’content’]”) 选取包含属性 class 的值为 content 的节点，不管是 div 元素还是其他元素 xpath(“//@[id=’center’]”) 选取属性 id 的值为 center 的节点，不管是 div 元素还是其他元素 如果你对于提取节点没有头绪的时候，可以使用通配符来暂时替代。等查看输出内容之后再进一步确认。 路径表达式 描述 xpath(‘/div/*’) 选取 div 元素节点下的所有节点 xpath(‘/div[@*]’) 选取所有带属性的 div 元素节点 “分支” —— 关系节点与谓语这一步的过程其实是通过起点一步步来寻找最终包含我们所需内容的节点。我们有时需要使用到相邻节点信息。因此，我们需要了解关系节点或者谓语。 关系节点 一般而言，DOM 树中一个普通节点具有父节点、兄弟节点、子节点。当然也有例外的情况。这些有些节点比较特殊，可能没有父节点，如根节点；也有可能是没有子节点，如深度最大的节点。Xpath 也是有支持获取关系节点的语法。 关系 路径表达式 描述 parent（父） xpath(‘./parent::*’) 选取当前节点的父节点 ancestor（先辈） xpath(‘./ancestor::*’) 选取当前节点的所有先辈节点，包括父、祖父等 ancestor-or-self（先辈及本身） xpath(‘./ancestor-or-self::*’) 选取当前节点的所有先辈节点以及节点本身 child（子） xpath(‘./child::*’) 选取当前节点的所有子节点 descendant（后代） xpath(‘./descendant::*’) 选取当前节点的所有后代节点，包括子节点、孙节点等 following xpath(‘./following ::*’) 选取当前节点结束标签后的所有节点 following-sibing xpath(‘./following-sibing::*’) 选取当前节点之后的兄弟节点 preceding xpath(‘./preceding::*’) 选取当前节点开始标签前的所有节点 preceding-sibling xpath(‘./parent::*’) 选取当前节点之前的兄弟节点 self（本身） xpath(‘./self::*’) 选取当前节点本身 谓语 谓语用来查找某个特定的节点或者包含某个指定的值的节点。同时，它是被嵌在方括号中的。 路径表达式 描述 xpath(‘./body/div[1]’) 选取 body 元素节点下的第一个 div 子节。 xpath(‘./body/div[last()]’) 选取 body 元素节点下的最后一个 div 子节。 xpath(‘./body/div[last()-1]’) 选取 body 元素节点下的倒数第二个 div 子节。 xpath(‘./body/div[position()-3]’) 选取 body 元素节点下的前二个 div 子节。 xpath(‘./body/div[@class]’) 选取 body 元素节点下的所有带有 class 属性的 div 子节。 xpath(“./body/div[@class=’content’]”) 选取 body 元素节点下的 class 属性值为 centent 的 div 子节。 “绿叶” —— 节点内容以及属性到了这一步，我们已经找到所需内容的节点了。接下来就是获取该节点中的内容了。Xpath 语法提供了提供节点的文本内容以及属性内容的功能。 路径表达式 描述 text() 获取节点的本文内容 @属性 获取节点的属性内容 具体用法见以下实例： 路径表达式 描述 xpath(‘./a/text()’) 获取当前节点中 a 元素节点中的本文内容 xpath(‘./a/@href’) 获取当前节点中 a 元素节点中的 href 属性的内容 lxml 的用法安装 lxmlpip 是安装库文件的最简便的方法，具体命令如下： 1234pip install lxml# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install lxml 使用 lxmllxml 使用起来是比较简单的。我们首先要使用 lxml 的 etree 将 html 页面进行初始化，然后丢给 Xpath 匹配即可。具体用法如下： 12345from lxml import etreehtml = requests.get(url) # 使用 requests 请求网页selector = etree.HTML(html.text)content = selector.xpath('//a/text()') 没错，就这短短几行代码即可完成信息提取。值得注意的是：xpath 查找匹配返回的类型有可能是一个值，也有可能是一个存放多个值的列表。这个取决于你的路径表达式是如何编写的。","link":"/813.html"},{"title":"爬虫实战二：爬取电影天堂的最新电影","text":"前面两篇文章介绍 requests 和 xpath 的用法。我们推崇学以致用，所以本文讲解利用这两个工具进行实战。 爬取目标本次爬取的站点选择电影天堂，网址是： www.ydtt8.net。爬取内容是整个站点的所有电影信息，包括电影名称，导演、主演、下载地址等。具体抓取信息如下图所示： 设计爬虫程序确定爬取入口电影天堂里面的电影数目成千上万，电影类型也是让人眼花缭乱。我们为了保证爬取的电影信息不重复， 所以要确定一个爬取方向。目前这情况真让人无从下手。但是，我们点击主页中的【最新电影】选项，跳进一个新的页面。蓦然有种柳暗花明又一村的感觉。 由图可知道，电影天堂有 5 个电影栏目，分别为最新电影、日韩电影、欧美电影、国内电影、综合电影。每个栏目又有一定数量的分页，每个分页有 25 条电影信息。那么程序的入口可以有 5 个 url 地址。这 5 个地址分别对应每个栏目的首页链接。 爬取思路知道爬取入口，后面的工作就容易多了。我通过测试发现这几个栏目除了页面的 url 地址不一样之外，其他例如提取信息的 xpath 路径是一样的。因此，我把 5 个栏目当做 1 个类，再该类进行遍历爬取。 我这里“最新电影”为例说明爬取思路。1）请求栏目的首页来获取到分页的总数，以及推测出每个分页的 url 地址；2）将获取到的分页 url 存放到名为 floorQueue 队列中；3）从 floorQueue 中依次取出分页 url，然后利用多线程发起请求；4）将获取到的电影页面 url 存入到名为 middleQueue 的队列；5）从 middleQueue 中依次取出电影页面 url，再利用多线程发起请求；6）将请求结果使用 Xpath 解析并提取所需的电影信息；7）将爬取到的电影信息存到名为 contentQueue 队列中；8）从 contentQueue 队列中依次取出电影信息，然后存到数据库中。 设计爬虫架构根据爬取思路，我设计出爬虫架构。如下图所示： 代码实现主要阐述几个重要的类的代码 main 类 主要工作两个：第一，实例化出一个dytt8Moive对象，然后开始爬取信息。第二，等爬取结束，将数据插入到数据库中。 处理爬虫的逻辑代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 截止到2017-08-08, 最新电影一共才有 164 个页面LASTEST_MOIVE_TOTAL_SUM = 6 #164# 请求网络线程总数, 线程不要调太好, 不然会返回很多 400THREAD_SUM = 5def startSpider(): # 实例化对象 # 获取【最新电影】有多少个页面 LASTEST_MOIVE_TOTAL_SUM = dytt_Lastest.getMaxsize() print('【最新电影】一共 ' + str(LASTEST_MOIVE_TOTAL_SUM) + ' 有个页面') dyttlastest = dytt_Lastest(LASTEST_MOIVE_TOTAL_SUM) floorlist = dyttlastest.getPageUrlList() floorQueue = TaskQueue.getFloorQueue() for item in floorlist: floorQueue.put(item, 3) # print(floorQueue.qsize()) for i in range(THREAD_SUM): workthread = FloorWorkThread(floorQueue, i) workthread.start() while True: if TaskQueue.isFloorQueueEmpty(): break else: pass for i in range(THREAD_SUM): workthread = TopWorkThread(TaskQueue.getMiddleQueue(), i) workthread.start() while True: if TaskQueue.isMiddleQueueEmpty(): break else: pass insertData() if __name__ == '__main__': startSpider() 创建数据库以及表，接着再把电影信息插入到数据库的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def insertData(): DBName = 'dytt.db' db = sqlite3.connect('./' + DBName, 10) conn = db.cursor() SelectSql = 'Select * from sqlite_master where type = \"table\" and name=\"lastest_moive\";' CreateTableSql = ''' Create Table lastest_moive ( 'm_id' INTEGER PRIMARY KEY, 'm_type' varchar(100), 'm_trans_name' varchar(200), 'm_name' varchar(100), 'm_decade' varchar(30), 'm_conutry' varchar(30), 'm_level' varchar(100), 'm_language' varchar(30), 'm_subtitles' varchar(100), 'm_publish' varchar(30), 'm_IMDB_socre' varchar(50), 'm_douban_score' varchar(50), 'm_format' varchar(20), 'm_resolution' varchar(20), 'm_size' varchar(10), 'm_duration' varchar(10), 'm_director' varchar(50), 'm_actors' varchar(1000), 'm_placard' varchar(200), 'm_screenshot' varchar(200), 'm_ftpurl' varchar(200), 'm_dytt8_url' varchar(200) ); ''' InsertSql = ''' Insert into lastest_moive(m_type, m_trans_name, m_name, m_decade, m_conutry, m_level, m_language, m_subtitles, m_publish, m_IMDB_socre, m_douban_score, m_format, m_resolution, m_size, m_duration, m_director, m_actors, m_placard, m_screenshot, m_ftpurl, m_dytt8_url) values(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?); ''' if not conn.execute(SelectSql).fetchone(): conn.execute(CreateTableSql) db.commit() print('==== 创建表成功 ====') else: print('==== 创建表失败, 表已经存在 ====') count = 1 while not TaskQueue.isContentQueueEmpty(): item = TaskQueue.getContentQueue().get() conn.execute(InsertSql, Utils.dirToList(item)) db.commit() print('插入第 ' + str(count) + ' 条数据成功') count = count + 1 db.commit() db.close() TaskQueue 类 维护 floorQueue、middleQueue、contentQueue 三个队列的管理类。之所以选择队列的数据结构，是因为爬虫程序需要用到多线程，队列能够保证线程安全。 dytt8Moive 类 dytt8Moive 类是本程序的主心骨。程序最初的爬取目标是 5 个电影栏目，但是目前只现实了爬取最新栏目。如果你想爬取全部栏目电影，只需对 dytt8Moive 稍微改造下即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252class dytt_Lastest(object): # 获取爬虫程序抓取入口 breakoutUrl = 'http://www.dytt8.net/html/gndy/dyzz/index.html' def __init__(self, sum): self.sum = sum # 获取【最新电影】有多少个页面 # 截止到2017-08-08, 最新电影一共才有 164 个页面 @classmethod def getMaxsize(cls): response = requests.get(cls.breakoutUrl, headers=RequestModel.getHeaders(), proxies=RequestModel.getProxies(), timeout=3) # 需将电影天堂的页面的编码改为 GBK, 不然会出现乱码的情况 response.encoding = 'GBK' selector = etree.HTML(response.text) # 提取信息 optionList = selector.xpath(\"//select[@name='sldd']/text()\") return len(optionList) - 1 # 因首页重复, 所以要减1 def getPageUrlList(self): ''' 主要功能：目录页url取出，比如：http://www.dytt8.net/html/gndy/dyzz/list_23_'+ str(i) + '.html ''' templist = [] request_url_prefix = 'http://www.dytt8.net/html/gndy/dyzz/' templist = [request_url_prefix + 'index.html'] for i in range(2, self.sum + 1): templist.append(request_url_prefix + 'list_23_' + str(i) + '.html') for t in templist: print('request url is ### ' + t + ' ###') return templist @classmethod def getMoivePageUrlList(cls, html): ''' 获取电影信息的网页链接 ''' selector = etree.HTML(html) templist = selector.xpath(\"//div[@class='co_content8']/ul/td/table/tr/td/b/a/@href\") # print(len(templist)) # print(templist) return templist @classmethod def getMoiveInforms(cls, url, html): ''' 解析电影信息页面的内容, 具体如下： 类型 : 疾速特攻/疾速追杀2][BD-mkv.720p.中英双字][2017年高分惊悚动作] ◎译名 : ◎译\\u3000\\u3000名\\u3000疾速特攻/杀神John Wick 2(港)/捍卫任务2(台)/疾速追杀2/极速追杀：第二章/约翰·威克2 ◎片名 : ◎片\\u3000\\u3000名\\u3000John Wick: Chapter Two ◎年代 : ◎年\\u3000\\u3000代\\u30002017 ◎国家 : ◎产\\u3000\\u3000地\\u3000美国 ◎类别 : ◎类\\u3000\\u3000别\\u3000动作/犯罪/惊悚 ◎语言 : ◎语\\u3000\\u3000言\\u3000英语 ◎字幕 : ◎字\\u3000\\u3000幕\\u3000中英双字幕 ◎上映日期 ：◎上映日期\\u30002017-02-10(美国) ◎IMDb评分 : ◎IMDb评分\\xa0 8.1/10 from 86,240 users ◎豆瓣评分 : ◎豆瓣评分\\u30007.7/10 from 2,915 users ◎文件格式 : ◎文件格式\\u3000x264 + aac ◎视频尺寸 : ◎视频尺寸\\u30001280 x 720 ◎文件大小 : ◎文件大小\\u30001CD ◎片长 : ◎片\\u3000\\u3000长\\u3000122分钟 ◎导演 : ◎导\\u3000\\u3000演\\u3000查德·史塔赫斯基 Chad Stahelski ◎主演 : ◎简介 : 暂不要该字段 ◎获奖情况 : 暂不要该字段 ◎海报 影片截图 下载地址 ''' # print(html) contentDir = { 'type': '', 'trans_name': '', 'name': '', 'decade': '', 'conutry': '', 'level': '', 'language': '', 'subtitles': '', 'publish': '', 'IMDB_socre': '', 'douban_score': '', 'format': '', 'resolution': '', 'size': '', 'duration': '', 'director': '', 'actors': '', 'placard': '', 'screenshot': '', 'ftpurl': '', 'dytt8_url': '' } selector = etree.HTML(html) content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/p/text()\") # 匹配出来有两张图片, 第一张是海报, 第二张是电影画面截图 imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/p/img/@src\") # print(content) # 为了兼容 2012 年前的页面 if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/span/text()\") # 有些页面特殊, 需要用以下表达式来重新获取信息 # 电影天堂页面好混乱啊~ if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/div/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/font/text()\") if len(content) &lt; 5: content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/p/font/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/span/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/span/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/font/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/text()\") # print(content) # 不同渲染页面要采取不同的抓取方式抓取图片 if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/img/@src\") if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/img/@src\") if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/img/@src\") if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/div/img/@src\") # 类型 if content[0][0:1] != '◎': contentDir['type'] = '[' + content[0] actor = '' for each in content: if each[0:5] == '◎译\\u3000\\u3000名': # 译名 ◎译\\u3000\\u3000名\\u3000 一共占居6位 contentDir['trans_name'] = each[6: len(each)] elif each[0:5] == '◎片\\u3000\\u3000名': # 片名 contentDir['name'] = each[6: len(each)] elif each[0:5] == '◎年\\u3000\\u3000代': # 年份 contentDir['decade'] = each[6: len(each)] elif each[0:5] == '◎产\\u3000\\u3000地': # 产地 contentDir['conutry'] = each[6: len(each)] elif each[0:5] == '◎类\\u3000\\u3000别': # 类别 contentDir['level'] = each[6: len(each)] elif each[0:5] == '◎语\\u3000\\u3000言': # 语言 contentDir['language'] = each[6: len(each)] elif each[0:5] == '◎字\\u3000\\u3000幕': # 字幕 contentDir['subtitles'] = each[6: len(each)] elif each[0:5] == '◎上映日期': # 上映日期 contentDir['publish'] = each[6: len(each)] elif each[0:7] == '◎IMDb评分': # IMDb评分 contentDir['IMDB_socre'] = each[9: len(each)] elif each[0:5] == '◎豆瓣评分': # 豆瓣评分 contentDir['douban_score'] = each[6: len(each)] elif each[0:5] == '◎文件格式': # 文件格式 contentDir['format'] = each[6: len(each)] elif each[0:5] == '◎视频尺寸': # 视频尺寸 contentDir['resolution'] = each[6: len(each)] elif each[0:5] == '◎文件大小': # 文件大小 contentDir['size'] = each[6: len(each)] elif each[0:5] == '◎片\\u3000\\u3000长': # 片长 contentDir['duration'] = each[6: len(each)] elif each[0:5] == '◎导\\u3000\\u3000演': # 导演 contentDir['director'] = each[6: len(each)] elif each[0:5] == '◎主\\u3000\\u3000演': # 主演 actor = each[6: len(each)] for item in content: if item[0: 4] == '\\u3000\\u3000\\u3000\\u3000': actor = actor + '\\n' + item[6: len(item)] # 主演 contentDir['actors'] = actor # 海报 if imgs[0] != None: contentDir['placard'] = imgs[0] # 影片截图 if imgs[1] != None: contentDir['screenshot'] = imgs[1] # 下载地址 ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/table/tbody/tr/td/a/text()\") # 为了兼容 2012 年前的页面 if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/table/tbody/tr/td/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/div/table/tbody/tr/td/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/table/tbody/tr/td/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/span/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/div/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/span/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/span/div/table/tbody/tr/td/font/a/text()\") contentDir['ftpurl'] = ftp[0] # 页面链接 contentDir['dytt8_url'] = url print(contentDir) return contentDir getMoiveInforms 方法是主要负责解析电影信息节点并将其封装成字典。在代码中，你看到 Xpath 的路径表达式不止一条。因为电影天堂的电影详情页面的排版参差不齐，所以单单一条内容提取表达式、海报和影片截图表达式、下载地址表达式远远无法满足。 选择字典类型作为存储电影信息的数据结构，也是自己爬坑之后才决定的。这算是该站点另一个坑人的地方。电影详情页中有些内容节点是没有，例如类型、豆瓣评分，所以无法使用列表按顺序保存。 爬取结果我这里展示自己爬取最新栏目中 4000 多条数据中前面部分数据。 如果你想获取网站的源码, 点击☞☞☞Github 仓库地址","link":"/814.html"},{"title":"用 Python 学习数据结构, 有它就不用愁","text":"数据结构，我们对它已经是耳熟能详。对于计算机相关专业的大学生来说，它是一门专业必修课。从事软件开发的人员则把它作为谋生必备技能。这充分体现数据结构的重要性。因此，我们对数据结构是不得不学。 虽然数据结构的实现不限制语言，但市面上很多教程书籍都是以 C 语言作为编程语言进行讲解。如果你喜欢且在学习 Python，可能会陷入苦于这样的烦恼中。那就是没有 Python 版本的数据结构实现代码。莫慌！我给大家推荐一个第三方库，它能让你这种烦恼立刻云消雾散。 它就是Pygorithm 地址：https://github.com/OmkarPathak/pygorithm Pygorithm 是由一个热心肠的印度小哥编写的开源项目。他编写创建该库的初衷是处于教学目的。我们不仅可以阅读源码的方式学习数据结构，而且可以把它当做现成工具来使用。 安装安装 python 库，我推荐使用 pip 方式，方便又省事。 1234pip install Pygorithm# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install Pygorithm 支持的类型Pygorithm 实现的数据结构类型有以下这几种，括号中表示包名。 栈 栈 (data_structures.stack.Stack) 中缀表达式转换为后缀表达式 (data_structures.stack.InfixToPostfix) 队列 队列 (data_structures.queue.Queue) 双端队列 (data_structures.queue.Deque) 链表 单向链表 (data_structures.linked_list.SinglyLinkedList) 双向链表 (data_structures.linked_list.DoublyLinkedList) 树 二叉树 (data_structures.tree.BinaryTree) 搜索二叉树 (data_structures.tree.BinarySearchTree) 图 图 (data_structures.graph.Graph) 拓扑排序 (data_structures.graph.TopologicalSort) 有向图 (data_structures.graph.CheckCycleDirectedGraph) 无向图 (data_structures.graph.CheckCycleUndirectedGraph) 堆 堆 (data_structures.heap.Heap) 字典树 字典树 （data_structures.trie.Trie） 常见算法你也许没有想到吧。Pygorithm 中也实现一些常见的路径搜索、查找、排序等算法。 常见的路径搜索算法： Dijkstra(迪杰斯特拉) Unidirectional AStar（单向 A*算法） BiDirectional AStar（双向 A*算法） 常见的查找算法： Linear Search (线性查找) Binary Search (二分法查找) Breadth First Search (广度优先搜索) Depth First Search (深度优先搜索) 常见的排序算法： bubble_sort（冒泡算法） bucket_sort（桶排序） counting_sort （计数排序） heap_sort（堆排序） insertion_sort（插入排序） merge_sort（归并排序） quick_sort （快速排序） selection_sort（选择排序） shell_sort（希尔排序）","link":"/815.html"},{"title":"学会运用爬虫框架 Scrapy (一)","text":"对于规模小、爬取数据量小、对爬取速度不敏感的爬虫程序， 使用 Requests 能轻松搞定。这些爬虫程序主要功能是爬取网页、玩转网页。如果我们需要爬取网站以及系列网站，要求爬虫具备爬取失败能复盘、爬取速度较高等特点。很显然 Requests 不能完全满足我们的需求。因此，需要一功能更加强大的第三方爬虫框架库 —— Scrapy 简介 ScrapyScrapy 是一个为了方便人们爬取网站数据，提取结构性数据而编写的分布式爬取框架。它可以应用在包括数据挖掘， 信息处理或存储历史数据等一系列的程序中。因其功能颇多，所以学会它需要一定的时间成本。 Scrapy 的特性Scrapy 是一个框架。因此，它集一些各功能强大的 python 库的优点于一身。下面列举其一些特性： HTML, XML源数据 选择及提取 的内置支持 提供了一系列在spider之间共享的可复用的过滤器(即 Item Loaders)，对智能处理爬取数据提供了内置支持。 通过 feed导出 提供了多格式(JSON、CSV、XML)，多存储后端(FTP、S3、本地文件系统)的内置支持 提供了media pipeline，可以 自动下载 爬取到的数据中的图片(或者其他资源)。 高扩展性。您可以通过使用 signals ，设计好的API(中间件, extensions, pipelines)来定制实现您的功能。 内置的中间件及扩展为下列功能提供了支持: cookies and session 处理 HTTP 压缩 HTTP 认证 HTTP 缓存 user-agent模拟 robots.txt 爬取深度限制 健壮的编码支持和自动识别，用于处理外文、非标准和错误编码问题 针对多爬虫下性能评估、失败检测，提供了可扩展的 状态收集工具 。 内置 Web service, 使您可以监视及控制您的机器。 安装 ScrapyScrapy 是单纯用 Python 语言编写的库。所以它有依赖一些第三方库，如lxml, twisted,pyOpenSSL等。我们也无需逐个安装依赖库，使用 pip 方式安装 Scrapy 即可。pip 会自动安装 Scrapy 所依赖的库。随便也说下 Scrapy 几个重要依赖库的作用。 lxml：XML 和 HTML 文本解析器，配合 Xpath 能提取网页中的内容信息。如果你对 lxml 和 Xpath 不熟悉，你可以阅读我之前介绍该库用法的文章。 Twisted：Twisted 是 Python 下面一个非常重要的基于事件驱动的IO引擎。 pyOpenSSL：pyopenssl 是 Python 的 OpenSSL 接口。 在终端执行以下命令来安装 Scrapy 1234pip install Scrapy # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install Scrapy 你在安装过程中也许会报出安装 Twisted 失败的错误： 1234567running build_extbuilding 'twisted.test.raiser' extensionerror: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": http://landinghub.visualstudio.com/visual-cpp-build-tools----------------------------------------Failed building wheel for TwistedRunning setup.py clean for TwistedFailed to build Twisted 原因是 Twisted 底层是由 C 语言编写的，所以需要安装C语言的编译环境。对于Python3.5来说，可以通过安装 Visual C++ Build Tools 来安装这个环境。打开上面报错文本中的链接，下载并安装 visualcppbuildtools_full 。等安装完成，再执行 安装 Scrapy 命令。 安装成功之后如下图： 初探 ScrapyScrapy 项目解析Scrapy 新建项目需通过命令行操作。在指定文件夹中，打开终端执行以下命令： 1scrapy startproject 项目的名字 我新建一个名为 scrapy_demo，执行结果如下。 使用 Pycharm 打开该项目，我们会发现项目的层级架构以及文件。 这些文件的作用是： scrapy.cfg：项目的配置文件，开发无需用到。 scrapy_demo：项目中会有两个同名的文件夹。最外层表示 project，里面那个目录代表 module（项目的核心）。 scrapy_demo/items.py：以字段形式定义后期需要处理的数据。 scrapy_demo/pipelines.py：提取出来的 Item 对象返回的数据并进行存储。 scrapy_demo/settings.py：项目的设置文件。可以对爬虫进行自定义设置，比如选择深度优先爬取还是广度优先爬取，设置对每个IP的爬虫数，设置每个域名的爬虫数，设置爬虫延时，设置代理等等。 scrapy_demo/spider： 这个目录存放爬虫程序代码。 __init__.py：python 包要求，对 scrapy 作用不大。 Scrapy 的架构我们刚接触到新事物，想一下子就熟悉它。这明显是天方夜谭。应按照一定的顺序层次、逐步深入学习。学习 Scrapy 也不外乎如此。在我看来，Scrapy 好比由许多组件拼装起来的大机器。因此，可以采取从整体到局部的顺序学习 Scrapy。下图是 Scrapy 的架构图，它能让我们对 Scrapy 有了大体地认识。后续的文章会逐个介绍其组件用法。 我按照从上而下，从左往右的顺序阐述各组件的作用。 Scheduler：调度器。负责接受 Engine 发送过来的 Requests 请求，并将其队列化； Item Pipeline：Item Pipeline负责处理被spider提取出来的item。其有典型应用，如清理 HTML 数据、验证爬取的数据（检查 item 包含某些字段）、查重（并丢弃）、爬取数据持久化（存入数据库、写入文件等）； Scrapy Engine：引擎是 Scrapy 的中枢。它负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件； Downloader Middlewares：下载中间件是 Engine 和 Downloader 的枢纽。负责处理 Downloader 传递给 Engine 的 responses；它还支持自定义扩展。 Downloader：负责下载 Engine 发送的所有 Requests 请求，并将其获取到的 responses 回传给 Scrapy Engine； Spider middlewares：Spider 中间件是 Engine 和 Spider 的连接桥梁；它支持自定义扩展来处理 Spider 的输入(responses) 以及输出 item 和 requests 给 Engine ； Spiders：负责解析 Responses 并提取 Item 字段需要的数据，再将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)； Scrapy 工作机制我们对 Scrapy 有了大体上的认识。接下来我们了解下 Scrapy 内部的工作流程。同样先放出一张图，然后我再细细讲解。 当引擎(Engine) 收到 Spider 发送过来的 url 主入口地址（其实是一个 Request 对象, 因为 Scrapy 内部是用到 Requests 请求库），Engine 会进行初始化操作。 Engine 请求调度器（Scheduler），让 Scheduler 调度出下一个 url 给 Engine。 Scheduler 返回下一个 url 给 Engine。 Engine 将 url通过下载中间件(请求(request)方向)转发给下载器(Downloader)。 一旦页面下载完毕，Downloader 生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给 Engine 引擎将从下载器中接收到 Response 发送给Spider处理。 Spider 处理 Response 并返回爬取到的 Item 及新的 Request 给引擎。 Engine 将 Spider 返回的爬取到的 Item 转发给Item Pipeline，顺便也将将 Request 给调度器。 重复（第2步)直到调度器中没有更多地request，引擎关闭该网站。","link":"/916.html"},{"title":"学会运用爬虫框架 Scrapy (二)","text":"上篇文章介绍了爬虫框架 Scrapy 如何安装，以及其特性、架构、数据流程。相信大家已经对 Scrapy 有人了初步的认识。本文是 Scrapy 系列文章的第二篇，主要通过一个实例讲解 scrapy 的用法。 选取目标网络爬虫，顾名思义是对某个网站或者系列网站，按照一定规则进行爬取信息。爬取程序的首要工作当然是选定爬取目标。本次爬取目标选择是V电影，网址是http://www.vmovier.com/。爬取内容是[最新推荐]栏目的前15条短视频数据信息。具体信息包括封面、标题、详细说明以及视频播放地址。 定义 Item为什么将爬取信息定义清楚呢？因为接下来 Item 需要用到。在 Item.py 文件中，我们以类的形式以及 Field 对象来声明。其中 Field 对象其实是一个字典类型，用于保存爬取到的数据。而定义出来的字段，可以简单理解为数据库表中的字段，但是它没有数据类型。Item 则复制了标准的 dict API，存放以及读取跟字典没有差别。 V电影的 Item，我们可以这样定义： 12345678910111213import scrapyclass ScrapyDemoItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 封面 cover = scrapy.Field() # 标题 title = scrapy.Field() # 简述 dec = scrapy.Field() # 播放地址 playUrl = scrapy.Field() 编写 SpiderSpider 目录是我们爬虫程序爬取网站以及提取信息的模块。我们首先在目录下新建一个名为 VmoiveSpider 的文件。同时，该类继承scrapy.Spider。 这里我们用到的scrapy.spider.Spider 是 Scrapy 中最简单的内置 spider。继承 spider 的类需要定义父类中的属性以及实现重要的方法。 name 这个属性是非常重要的，所以必须定义它。定义 name 目的是为爬虫程序命名。因此，还要保持 name 属性是唯一的。它是 String 类型，我们在 VmoiveSpider 可以定义： 1name = 'vmoive' start_urls start_urls 是 Url 列表，也是必须被定义。可以把它理解为存放爬虫程序的主入口 url 地址的容器。 allowed_domains 可选字段。包含了spider允许爬取的域名(domain)列表(list)。 当 OffsiteMiddleware 启用时， 域名不在列表中的URL不会被跟进。根据 V 电影的 url 地址，我们可以这样定义： 1allowed_domains = ['vmovier.com'] parse(response) parser 方法是Scrapy处理下载的response的默认方法。它同样必须被实现。parse 主要负责处理 response 并返回处理的数据以及跟进的URL。该方法及其他的Request回调函数必须返回一个包含 Request 及(或) Item 的可迭代的对象。 在 scrapy_demo/sipders/VmoiveSpider 的完整代码如下： 12345678910111213141516171819202122232425262728#!/usr/bin/env python# coding=utf-8import scrapyfrom scrapy_demo.items import ScrapyDemoItemclass VmoiveSpider(scrapy.Spider): # name是spider最重要的属性, 它必须被定义。同时它也必须保持唯一 name = 'vmoive' # 可选定义。包含了spider允许爬取的域名(domain)列表(list) allowed_domains = ['vmovier.com'] start_urls = [ 'http://www.vmovier.com/' ] def parse(self, response): self.log('item page url is ==== ' + response.url) moivelist = response.xpath(\"//li[@class='clearfix']\") for m in moivelist: item = ScrapyDemoItem() item['cover'] = m.xpath('./a/img/@src')[0].extract() item['title'] = m.xpath('./a/@title')[0].extract() item['dec'] = m.xpath(\"./div/div[@class='index-intro']/a/text()\")[0].extract() print(item) yield item 运行程序在项目目录下打开终端，并执行以下命令。我们没有pipelines.py中将爬取结果进行存储，所以我们使用 scrapy 提供的导出数据命令，将 15 条电影信息导出到名为 items.json 文件中。其中 vmoive 为刚才在 VmoiveSpider 中定义的 name 属性的值。 1scrapy crawl vmoive -o items.json 运行的部分结果如下： 12345{'cover': 'http://cs.vmoiver.com/Uploads/cover/2017-09-08/59b25a504e4e0_cut.jpeg@600w_400h_1e_1c.jpg', 'dec': '15年过去了，但我依然有话说', 'title': '灾难反思纪录短片《“911”之殇》' } 深究在阅读上述代码过程中，大家可能会有两个疑问。第一，为什么要在 xpath 方法后面添加[0]？ 第二，为什么要在 [0] 后面添加 extract()方法 ? 请听我慢慢道来。 1) 添加个[0], 因为 xpath() 返回的结果是列表类型。我以获取标题内容为例子讲解不添加[0]会出现什么问题。那么代码则变为 1m.xpath('./a/@title').extract() 运行结果会返回一个列表，而不是文本信息。 1['灾难反思纪录短片《“911”之殇》'] 2）这里涉及到内建选择器 Selecter 的知识。extract()方法的作用是串行化并将匹配到的节点返回一个unicode字符串列表。看了定义，是不是更加懵逼了。那就看下运行结果来压压惊。不加上 extract() 的运行结果如下： 1&lt;Selector xpath='./a/@title' data='灾难反思纪录短片《“911”之殇》'&gt; 进阶上述代码只是在 V电影主页中提取信息，而进入电影详情页面中匹配搜索信息。因此，我们是获取不到电影的播放地址的。如何搞定这难题？我们可以在 parse 方法中做文章。parse() 前文提到它必须返回一个 Reuqest 对象或者 Item。再者， Request 中就包含 url。换句话说，我们只有获取到电影详情页的 url 地址，并在传递给返回的 Request 对象中。 因此，代码可以这么改进： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/usr/bin/env python# coding=utf-8import scrapyfrom scrapy_demo.items import ScrapyDemoItemclass VmoiveSpider(scrapy.Spider): # name是spider最重要的属性, 它必须被定义。同时它也必须保持唯一 name = 'vmoive' # 可选定义。包含了spider允许爬取的域名(domain)列表(list) allowed_domains = ['vmovier.com'] start_urls = [ 'http://www.vmovier.com/' ] def parse(self, response): self.log('item page url is ==== ' + response.url) moivelist = response.xpath(\"//li[@class='clearfix']\") for m in moivelist: item = ScrapyDemoItem() item['cover'] = m.xpath('./a/img/@src')[0].extract() item['title'] = m.xpath('./a/@title')[0].extract() item['dec'] = m.xpath(\"./div/div[@class='index-intro']/a/text()\")[0].extract() # print(item) # 提取电影详细页面 url 地址 urlitem = m.xpath('./a/@href')[0].extract() url = response.urljoin(urlitem) # 如果你想将上面的 item 字段传递给 parse_moive, 使用 meta 参数 yield scrapy.Request(url, callback=self.parse_moive, meta={ 'cover':item['cover'], 'title': item['title'], 'dec': item['dec'], }) def parse_moive(self, response): item = ScrapyDemoItem() item['cover'] = response.meta['cover'] item['title'] = response.meta['title'] item['dec'] = response.meta['dec'] item['playUrl'] = response.xpath(\"//div[@class='p00b204e980']/p/iframe/@src\")[0].extract() yield item 再次运行程序，查看运行结果。 数据持久化在实际生产中，我们很少把数据导出到 json 文件中。因为后期维护、数据查询、数据修改都是一件麻烦的事情。我们通常是将数据保存到数据库中。 我们先定义并创建数据库表 12345678DROP TABLE IF EXISTS vmoiveCREATE TABLE vmoive( id INT(6) NOT NULL AUTO_INCREMENT PRIMARY KEY , cover VARCHAR(255), title VARCHAR(255), mdec VARCHAR(255), playUrl VARCHAR(255)) DEFAULT CHARSET=utf8; 在 settings 文件中增加数据库的配置 123456789101112# Configure item pipelines 这里默认是注释的# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = { 'scrapy_demo.pipelines.ScrapyDemoPipeline': 300, # 保存到 mysql 数据库中}# Mysql 配置信息# 根据你的环境修改MYSQL_HOST = '127.0.0.1'MYSQL_DBNAME = 'vmoive' # 数据库名MYSQL_USER = 'root' # 数据库用户MYSQL_PASSWORD = '123456' # 数据库密码 在 scrapy 中，我们要在 pipeline 文件中编写处理数据存储的代码。 12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*-import pymysqlfrom scrapy_demo import settingsclass ScrapyDemoPipeline(object): def __init__(self,): self.conn = pymysql.connect( host = settings.MYSQL_HOST, db = settings.MYSQL_DBNAME, user = settings.MYSQL_USER, passwd = settings.MYSQL_PASSWORD, charset = 'utf8', # 编码要加上，否则可能出现中文乱码问题 use_unicode = False ) self.cursor = self.conn.cursor() # pipeline 默认调用 def process_item(self, item, spider): # 调用插入数据的方法 self.insertData(item) return item # 插入数据方法 def insertData(self, item): sql = \"insert into vmoive(cover, title, mdec, playUrl) VALUES(%s, %s, %s, %s);\" params = (item['cover'], item['title'], item['dec'], item['playUrl']) self.cursor.execute(sql, params) self.conn.commit()","link":"/917.html"},{"title":"学会运用爬虫框架 Scrapy (三)","text":"上篇文章介绍 Scrapy 框架爬取网站的基本用法。但是爬虫程序比较粗糙，很多细节还需打磨。本文主要是讲解 Scrapy 一些小技巧，能让爬虫程序更加完善。 设置 User-agentScrapy 官方建议使用 User-Agent 池, 轮流选择其中一个常用浏览器的 User-Agent来作为 User-Agent。scrapy 发起的 http 请求中 headers 部分中 User-Agent 字段的默认值是Scrapy/VERSION (+http://scrapy.org)，我们需要修改该字段伪装成浏览器访问网站。 1) 同样在 setting.py 中新建存储 User-Agent 列表, 1234567891011121314151617181920212223242526272829303132333435UserAgent_List = [ \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2224.3 Safari/537.36\", \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\", \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 4.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\", \"Mozilla/5.0 (X11; OpenBSD i386) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1944.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.3319.102 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2309.372 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2117.157 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1866.237 Safari/537.36\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.137 Safari/4E423F\", \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1\", \"Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10; rv:33.0) Gecko/20100101 Firefox/33.0\", \"Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/31.0\", \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20130401 Firefox/31.0\", \"Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20100101 Firefox/31.0\", \"Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16\", \"Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14\", \"Mozilla/5.0 (Windows NT 6.0; rv:2.0) Gecko/20100101 Firefox/4.0 Opera 12.14\", \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0) Opera 12.14\", \"Opera/9.80 (Windows NT 5.1; U; zh-sg) Presto/2.9.181 Version/12.00\"] 2) 在 middlewares.py 文件中新建一个名为RandomUserAgentMiddleware的代理中间层类 123456789101112import randomfrom scrapy_demo.settings import UserAgent_Listclass RandomUserAgentMiddleware(object): '''动态随机设置 User-agent''' def process_request(self, request, spider): ua = random.choice(UserAgent_List) if ua: request.headers.setdefault('User-Agent', ua) print(request.headers) 3) 在 settings.py 中配置 RandomUserAgentMiddleware , 激活中间件 123456789DOWNLOADER_MIDDLEWARES = { # 第二行的填写规则 # yourproject.middlewares(文件名).middleware类 # 设置 User-Agent 'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None, 'scrapy_demo.middlewares.RandomUserAgentMiddleware': 400, # scrapy_demo 是你项目的名称} 禁用cookies有些站点会使用 cookies 来发现爬虫的轨迹。因此，我们最好禁用 cookies 在 settings.py 文件中新增以下配置。 123# 默认是被注释的, 也就是运行使用 cookies# Disable cookies (enabled by default)COOKIES_ENABLED = False 设置下载延迟当 scrapy 的下载器在下载同一个网站下一个页面前需要等待的时间。我们设置下载延迟, 可以有效避免下载器获取到下载地址就立刻执行下载任务的情况发生。从而可以限制爬取速度, 减轻服务器压力。 在 settings.py 文件中新增以下配置。 1234567# 默认是被注释的# Configure a delay for requests for the same website (default: 0)# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay# See also autothrottle settings and docsDOWNLOAD_DELAY = 3 # 单位是秒, 上述设置是延迟 3s。 # 同时还支持设置小数, 例 0.3, 延迟 300 ms 设置代理有些网站设置反爬虫机制，这使得我们的爬虫程序可能爬到一定数量网页就爬取不下去了。我们需要装饰下爬虫，让它访问网站行为更像类人行为。使用 IP 代理池能突破大部分网站的限制。 1) 我们可以通过国内一些知名代理网站(例如：迅代理、西刺代理)获取代理服务器地址。 我将自己收集一些代理地址以列表形式保存到 settings.py 文件中 1234567891011# 代理地址具有一定的使用期限, 不保证以下地址都可用。PROXY_LIST = [ \"https://175.9.77.240:80\", \"http://61.135.217.7:80\", \"http://113.77.101.113:3128\" \"http://121.12.42.180:61234\", \"http://58.246.59.59:8080\", \"http://27.40.144.98:808\", \"https://119.5.177.167:4386\", \"https://210.26.54.43:808\",] 2) 在 middlewares.py 文件中新建一个名为ProxyMiddleware的代理中间层类 12345678910import randomfrom scrapy_demo.settings import PROXY_LISTclass ProxyMiddleware(object): # overwrite process request def process_request(self, request, spider): # Set the location of the proxy # request.meta['proxy'] = \"https://175.9.77.240:80\" request.meta['proxy'] = random.choice(PROXY_LIST) 3) 在 settings.py 文件中增加代理配置： 123456789DOWNLOADER_MIDDLEWARES = { # 第二行的填写规则 # yourproject.middlewares(文件名).middleware类 # 设置代理 'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 110, 'scrapy_demo.middlewares.ProxyMiddleware': 100, # scrapy_demo 是你项目的名称} 除此之外，如果你比较狠的话，可以采用 VPN + Tor 方式来突破反爬虫机制。 减小下载超时如果您对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)， 减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。 在 settings.py 文件中增加配置： 1DOWNLOAD_TIMEOUT = 15 页面跟随规则在爬取网站时，可能一些页面是我们不想爬取的。如果使用 最基本的 Spider，它还是会将这些页面爬取下来。因此，我们需要使用更加强大的爬取类CrawlSpider。 我们的爬取类继承 CrawlSpider，必须新增定义一个 rules 属性。rules 是一个包含至少一个 Rule（爬取规则）对象的 list。 每个 Rule 对爬取网站的动作定义了特定表现。CrawlSpider 也是继承 Spider 类，所以具有Spider的所有函数。 Rule 对象的构造方法如下： 1Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None) 我们在使用 Rule 一般只会用到前面几个参数，它们作用如下： link_extractor： 它是一个 Link Extractor 对象。 其定义了如何从爬取到的页面提取链接。link_extractor既可以自己定义，也可以使用已有LinkExtractor类，主要参数为： allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。 deny：与这个正则表达式(或正则表达式列表)不匹配的 Url 一定不提取。 allow_domains：会被提取的链接的domains。 deny_domains：一定不会被提取链接的domains。 restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。还有一个类似的restrict_css callback：从 link_extractor 中每获取到链接时将会调用该函数。它指定一个回调方法。会返回一个包含 Item 对象的列表。 follow：它 是一个布尔(boolean)值，指定了根据该规则从 response 提取的链接是否需要跟进。 如果 callback 为None， follow 默认设置为 True ，否则默认为 False 。 process_links：从link_extractor中获取到链接列表时将会调用该函数。它同样需要指定一个方法，该方法主要用来过滤 Url。 我以爬取豆瓣电影 Top 250 页面为例子进行讲解如何利用 rules 进行翻页爬取。 在页面的底部，有这样的分页。我们想通过抓取翻页 url 进行下一个页面爬取。 通过分析页面可知，链接的规则是 1https://movie.douban.com/top250?start=当前分页第一个电影序号&amp;filter=分页数 我使用 xpath 来匹配，当然你也可以使用正则表达式或者 CSS 选择器。rules 可以这样定义： 1234567rules = ( Rule(LinkExtractor(allow=(), restrict_xpaths=('//div[@class=\"paginator\"]',)), follow=True, callback='parse_item', process_links='process_links', ),) 完整的 spider 代码如下： 12345678910111213141516171819202122# -*- coding: utf-8 -*-from scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorclass DoubanTop250(CrawlSpider): name = 'movie_douban' allowed_domains = ['douban.com'] start_urls = [\"https://movie.douban.com/top250\"] rules = ( Rule(LinkExtractor(allow=(), restrict_xpaths=('//div[@class=\"paginator\"]',)), follow=True, callback='parse_item', process_links='process_links', ), ) def parse_item(self, response): # 解析 response, 将其转化为 item yield item 另外，LinkExtractor 参数中的 allow() 和 deny() ，我们也是经常使用到。规定爬取哪些页面是否要进行爬取。 7 动态创建Item类对于有些应用，item的结构由用户输入或者其他变化的情况所控制。我们可以动态创建class。 1234567from scrapy.item import DictItem, Fielddef create_item_class(class_name, field_list): fields = { field_name: Field() for field_name in field_list } return type(class_name, (DictItem,), {'fields': fields})","link":"/918.html"},{"title":"学会运用爬虫框架 Scrapy (四)  —— 高效下载图片","text":"爬虫程序爬取的目标通常不仅仅是文字资源，经常也会爬取图片资源。这就涉及如何高效下载图片的问题。这里高效下载指的是既能把图片完整下载到本地又不会对网站服务器造成压力。也许你会这么做，在 pipeline 中自己实现下载图片逻辑。但 Scrapy 提供了图片管道ImagesPipeline，方便我们操作下载图片。 为什么要选用 ImagesPipeline ？ImagesPipeline 具有以下特点： 将所有下载的图片转换成通用的格式（JPG）和模式（RGB） 避免重新下载最近已经下载过的图片 缩略图生成 检测图像的宽/高，确保它们满足最小限制 具体实现定义字段在 item.py 文件中定义我们两个字段image_urls 和images_path 12345678import scrapyclass PicsDownloadItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() image_urls = scrapy.Field() # 图片的下载地址， 该字段是存储图片的列表 image_path = scrapy.Field() # 图片本地存储路径(相对路径) 编写 spider我以爬取 freebuf 首页部分图片为例子讲解。具体代码如下： 12345678910111213141516171819202122import scrapyfrom pics_download.items import PicsDownloadItemclass freebuf_pic_spider(scrapy.Spider): name = 'freebuf' allowed_domains = ['freebuf.com'] start_urls = [ 'http://www.freebuf.com/' ] def parse(self, response): self.log(response.headers) # 获取 freebuf 首页所有的图片, 以列表形式保存到 image_urls 字段中。 piclist = response.xpath(\"//div[@class='news-img']/a/img/@src\").extract() if piclist: item = PicsDownloadItem() item['image_urls'] = piclist yield item 实现 Pipeline我新建一个名为PicsDownloadPipeline的类。需要注意一点的是： Scrapy 默认生成的类是继承Object， 要将该类修改为继承ImagesPipeline。然后实现get_media_requests和item_completed这两个函数。 get_media_requests(item, info) ImagePipeline 根据 image_urls 中指定的 url 进行爬取，可以通过 get_media_requests 为每个 url 生成一个 Request。具体实现如下： 123def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) item_completed(self, results, item, info) 当一个单独项目中的所有图片请求完成时，该方法会被调用。处理结果会以二元组的方式返回给 item_completed() 函数。这个二元组定义如下：(success, image_info_or_failure)其中，第一个元素表示图片是否下载成功；第二个元素是一个字典，包含三个属性： 1) url - 图片下载的url。这是从 get_media_requests() 方法返回请求的url。2) path - 图片存储的路径（类似 IMAGES_STORE）3) checksum - 图片内容的 MD5 hash 具体实现如下： 1234567def item_completed(self, results, item, info): # 将下载的图片路径（传入到results中）存储到 image_paths 项目组中，如果其中没有图片，我们将丢弃项目: image_path = [x['path'] for ok, x in results if ok] if not image_path: raise DropItem(\"Item contains no images\") item['image_path'] = image_path return item 综合起来，PicsDownloadPipeline 的实现下载图片逻辑的代码如下： 1234567891011121314151617import scrapyfrom scrapy.exceptions import DropItemfrom scrapy.pipelines.images import ImagesPipelineclass PicsDownloadPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) def item_completed(self, results, item, info): # 将下载的图片路径（传入到results中）存储到 image_paths 项目组中，如果其中没有图片，我们将丢弃项目: image_path = [x['path'] for ok, x in results if ok] if not image_path: raise DropItem(\"Item contains no images\") item['image_path'] = image_path return item 配置设置在 setting.py 配置存放图片的路径以及自定义下载的图片管道。 123456789# 设置存放图片的路径IMAGES_STORE = 'D:\\\\freebuf'# 配置自定义下载的图片管道， 默认是被注释的ITEM_PIPELINES = { # 第二行的填写规则 # yourproject.middlewares(文件名).middleware类 'pics_download.pipelines.PicsDownloadPipeline': 300, # pics_download 是你项目的名称} 运行程序在 Scrapy 项目的根目录下，执行以下命令： 1scrapy crawl freebuf # freebuf 是我们在 spider 定义的 name 属性 如果你使用的 Python 版本是 3.x 的，可能会报出以下的错误。 123 File \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\pipelines\\images.py\", line 15, in &lt;module&gt; from PIL import ImageModuleNotFoundError: No module named 'PIL' 这是因为 Scrapy 框架用到这个Python Imaging Library (PIL)图片加载库，但是这个库只支持 2.x 版本，所以会运行出错。对于使用 Python 3.x 版本的我们，难道就束手无策？Scrapy 的开发者建议我们使用更好的图片加载库Pillow。为什么说更好呢？一方面是兼容了 PIL，另一方面在该库支持生成缩略图。 因此，我们安装 Pillow 就能解决运行报错的问题。具体安装 Pillow命令如下： 1234pip install pillow # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install pillow 安装之后，重新运行爬虫程序。Scrapy 会运行结果中显示我们定义的image_urls 和images_path字段。 运行结果我们会发现在 D 盘有个名为freebuf的文件夹。在该文件夹中有个full文件夹，里面存放我们刚才爬取到的图片。 如果有在 setting.py 文件中设置生成缩略图。 1234IMAGES_THUMBS = { 'small': (50, 50), # (宽， 高) 'big': (270, 270),} 那么到时候，与full同级的目录下会多出个thumbs文件夹。里面会有两个文件夹small和big，分别对应小分辨率的图片和大分辨率的图片。 ##优化 避免重复下载在 setting.py 中新增以下配置可以避免下载最近已经下载的图片。 12# 90天的图片失效期限IMAGES_EXPIRES = 90 设置该字段，对于已经完成爬取的网站，重新运行爬虫程序。爬虫程序不会重新下载新的图片资源。 自动限速（AutoTrottle）下载图片是比较消耗服务器的资源以及流量。如果图片资源比较大，爬虫程序一直在下载图片。这会对目标网站造成一定的影响。同时，爬虫有可能遭到封杀的情况。 因此，我们有必要对爬虫程序做爬取限速处理。Scrapy 已经为我们提供了AutoTrottle功能。 只要在 setting.py 中开启AutoTrottle功能并配置限速算法即可。我采用默认的配置，具体配置如下： 12345678910# 启用AutoThrottle扩展AUTOTHROTTLE_ENABLED = True# 初始下载延迟(单位:秒)AUTOTHROTTLE_START_DELAY = 5# 在高延迟情况下最大的下载延迟(单位秒)AUTOTHROTTLE_MAX_DELAY = 60# 设置 Scrapy应该与远程网站并行发送的平均请求数, 目前是以1个并发请求数AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0# 启用AutoThrottle调试模式#AUTOTHROTTLE_DEBUG = False 值得注意的是，启用AutoThrottle扩展时，仍然受到DOWNLOAD_DELAY（下载延迟）和CONCURRENT_REQUESTS_PER_DOMAIN（对单个网站进行并发请求的最大值）以及CONCURRENT_REQUESTS_PER_IP（对单个IP进行并发请求的最大值）的约束。","link":"/919.html"},{"title":"学会运用爬虫框架 Scrapy (五)  —— 部署爬虫","text":"本文是 Scrapy 爬虫系列的最后一篇文章。主要讲述如何将我们编写的爬虫程序部署到生产环境中。我们使用由 scrapy 官方提供的爬虫管理工具 scrapyd 来部署爬虫程序。 为什么使用 scrapyd?一是它由 scrapy 官方提供的，二是我们使用它可以非常方便地运用 JSON API来部署爬虫、控制爬虫以及查看运行日志。 使用 scrapyd原理选择一台主机当做服务器，安装并启动 scrapyd 服务。再这之后，scrapyd 会以守护进程的方式存在系统中，监听爬虫地运行与请求，然后启动进程来执行爬虫程序。 安装 scrapyd使用 pip 能比较方便地安装 scrapyd。 1234pip install scrapyd # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install scrapyd 启动 scrapyd在终端命令行下以下命令来启动服务： 1scrapyd 启动服务结果如下： scrapyd 也提供了 web 的接口。方便我们查看和管理爬虫程序。默认情况下 scrapyd 监听 6800 端口，运行 scrapyd 后。在本机上使用浏览器访问 http://localhost:6800/地址即可查看到当前可以运行的项目。 项目部署直接使用 scrapyd-client 提供的 scrapyd-deploy 工具 原理scrapyd 是运行在服务器端，而 scrapyd-client 是运行在客户端。客户端使用 scrapyd-client 通过调用 scrapyd 的 json 接口来部署爬虫项目。 安装 scrapyd-client在终端下运行以下安装命令： 1234pip install scrapyd-client # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install scrapyd-client 配置项目的服务器信息修改工程目录下的 scrapy.cfg 文件。 12345678910111213# Automatically created by: scrapy startproject## For more information about the [deploy] section see:# https://scrapyd.readthedocs.org/en/latest/deploy.html[settings]default = demo.settings # test 为 project 的名称, 默认创建的[deploy:server]# 默认情况下并没有 server，它只是为服务器指定一个名字url = http://localhost:6800/ # 这是部署到本地, 如果你使用其他机器远程部署程序, 需将本地地址换成服务器的 IP 地址。默认是注释的project = test # test 为 project 的名称, 默认创建的 如果你服务器有配置 HTTP basic authentication 验证，那么需要在 scrapy.cfg 文件增加用户名和密码。这是用于登录服务器用的。 123456789[settings]default = demo.settings # demo 为 project 的名称, 默认创建的[deploy:server]# 默认情况下并没有 server，它只是为服务器指定一个名字url = http://192.168.161.129:6800/ # 这是部署到远程服务器project = demo # demo 为 project 的名称, 默认创建的username = monkeypassword = 123456 # 如果不需要密码可以不写 部署爬虫程序在爬虫项目根目录下执行下面的命令: 1scrapyd-deploy &lt;target&gt; -p &lt;project&gt; 其中 target 为上一步配置的服务器名称，project 为项目名称，可以根据实际情况自己指定。 我指定 target 为 server，project 为 demo，所以我要执行的命令如下： 1scrapyd-deploy server -p demo 部署操作会打包你的当前项目，如果当前项目下有setup.py文件，就会使用它，没有的会就会自动创建一个。(如果后期项目需要打包的话，可以根据自己的需要修改里面的信息，也可以暂时不管它). 从返回的结果里面，我们可以看到部署的状态，项目名称，版本号和爬虫个数，以及当前的主机名称. 运行结果如下： 12345$ scrapyd-deploy server -p demoPacking version 1507376760Deploying to project \"demo\" in http://localhost:6800/addversion.jsonServer response (200):{\"status\": \"ok\", \"project\": \"demo\", \"version\": \"1507376760\", \"spiders\": 1, \"node_name\": \"james-virtual-machine\"} 使用以下命令检查部署爬虫结果： 1scrapyd-deploy -l 服务器名称 我指定服务器名称为 server，所以要执行命令如下： 123$ scrapyd-deploy -L severdefaultdemo 刷新 http://localhost:6800/ 页面, 也可以看到Available projects: demo的字样。 使用 API 管理爬虫scrapyd 的 web 界面比较简单，主要用于监控，所有的调度工作全部依靠接口实现。官方推荐使用 curl 来管理爬虫。所以要先安装 curl。 windows 用户可以到该网站https://curl.haxx.se/download.html下载 curl 安装包进行安装。 ubuntu/Mac 用户直接使用命令行安装即可。 开启爬虫 schedule在爬虫项目的根目录下，使用终端运行以下命令： 1curl http://localhost:6800/schedule.json -d project=demo -d spider=demo_spider 成功启动爬虫结果如下： 12curl http://localhost:6800/schedule.json -d project=tutorial -d spider=tencent{\"status\": \"ok\", \"jobid\": \"94bd8ce041fd11e6af1a000c2969bafd\", \"node_name\": \"james-virtual-machine\"} 取消爬虫1curl http://localhost:6800/cancel.json -d project=demo -d job=94bd8ce041fd11e6af1a000c2969bafd 列出项目1curl http://localhost:6800/listprojects.json 列出爬虫、版本、job 信息1curl http://localhost:6800/listspiders.json?project=demo 删除爬虫项目1curl http://localhost:6800/delproject.json -d project=demo","link":"/1020.html"},{"title":"爬虫与反爬虫的博弈","text":"今天猴哥给大家说说爬虫与反爬虫的博弈。 前言近来这两三个月，我陆续将自己学到的爬虫技术分享出来。以标准网络库 urllib 的用法起笔，接着介绍各种内容提供工具，再到后续的 scrapy 爬虫框架系列。我的爬虫分享之旅已经接近尾声了。本文就来聊聊如何防止爬虫被 ban 以及如何限制爬虫。 介绍我们编写的爬虫在爬取网站的时候，要遵守 robots 协议，爬取数据做到“盗亦有道”。在爬取数据的过程中，不要对网站的服务器造成压力。尽管我们做到这么人性化。对于网络维护者来说，他们还是很反感爬虫的。因为爬虫的肆意横行意味着自己的网站资料泄露，甚至是自己刻意隐藏在网站的隐私的内容也会泄露。所以，网站维护者会运用各种方法来拦截爬虫。 攻防战 场景一 防：检测请求头中的字段，比如：User-Agent、referer等字段。 攻：只要在 http 请求的 headers 中带上对于的字段即可。下图中的七个字段被大多数浏览器用来初始化所有网络请求。建议将以下所有字段都带上。 场景二 防：后台对访问的 IP 进行统计，如果单个 IP 访问超过设定的阈值，给予封锁。虽然这种方法效果还不错， 但是其实有两个缺陷。 一个是非常容易误伤普通用户， 另一个就是 IP 其实不值钱， 各种代理网站都有出售大量的 IP 代理地址。 所以建议加大频率周期,每小时或每天超过一定次数屏蔽 IP 一段时间（不提示时间）。 攻：针对这种情况，可通过使用代理服务器解决。同时，爬虫设置下载延迟，每隔几次请求，切换一下所用代理的IP地址。 场景三 防：后台对访问进行统计， 如果单个 userAgent 访问超过阈值， 予以封锁。这种方法拦截爬虫效果非常明显，但是杀伤力过大，误伤普通用户概率非常高。所以要慎重使用。攻：收集大量浏览器的 userAgent 即可。 场景四 防：网站对访问有频率限制，还设置验证码。增加验证码是一个既古老又相当有效果的方法。能够让很多爬虫望风而逃。而且现在的验证码的干扰线, 噪点都比较多，甚至还出现了人类肉眼都难以辨别的验证码（12306 购票网站）。攻：python+tesseract 验证码识别库模拟训练，或使用类似 tor 匿名中间件（广度遍历IP） 场景五 防：网站页面是动态页面，采用 Ajax 异步加载数据方式来呈现数据。这种方法其实能够对爬虫造成了绝大的麻烦。 攻：首先用 Firebug 或者 HttpFox 对网络请求进行分析。如果能够找到 ajax 请求，也能分析出具体的参数和响应的具体含义。则直接模拟相应的http请求，即可从响应中得到对应的数据。这种情况，跟普通的请求没有什么区别。 能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把 ajax 请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求，请看场景六。 场景六 防：基于 JavaScript 的反爬虫手段，主要是在响应数据页面之前，先返回一段带有JavaScript 代码的页面，用于验证访问者有无 JavaScript 的执行环境，以确定使用的是不是浏览器。例如淘宝、快代理这样的网站。 这种反爬虫方法。通常情况下，这段JS代码执行后，会发送一个带参数key的请求，后台通过判断key的值来决定是响应真实的页面，还是响应伪造或错误的页面。因为key参数是动态生成的，每次都不一样，难以分析出其生成方法，使得无法构造对应的http请求。 攻：采用 selenium+phantomJS 框架的方式进行爬取。调用浏览器内核，并利用phantomJS 执行 js 来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。","link":"/1021.html"},{"title":"Python 绘图,我只用 Matplotlib(一)","text":"当我们的爬虫程序已经完成使命，帮我们抓取大量的数据。你内心也许会空落落的。或许你会疑惑，自己抓取这些数据有啥用？如果要拿去分析，那要怎么分析呢？ 说到数据分析，Python 完全能够胜任这方面的工作。Python 究竟如何在数据分析领域做到游刃有余？因为它有“四板斧”，分别是Matplotlib、NumPy、SciPy/Pandas。Matplotlib 是画图工具，NumPy 是矩阵运算库，SciPy 是数学运算工具，Pandas 是数据处理的工具。 为什么选择 Matplotlib？Python 有很多强大的画图库，为什么我偏偏独爱 Maplotlib？我先买个关子，先来看看还有哪些库。 SeabornSeaborn 是一个基于 Matplotlib 的高级可视化效果库， 偏向于统计作图。因此，针对的点主要是数据挖掘和机器学习中的变量特征选取。相比 Matplotlib ，它语法相对简化些，绘制出来的图不需要花很多功夫去修饰。但是它绘图方式比较局限，不过灵活。 BokehBokeh 是基于 javascript 来实现交互可视化库，它可以在WEB浏览器中实现美观的视觉效果。但是它也有明显的缺点。其一是版本时常更新，最重要的是有时语法还不向下兼容。这对于我们来说是噩梦。其二是语法晦涩，与 matplotlib做比较，可以说是有过之而无不及。 ggplotggplot 是 yhat 大神基于 R 语言的 ggplot2 制作的 python 版本库。 如果你使用 R 语言的话，ggplot2 可以算是必不可少的工具。所以，很多人都推荐使用该库。不过可惜的是，yhat 大神已经停止维护该库了。 PlotlyPlotly 也是一个做可视化交互的库。它不仅支持 Python 还支持 R 语言。Plotly 的优点是能提供 WEB 在线交互，配色也真心好看。如果你是一名数据分析师，Plotly 强大的交互功能能助你一臂之力完成展示。 MapboxMapbox 使用处理地理数据引擎更强的可视化工具库。如果你需要绘制地理图，那么它值得你信赖。 总之， Python 绘图库众多，各有特点。但是 Maplotlib 是最基础的 Python 可视化库。如果你将学习 Python 数据可视化。那么 Maplotlib 是非学不可，然后再学习其他库做纵横向的拓展。 Matplotlib 能绘制什么图？Matiplotlib 非常强大，所以最基本的图表自然不在话下。例如说：直线图 曲线图 柱状图 直方图 饼图 散点图 只能绘制这些最基础的图？显示是不可能的，还能绘制些高级点的图例如：高级点的柱状图 等高线图 类表格图形 不仅仅只有这些，还能绘制 3D 图形。例如三维柱状图 3D 曲面图 因此，Matplotlib 绘制的图种类能够满足我们做数据分析了。 安装 Matplotlib看到这里，你是否惊叹不已，很很迫不及待地想学习 Matplotlib。而工欲善其事，必先利其器。我们先来学习如何安装 Matplotlib。其实也是很简单，我们借助 pip 工具来安装。 在终端执行以下命令来安装 Matplotlib 1234pip install Matplotlib # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install Matplotlib","link":"/1022.html"},{"title":"Python 绘图,我只用 Matplotlib(二)","text":"上篇文章，我们了解到 Matplotlib 是一个风格类似 Matlab 的基于 Python 的绘图库。它提供了一整套和matlab相似的命令API，十分适合交互式地进行制图。而且我们也可以方便地将它作为绘图控件，嵌入GUI应用程序中。本文主要走进 Matplotlib 的世界，初步学会绘制图形。 基础知识在学习绘制之前，先来了解下 Matplotlib 基础概念。 库我们绘制图形主要用到两个库，matplotlib.pyplot和numpy。在编码过程中，这两个库的使用频率较高，而这两个库的名字较长。这难免会给我们带来不便。所以我们一般给其设置别名， 大大减少重复性工作量。具体如下代码： 12import matplotlib.pyplot as plt # 导入模块 matplotlib.pyplot，并简写成 plt import numpy as np # 导入模块 numpy，并简写成 np numpy 是 Python 用于数学运算的库，它是在安装 matplotlib 时候顺带安装的。pyplot 是 matplotlib 一个子模块，主要为底层的面向对象的绘图库提供状态机界面。状态机隐式地自动创建数字和坐标轴以实现所需的绘图。 matplotlib 中的所有内容都按照层次结果进行组织。顶层就是由 pyplot 提供的 matplotlib “状态机环境”。基于这个状态机环境，我们就可以创建图形。 图形组成标签我在 matplotlib 官网上找图像组件说明图并在上面增加中文翻译。通过这张图，我们对 matplotlib 整体地认识。 接下来，我主要讲解 matplotlib 中几个重要的标签。 Figure Figure 翻译成中文是图像窗口。Figure 是包裹 Axes、tiles、legends 等组件的最外层窗口。它其实是一个 Windows 应用窗口 。Figure 中最主要的元素是 Axes（子图）。一个 Figure 中可以有多个子图，但至少要有一个能够显示内容的子图。 Axes Axes 翻译成中文是轴域/子图。Axes 是带有数据的图像区域。从上文可知，它是位于 Figure 里面。那它和 Figure 是什么关系？这里可能文字难以表述清楚，我以图说文。用两图带你彻底弄清它们的关系。 在看运行结果之前，我先呈上代码给各位看官品尝。 12345fig = plt.figure() # 创建一个没有 axes 的 figurefig.suptitle('No axes on this figure') # 添加标题以便我们辨别fig, ax_lst = plt.subplots(2, 2) # 创建一个以 axes 为单位的 2x2 网格的 figure plt.show() 根据运行结果图，我们不难看出。左图的 Figure1 中没有 axes，右图的 Figure2 中有 4 个 axes。因此，我们可以将 Axes 理解为面板，而面板是覆在窗口(Figure) 上。 Axis Axis 在中文的意思是轴。官网文档对 Axis 定义解释不清楚，让我们看得云里雾里的。如果你有留意前文的组成说明图，可以看到 X Axis 和 Y Axis 的字样。按照平常人的见识， 观察该图就能明白 Axis 是轴的意思。此外，Axis 和 Axes 以及 Figure 这三者关系，你看完下图，会恍然大悟。 绘制第一张图按照剧本发展，我接下来以绘制曲线并逐步美化它为例子，一步步讲解如何绘制图形。在这过程中，我也会逐一说明各个函数的作用。 初步绘制曲线12345678910import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-2, 6, 50)y1 = x + 3 # 曲线 y1y2 = 3 - x # 曲线 y2plt.figure() # 定义一个图像窗口plt.plot(x, y1) # 绘制曲线 y1plt.plot(x, y2) # 绘制曲线 y2plt.show() 调用np.linspace是创建一个 numpy 数组，并记作 x。x 包含了从 -2 到 6 之间等间隔的 50 个值。y1 和 y2 则分别是这 50 个值对应曲线的函数值组成的 numpy 数组。前面的操作还处于设置属性的阶段，还没有开始绘制图形。plt.figure() 函数才意味着开始执行绘图操作。最后别忘记调用show()函数将图形呈现出来。 简单修饰我们已经绘制出两条直线，但样式比较简陋。所以我给两条曲线设置鲜艳的颜色、线条类型。同时，还给纵轴和横轴的设置上下限，增加可观性。 123456789101112131415161718192021222324import matplotlib.pyplot as pltimport numpy as np# 创建一个点数为 8 x 6 的窗口, 并设置分辨率为 80像素/每英寸plt.figure(figsize=(8, 6), dpi=80)# 再创建一个规格为 1 x 1 的子图plt.subplot(1，1，1)x = np.linspace(-2, 6, 50)y1 = x + 3 # 曲线 y1y2 = 3 - x # 曲线 y2# 绘制颜色为蓝色、宽度为 1 像素的连续曲线 y1plt.plot(x, y1, color=\"blue\", linewidth=1.0, linestyle=\"-\")# 绘制颜色为紫色、宽度为 2 像素的不连续曲线 y2plt.plot(x, y2, color=\"#800080\", linewidth=2.0, linestyle=\"--\")# 设置横轴的上下限plt.xlim(-1, 6)# 设置纵轴的上下限plt.ylim(-2, 10)plt.show() 设置纵横轴标签在图像中，我们不能一味地认为横轴就是 X 轴，纵轴就是 Y 轴。图形因内容数据不同，纵横轴标签往往也会不同。这也体现了给纵横轴设置标签说明的重要性。 1234567...# 设置横轴标签plt.xlabel(\"X\")# 设置纵轴标签plt.ylabel(\"Y\")plt.show() 设置精准刻度matplotlib 画图设置的刻度是由曲线以及窗口的像素点等因素决定。这些刻度精确度无法满足需求，我们需要手动添加刻度。上图中，纵轴只显示 2 的倍数的刻度，横轴只显示 1 的倍数的刻度。我们为其添加精准刻度，纵轴变成单位间隔为 1 的刻度，横轴变成单位间隔为 0.5 的刻度。 1234567...# 设置横轴精准刻度plt.xticks([-1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5])# 设置纵轴精准刻度plt.yticks([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])plt.show() xticks() 和 yticks() 需要传入一个列表作为参数。 该方法默认是将列表的值来设置刻度标签，如果你想重新设置刻度标签，则需要传入两个列表参数给 xticks() 和 yticks() 。第一个列表的值代表刻度，第二个列表的值代表刻度所显示的标签。 12345678...# 设置横轴精准刻度plt.xticks([-1, 0, 1, 2, 3, 4, 5, 6], [\"-1m\", \"0m\", \"1m\", \"2m\", \"3m\", \"4m\", \"5m\", \"6m\"])# 设置纵轴精准刻度plt.yticks([-2, 0, 2, 4, 6, 8, 10], [\"-2m\", \"0m\", \"2m\", \"4m\", \"6m\", \"8m\", \"10m\"])plt.show() 添加图例如果需要在图的左上角添加一个图例。我们只需要在 plot() 函数里以「键 - 值」的形式增加一个参数。首先我们需要在绘制曲线的时候，增加一个 label 参数，然后再调用 plt.legend() 绘制出一个图例。plt.legend() 需要传入一个位置值。loc 的值可选如下： 值 说明 best 自动选择最佳位置，默认是左上 upper right 右上 upper left 左上 lower right 右下 lower left 左下 right 右边，默认是右上。如果因图形挡住右上，会自动往下选择空白地方绘制 center right 垂直居中且靠右 center left 垂直居中且靠左 lower center 垂直居中且靠底部 upper center 垂直居中且靠顶部 center 居中 1234567...# 绘制颜色为蓝色、宽度为 1 像素的连续曲线 y1plt.plot(x, y1, color=\"blue\", linewidth=1.0, linestyle=\"-\", label=\"y1\")# 绘制颜色为紫色、宽度为 2 像素的不连续曲线 y2plt.plot(x, y2, color=\"#800080\", linewidth=2.0, linestyle=\"--\", label=\"y2\")plt.legend(loc=\"upper left\")... 注释特殊点位有时某些数据点非常关键，需要突显出来。我们需要将改点绘制出来，即绘制散点图，再对其做注释。实现上述需求，我们要用到scatter()和annotate()函数。scatter() 是用于绘制散图，这里我们只是用其来绘制单个点。scatter() 用法，后续文章会详细对其用法做说明。annotate()则是添加标注 。 scatter() 函数必须传入两个参数 x 和 y。值得注意得是，它们的数据类型是列表。x 代表要标注点的横轴位置，y 代表要标注点的横轴位置。x 和 y 列表中下标相同的数据是对应的。例如 x 为 [3, 4]，y 为 [6, 8]，这表示会绘制点（3，6），（4， 8）。因此，x 和 y 长度要一样。 annotate函数同样也有两个必传参数，一个是标注内容，另一个是 xy。标注内容是一个字符串。xy 表示要在哪个位置（点）显示标注内容。xy 位置地选定。一般是在scatter() 绘制点附近，但不建议重合，这样会影响美观。 1234567891011121314151617181920...# 绘制颜色为蓝色、宽度为 1 像素的连续曲线 y1plt.plot(x, y1, color=\"blue\", linewidth=1.0, linestyle=\"-\", label=\"y1\")# 绘制散点(3, 6)plt.scatter([3], [6], s=30, color=\"blue\") # s 为点的 size# 对(3, 6)做标注plt.annotate(\"(3, 6)\", xy=(3.3, 5.5), # 在(3.3, 5.5)上做标注 fontsize=16, # 设置字体大小为 16 xycoords='data') # xycoords='data' 是说基于数据的值来选位置# 绘制颜色为紫色、宽度为 2 像素的不连续曲线 y2plt.plot(x, y2, color=\"#800080\", linewidth=2.0, linestyle=\"--\", label=\"y2\")# 绘制散点(3, 0)plt.scatter([3], [0], s=50, color=\"#800080\")# 对(3, 0)做标注plt.annotate(\"(3, 0)\", xy=(3.3, 0), # 在(3.3, 0)上做标注 fontsize=16, # 设置字体大小为 16 xycoords='data') # xycoords='data' 是说基于数据的值来选位置 点已经被标注出来了，如果你还想给点添加注释。这需要使用text()函数。text(x，y，s) 作用是在点(x，y) 上添加文本 s。matplotlib 目前好像对中午支持不是很友好， 中文均显示为乱码。 1234567···# 绘制散点(3, 0)plt.scatter([3], [0], s=50, color=\"#800080\")# 对(3, 0)做标注plt.annotate(\"(3, 0)\", xy=(3.3, 0))plt.text(4, -0.5, \"this point very important\", fontdict={'size': 12, 'color': 'green'}) # fontdict设置文本字体 到此为止，我们基本上完成了绘制直线所有工作。Matplotlib 能绘制种类繁多且绘图功能强大，所以我接下来的文章将单独对每种类型图做分享讲解。","link":"/1123.html"},{"title":"彻底理解 Iterable、Iterator、generator","text":"本文介绍猴哥对于Python中的Iterable、Iterator、generator的理解。 Iterable我们一般称Iterable为可迭代对象。Python 中任意的对象，只要它定义了可以返回一个迭代器的__iter__方法，或者定义了可以支持下标索引的__getitem__方法，那么它就是一个可迭代对象。我们常用到的集合数据类型都是 Iterable。例如列表（list）、元组（tuple）、字典（dict）、集合（set）、字符串（str）等。 我定义了一个列表 numlist，打印出该列表的方法。 1234numlist = [1, 2, 3]print(numlist)print(numlist.__iter__) # 调用__iter__方法print(numlist.__getitem__) # 调用__getitem__方法 运行结果如下： 根据运行结果，我们可知列表就是个可迭代对象。Python 的collections库有个isinstance()函数。可以用来判断一个对象是否是 Iterable 对象。 12345from collections import Iterable isinstance({}, Iterable) isinstance((), Iterable) isinstance(999, Iterable) 运行结果为： 如果我们每次都要使用这个函数来判断一个对象是否为可迭代对象，这样操作有点麻烦。有没有快速判定的方法呢？答案是肯定的。可以直接使用 for 循环进行遍历的对象就是可迭代对象。 除此之外，generator(生成器) 和带 yield 的 generator function 也是可迭代的对象。 IteratorIterator是迭代器的意思。任意对象，只要定义了next()（Python 2 版本）或者__next__()（Python 3 版本） 方法，那么它就是一个迭代器。迭代器中还有另一个函数__iter__()，它和 next() 方法形成迭代器协议。 iter()返回主要是返回迭代器对象本身，即return self。如果你自己定义个迭代器，实现该函数就能使用for ... in ...语句遍历了。 next()获取容器中的下一个元素，当没有可访问元素后，就抛出StopIteration异常。 遍历迭代器有两个方式。一种是使用 next() 函数；另一种则是使用 for each 循环，本质上就是通过不断调用 next() 函数实现的。 1234567891011121314151617181920from collections import Iteratornumlist = [1, 2, 3]# 将数组转化为迭代器ite1 = iter(numlist)print(ite1)for i in ite1: print(i)print(\"=========\")ite2 = iter(numlist)while True: try: num = ite2.__next__() print(num) except StopIteration: break 值得注意的是一个 Iterator 只能遍历一次。 generatorgenerator 翻译成中文是生成器。生成器也是一种特殊迭代器。它其实是生成器函数返回生成器的迭代，“生成器的迭代器”这个术语通常被称作”生成器”。yield 是生成器实现__next__()方法的关键。它作为生成器执行的暂停恢复点，可以对 yield 表达式进行赋值，也可以将 yield 表达式的值返回。任何包含 yield 语句的函数被称为生成器。 yield是一个语法糖，内部实现支持了迭代器协议，同时yield内部是一个状态机，维护着挂起和继续的状态。 个人认为，生成器算是 Python 非常棒的特性。它的出现能帮助大大节省些内存空间。假如我们要生成从 1 到 10 这 10 个数字，采用列表的方式定义，会占用 10 个地址空间。采用生成器，只会占用一个地址空间。因为生成器并没有把所有的值存在内存中，而是在运行时生成值。所以生成器只能访问一次。 创建一个从包含 1 到 10 的生成器的例子。 1234gen = (i for i in range(10))print(gen)for i in gen: print(i) 运行结果如下： 带有 yield 关键字 的例子。重点关注运行结果，这能让你对 yield 有更深的认识。 12345678910111213def testYield(n): for i in range(n): print(\"当前值: \", i) yield doubeNumber(i) print(\"第 \", i, \" 次运行\") print(\"testYield 运行结束\")def doubeNumber(i): return i*2 if __name__ == '__main__': for i in testYield(3): print(i, \"===\", i) 运行结果如下：","link":"/1124.html"},{"title":"Python 绘图,我只用 Matplotlib(三)","text":"上篇文章，我已经讲解绘制图像大致步骤，接下来的系列文章将分别对各种图形做讲解。其实就是了解各个图种的绘图 API。文章就讲解第一种图形，柱状图。 基础绘制柱状图，我们主要用到bar()函数。只要将该函数理解透彻，我们就能绘制各种类型的柱状图。 我们先看下bar()的构造函数：bar(x，height， width，*，align='center'，**kwargs) x包含所有柱子的下标的列表 height包含所有柱子的高度值的列表 width每个柱子的宽度。可以指定一个固定值，那么所有的柱子都是一样的宽。或者设置一个列表，这样可以分别对每个柱子设定不同的宽度。 align柱子对齐方式，有两个可选值：center和edge。center表示每根柱子是根据下标来对齐, edge则表示每根柱子全部以下标为起点，然后显示到下标的右边。如果不指定该参数，默认值是center。 其他可选参数有： color每根柱子呈现的颜色。同样可指定一个颜色值，让所有柱子呈现同样颜色；或者指定带有不同颜色的列表，让不同柱子显示不同颜色。 edgecolor每根柱子边框的颜色。同样可指定一个颜色值，让所有柱子边框呈现同样颜色；或者指定带有不同颜色的列表，让不同柱子的边框显示不同颜色。 linewidth每根柱子的边框宽度。如果没有设置该参数，将使用默认宽度，默认是没有边框。 tick_label每根柱子上显示的标签，默认是没有内容。 xerr每根柱子顶部在横轴方向的线段。如果指定一个固定值，所有柱子的线段将一直长；如果指定一个带有不同长度值的列表，那么柱子顶部的线段将呈现不同长度。 yerr每根柱子顶端在纵轴方向的线段。如果指定一个固定值，所有柱子的线段将一直长；如果指定一个带有不同长度值的列表，那么柱子顶部的线段将呈现不同长度。 ecolor设置 xerr 和 yerr 的线段的颜色。同样可以指定一个固定值或者一个列表。 capsize这个参数很有趣, 对xerr或者yerr的补充说明。一般为其设置一个整数，例如 10。如果你已经设置了yerr 参数，那么设置 capsize 参数，会在每跟柱子顶部线段上面的首尾部分增加两条垂直原来线段的线段。对 xerr 参数也是同样道理。可能看说明会觉得绕，如果你看下图就一目了然了。 error_kw设置 xerr 和 yerr 参数显示线段的参数，它是个字典类型。如果你在该参数中又重新定义了 ecolor 和 capsize，那么显示效果以这个为准。 log这个参数，我暂时搞不懂有什么用。 orientation设置柱子是显示方式。设置值为 vertical ，那么显示为柱形图。如果设置为 horizontal 条形图。不过 matplotlib 官网不建议直接使用这个来绘制条形图，使用barh来绘制条形图。 下面我就调用 bar 函数绘制一个最简单的柱形图。 123456789101112131415161718192021222324252627282930313233343536373839import matplotlib.pyplot as pltimport numpy as np# 创建一个点数为 8 x 6 的窗口, 并设置分辨率为 80像素/每英寸plt.figure(figsize=(8, 6), dpi=80)# 再创建一个规格为 1 x 1 的子图plt.subplot(1, 1, 1)# 柱子总数N = 6# 包含每个柱子对应值的序列values = (25, 32, 34, 20, 41, 50)# 包含每个柱子下标的序列index = np.arange(N)# 柱子的宽度width = 0.35# 绘制柱状图, 每根柱子的颜色为紫罗兰色p2 = plt.bar(index, values, width, label=\"rainfall\", color=\"#87CEFA\")# 设置横轴标签plt.xlabel('Months')# 设置纵轴标签plt.ylabel('rainfall (mm)')# 添加标题plt.title('Monthly average rainfall')# 添加纵横轴的刻度plt.xticks(index, ('Jan', 'Fub', 'Mar', 'Apr', 'May', 'Jun'))plt.yticks(np.arange(0, 81, 10))# 添加图例plt.legend(loc=\"upper right\")plt.show() 运行结果为： 进阶bar 函数的参数很多，你可以使用这些参数绘制你所需要柱形图的样式。如果你还不会灵活使用这样参数，那就让我们来学习 matplotlib 官方提供的例子。 123456789101112131415161718192021222324252627282930313233343536373839404142# Credit: Josh Hemannimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib.ticker import MaxNLocatorfrom collections import namedtuplen_groups = 5means_men = (20, 35, 30, 35, 27)std_men = (2, 3, 4, 1, 2)means_women = (25, 32, 34, 20, 25)std_women = (3, 5, 2, 3, 3)fig, ax = plt.subplots()index = np.arange(n_groups)bar_width = 0.35opacity = 0.4error_config = {'ecolor': '0.3'}rects1 = ax.bar(index, means_men, bar_width, alpha=opacity, color='b', yerr=std_men, error_kw=error_config, label='Men')rects2 = ax.bar(index + bar_width, means_women, bar_width, alpha=opacity, color='r', yerr=std_women, error_kw=error_config, label='Women')ax.set_xlabel('Group')ax.set_ylabel('Scores')ax.set_title('Scores by group and gender')ax.set_xticks(index + bar_width / 2)ax.set_xticklabels(('A', 'B', 'C', 'D', 'E'))ax.legend()fig.tight_layout()plt.show() 运行结果如下： 开动你的大脑，想想还能绘制出什么样式的柱形图。","link":"/1125.html"},{"title":"Python 定时任务(上)","text":"在项目中，我们可能遇到有定时任务的需求。其一：定时执行任务。例如每天早上 8 点定时推送早报。其二：每隔一个时间段就执行任务。比如：每隔一个小时提醒自己起来走动走动，避免长时间坐着。今天，我跟大家分享下 Python 定时任务的实现方法。 ##第一种办法是最简单又最暴力。那就是在一个死循环中，使用线程睡眠函数 sleep()。 12345678910111213from datetime import datetimeimport time'''每个 10 秒打印当前时间。'''def timedTask(): while True: print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) time.sleep(10)if __name__ == '__main__': timedTask() 这种方法能够执行固定间隔时间的任务。如果timedTask()函数之后还有些操作，我们还使用死循环 + 阻塞线程。这会使得timedTask()一直占有 CPU 资源，导致后续操作无法执行。我建议谨重使用。 ##既然第一种方法暴力，那么有没有比较优雅地方法？答案是肯定的。Python 标准库 threading 中有个 Timer 类。它会新启动一个线程来执行定时任务，所以它是非阻塞函式。 如果你有使用多线程的话，需要关心线程安全问题。那么你可以选使用threading.Timer模块。 123456789101112131415161718192021222324from datetime import datetimefrom threading import Timerimport time'''每个 10 秒打印当前时间。'''def timedTask(): ''' 第一个参数: 延迟多长时间执行任务(单位: 秒) 第二个参数: 要执行的任务, 即函数 第三个参数: 调用函数的参数(tuple) ''' Timer(10, task, ()).start()# 定时任务def task(): print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))if __name__ == '__main__': timedTask() while True: print(time.time()) time.sleep(5) 运行结果： 12341512486945.11963751512486950.1198732017-12-05 23:15:501512486955.133385 ##第三种方式是也是使用标准库中sched模块。sched 是事件调度器，它通过 scheduler 类来调度事件，从而达到定时执行任务的效果。 sched库使用起来也是非常简单。1）首先构造一个sched.scheduler类它接受两个参数：timefunc 和 delayfunc。timefunc 应该返回一个数字，代表当前时间，delayfunc 函数接受一个参数，用于暂停运行的时间单元。 一般使用默认参数就行，即传入这两个参数 time.time 和 time.sleep.当然，你也可以自己实现时间暂停的函数。 2）添加调度任务scheduler 提供了两个添加调度任务的函数: enter(delay, priority, action, argument=(), kwargs={}) 该函数可以延迟一定时间执行任务。delay 表示延迟多长时间执行任务，单位是秒。priority为优先级，越小优先级越大。两个任务指定相同的延迟时间，优先级大的任务会向被执行。action 即需要执行的函数，argument 和 kwargs 分别是函数的位置和关键字参数。 scheduler.enterabs(time, priority, action, argument=(), kwargs={}) 添加一项任务，但这个任务会在 time 这时刻执行。因此，time 是绝对时间.其他参数用法与 enter() 中的参数用法是一致。 3）把任务运行起来调用 scheduler.run()函数就完事了。 下面是 sche 使用的简单示例： 123456789101112131415161718192021from datetime import datetimeimport schedimport time'''每个 10 秒打印当前时间。'''def timedTask(): # 初始化 sched 模块的 scheduler 类 scheduler = sched.scheduler(time.time, time.sleep) # 增加调度任务 scheduler.enter(10, 1, task) # 运行任务 scheduler.run()# 定时任务def task(): print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))if __name__ == '__main__': timedTask() 值得注意的是： scheduler 中的每个调度任务只会工作一次，不会无限循环被调用。如果想重复执行同一任务， 需要重复添加调度任务即可。","link":"/1226.html"},{"title":"Python 定时任务(下)","text":"上篇文章，我们了解到有三种办法能实现定时任务，但是都无法做到循环执行定时任务。因此，需要一个能够担当此重任的库。它就是APScheduler。 简介APScheduler的全称是Advanced Python Scheduler。它是一个轻量级的 Python 定时任务调度框架。APScheduler 支持三种调度任务：固定时间间隔，固定时间点（日期），Linux 下的 Crontab 命令。同时，它还支持异步执行、后台执行调度任务。 安装使用 pip 包管理工具安装 APScheduler 是最方便快捷的。 123pip install APScheduler# 如果出现因下载失败导致安装不上的情况，建议使用代理pip --proxy http://代理ip:端口 install APScheduler 使用步骤APScheduler 使用起来还算是比较简单。运行一个调度任务只需要以下三部曲。 1) 新建一个 schedulers (调度器) 。2) 添加一个调度任务(job stores)。3) 运行调度任务。 下面是执行每 2 秒报时的简单示例代码： 12345678910111213141516171819import datetimeimport timefrom apscheduler.schedulers.background import BackgroundSchedulerdef timedTask(): print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])if __name__ == '__main__': # 创建后台执行的 schedulers scheduler = BackgroundScheduler() # 添加调度任务 # 调度方法为 timedTask，触发器选择 interval(间隔性)，间隔时长为 2 秒 scheduler.add_job(timedTask, 'interval', seconds=2) # 启动调度任务 scheduler.start() while True: print(time.time()) time.sleep(5) 基础组件APScheduler 有四种组件，分别是：调度器(scheduler)，作业存储(job store)，触发器(trigger)，执行器(executor)。 schedulers（调度器）它是任务调度器，属于控制器角色。它配置作业存储器和执行器可以在调度器中完成，例如添加、修改和移除作业。 triggers（触发器）描述调度任务被触发的条件。不过触发器完全是无状态的。 job stores（作业存储器）任务持久化仓库，默认保存任务在内存中，也可将任务保存都各种数据库中，任务中的数据序列化后保存到持久化数据库，从数据库加载后又反序列化。 executors（执行器）负责处理作业的运行，它们通常通过在作业中提交指定的可调用对象到一个线程或者进城池来进行。当作业完成时，执行器将会通知调度器。 schedulers（调度器）我个人觉得 APScheduler 非常好用的原因。它提供 7 种调度器，能够满足我们各种场景的需要。例如：后台执行某个操作，异步执行操作等。调度器分别是： BlockingScheduler : 调度器在当前进程的主线程中运行，也就是会阻塞当前线程。 BackgroundScheduler : 调度器在后台线程中运行，不会阻塞当前线程。 AsyncIOScheduler : 结合 asyncio 模块（一个异步框架）一起使用。 GeventScheduler : 程序中使用 gevent（高性能的Python并发框架）作为IO模型，和 GeventExecutor 配合使用。 TornadoScheduler : 程序中使用 Tornado（一个web框架）的IO模型，用 ioloop.add_timeout 完成定时唤醒。 TwistedScheduler : 配合 TwistedExecutor，用 reactor.callLater 完成定时唤醒。 QtScheduler : 你的应用是一个 Qt 应用，需使用QTimer完成定时唤醒。 triggers（触发器）APScheduler 有三种内建的 trigger:1）date 触发器date 是最基本的一种调度，作业任务只会执行一次。它表示特定的时间点触发。它的参数如下： 参数 说明 run_date (datetime 或 str) 作业的运行日期或时间 timezone (datetime.tzinfo 或 str) 指定时区 date 触发器使用示例如下： 12345678910111213141516from datetime import datetimefrom datetime import datefrom apscheduler.schedulers.background import BackgroundSchedulerdef job_func(text): print(text)scheduler = BackgroundScheduler()# 在 2017-12-13 时刻运行一次 job_func 方法scheduler .add_job(job_func, 'date', run_date=date(2017, 12, 13), args=['text'])# 在 2017-12-13 14:00:00 时刻运行一次 job_func 方法scheduler .add_job(job_func, 'date', run_date=datetime(2017, 12, 13, 14, 0, 0), args=['text'])# 在 2017-12-13 14:00:01 时刻运行一次 job_func 方法scheduler .add_job(job_func, 'date', run_date='2017-12-13 14:00:01', args=['text'])scheduler.start() 2）interval 触发器固定时间间隔触发。interval 间隔调度，参数如下： 参数 说明 weeks (int) 间隔几周 days (int) 间隔几天 hours (int) 间隔几小时 minutes (int) 间隔几分钟 seconds (int) 间隔多少秒 start_date (datetime 或 str) 开始日期 end_date (datetime 或 str) 结束日期 timezone (datetime.tzinfo 或str) 时区 interval 触发器使用示例如下： 12345678910111213import datetimefrom apscheduler.schedulers.background import BackgroundSchedulerdef job_func(text): print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])scheduler = BackgroundScheduler()# 每隔两分钟执行一次 job_func 方法scheduler .add_job(job_func, 'interval', minutes=2)# 在 2017-12-13 14:00:01 ~ 2017-12-13 14:00:10 之间, 每隔两分钟执行一次 job_func 方法scheduler .add_job(job_func, 'interval', minutes=2, start_date='2017-12-13 14:00:01' , end_date='2017-12-13 14:00:10')scheduler.start() 3）cron 触发器 在特定时间周期性地触发，和Linux crontab格式兼容。它是功能最强大的触发器。 我们先了解 cron 参数： 参数 说明 year (int 或 str) 年，4位数字 month (int 或 str) 月 (范围1-12) day (int 或 str) 日 (范围1-31 week (int 或 str) 周 (范围1-53) day_of_week (int 或 str) 周内第几天或者星期几 (范围0-6 或者 mon,tue,wed,thu,fri,sat,sun) hour (int 或 str) 时 (范围0-23) minute (int 或 str) 分 (范围0-59) second (int 或 str) 秒 (范围0-59) start_date (datetime 或 str) 最早开始日期(包含) end_date (datetime 或 str) 最晚结束时间(包含) timezone (datetime.tzinfo 或str) 指定时区 这些参数是支持算数表达式，取值格式有如下： cron 触发器使用示例如下： 1234567891011import datetimefrom apscheduler.schedulers.background import BackgroundSchedulerdef job_func(text): print(\"当前时间：\", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])scheduler = BackgroundScheduler()# 在每年 1-3、7-9 月份中的每个星期一、二中的 00:00, 01:00, 02:00 和 03:00 执行 job_func 任务scheduler .add_job(job_func, 'cron', month='1-3,7-9',day='0, tue', hour='0-3')scheduler.start() 作业存储(job store)该组件是对调度任务的管理。1）添加 job有两种添加方法，其中一种上述代码用到的 add_job()， 另一种则是scheduled_job()修饰器来修饰函数。 这个两种办法的区别是：第一种方法返回一个 apscheduler.job.Job 的实例，可以用来改变或者移除 job。第二种方法只适用于应用运行期间不会改变的 job。 第二种添加任务方式的例子： 123456789import datetimefrom apscheduler.schedulers.background import BackgroundScheduler@scheduler.scheduled_job(job_func, 'interval', minutes=2)def job_func(text): print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])scheduler = BackgroundScheduler()scheduler.start() 2）移除 job移除 job 也有两种方法：remove_job() 和 job.remove()。remove_job() 是根据 job 的 id 来移除，所以要在 job 创建的时候指定一个 id。job.remove() 则是对 job 执行 remove 方法即可 12345scheduler.add_job(job_func, 'interval', minutes=2, id='job_one')scheduler.remove_job(job_one)job = add_job(job_func, 'interval', minutes=2, id='job_one')job.remvoe() 3）获取 job 列表通过 scheduler.get_jobs() 方法能够获取当前调度器中的所有 job 的列表 4) 修改 job如果你因计划改变要对 job 进行修改，可以使用Job.modify() 或者 modify_job()方法来修改 job 的属性。但是值得注意的是，job 的 id 是无法被修改的。 12345678scheduler.add_job(job_func, 'interval', minutes=2, id='job_one')scheduler.start()# 将触发时间间隔修改成 5分钟scheduler.modify_job('job_one', minutes=5)job = scheduler.add_job(job_func, 'interval', minutes=2)# 将触发时间间隔修改成 5分钟job.modify(minutes=5) 5）关闭 job默认情况下调度器会等待所有正在运行的作业完成后，关闭所有的调度器和作业存储。如果你不想等待，可以将 wait 选项设置为 False。 12scheduler.shutdown()scheduler.shutdown(wait=false) 执行器(executor)执行器顾名思义是执行调度任务的模块。最常用的 executor 有两种：ProcessPoolExecutor 和 ThreadPoolExecutor 下面是显式设置 job store(使用mongo存储)和 executor 的代码的示例。注：本代码来源于网络 1234567891011121314151617181920212223242526272829303132from pymongo import MongoClientfrom apscheduler.schedulers.blocking import BlockingSchedulerfrom apscheduler.jobstores.mongodb import MongoDBJobStorefrom apscheduler.jobstores.memory import MemoryJobStorefrom apscheduler.executors.pool import ThreadPoolExecutor, ProcessPoolExecutor def my_job(): print 'hello world'host = '127.0.0.1'port = 27017client = MongoClient(host, port) jobstores = { 'mongo': MongoDBJobStore(collection='job', database='test', client=client), 'default': MemoryJobStore()}executors = { 'default': ThreadPoolExecutor(10), 'processpool': ProcessPoolExecutor(3)}job_defaults = { 'coalesce': False, 'max_instances': 3}scheduler = BlockingScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults)scheduler.add_job(my_job, 'interval', seconds=5) try: scheduler.start()except SystemExit: client.close()","link":"/1227.html"},{"title":"Python 实现识别弱图片验证码","text":"目前，很多网站为了防止爬虫肆意模拟浏览器登录，采用增加验证码的方式来拦截爬虫。验证码的形式有多种，最常见的就是图片验证码。其他验证码的形式有音频验证码，滑动验证码等。图片验证码越来越高级，识别难度也大幅提高，就算人为输入也经常会输错。本文主要讲解识别弱图片验证码。 图片验证码强度图片验证码主要采用加干扰线、字符粘连、字符扭曲方式来增强识别难度。 加干扰线加干扰线也分为两种，一种是线条跟字符同等颜色，另一种则线条的颜色是五颜六色。) 字符粘连各个字符之间的间隔比较小，互相依靠，能以分割。 字符扭曲字符显示的位置相对标准旋转一定角度。) 其中最弱的验证码为不具备以上的特征，干扰因素比较小。如下： 识别思路首先对图片做二值化来降噪处理，去掉图片中的噪点，干扰线等。然后将图片中的单个字符切分出来。最后识别每个字符。 图片的处理，我采用 Python 标准图像处理库 PIL。图片分割，我暂时采用谷歌开源库 Tesseract-OCR。字符识别则使用 pytesseract 库。 安装 Pillow 我使用的 Python 版本是 3.6， 而标准库 PIL 不支持 3.x。所以需要使用 Pillow 来替代。Pillow 是专门兼容 3.x 版本的 PIL 的分支。使用 pip 包管理工具安装 Pillow 是最方便快捷的。 123pip install Pillow# 如果出现因下载失败导致安装不上的情况，建议使用代理pip --proxy http://代理ip:端口 install Pillow Tesseract-OCR Tesseract：开源的OCR识别引擎，初期Tesseract引擎由HP实验室研发，后来贡献给了开源软件业，后经由Google进行改进，消除bug，优化，重新发布。这才让其重焕新生。 我们可以在 GitHub 上找到该库并下载。我是下载最新的 4.0 版本。github 的下载地址是：https://github.com/tesseract-ocr/tesseract/wiki/4.0-with-LSTM#400-alpha-for-windows pytesseract pytesseract 是 Tesseract-OCR 对进行包装，提供 Python 接口的库。同样可以使用 pip 方式来安装。 123pip install pytesseract# 如果出现因下载失败导致安装不上的情况，建议使用代理pip --proxy http://代理ip:端口 install pytesseract 代码实现获取并打开图片获取图片验证码，你可以通过使用网络请求库下载。我为了方便，将图片下载到本地并放在项目目录下。 123456789101112from PIL import Image'''获取图片'''def getImage(): fileName = '16.jpg' img = Image.open() # 打印当前图片的模式以及格式 print('未转化前的: ', img.mode, img.format) # 使用系统默认工具打开图片 # img.show() return img 预处理这一步主要是将图片进行降噪处理, 把图片从 “RGB” 模式转化为 “L” 模式，也就是把彩色图片变成黑白图片。再处理掉背景噪点，让字符和背景形成黑白的反差。 123456789101112131415161718192021'''1) 将图片进行降噪处理, 通过二值化去掉后面的背景色并加深文字对比度'''def convert_Image(img, standard=127.5): ''' 【灰度转换】 ''' image = img.convert('L') ''' 【二值化】 根据阈值 standard , 将所有像素都置为 0(黑色) 或 255(白色), 便于接下来的分割 ''' pixels = image.load() for x in range(image.width): for y in range(image.height): if pixels[x, y] &gt; standard: pixels[x, y] = 255 else: pixels[x, y] = 0 return image 打开彩色图片，PIL 会将图片解码为三通道的 “RGB” 图像。调用 convert(‘L’) 才会把图片转化为黑白图片。其中模式 “L” 为灰色图像, 它的每个像素用 8 个bit表示, 0 表示黑, 255 表示白, 其他数字表示不同的灰度。 在 PIL 中，从模式 “RGB” 转换为 “L” 模式是按照下面的公式转换的：L = R 的值 x 299/1000 + G 的值 x 587/1000+ B 的值 x 114/1000 图像的二值化，就是将图像上的像素点的灰度值两极分化(设置为 0 或 255，0表示黑，255表示白)，也就是将整个图像呈现出明显的只有黑和白的视觉效果。目的是加深字符与背景的颜色差，便于 Tesseract 的识别和分割。对于阈值的选取，我采用比较暴力的做法，直接使用 0 和 255 的平均值。 识别经过上述处理，图片验证码中的字符已经变成很清晰了。最后一步是直接用 pytesseract 库识别。 1234567891011121314import pytesseract'''使用 pytesseract 库来识别图片中的字符'''def change_Image_to_text(img): ''' 如果出现找不到训练库的位置, 需要我们手动自动 语法: tessdata_dir_config = '--tessdata-dir \"&lt;replace_with_your_tessdata_dir_path&gt;\"' ''' testdata_dir_config = '--tessdata-dir \"C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tessdata\"' textCode = pytesseract.image_to_string(img, lang='eng', config=testdata_dir_config) # 去掉非法字符，只保留字母数字 textCode = re.sub(\"\\W\", \"\", textCode) return textCode Tesseract-ORC 默认是没有指定安装路径。我们需要手动指定本地 Tesseract 的路径。不然会报出这样的错误： 1FileNotFoundError: [WinError 2] 系统找不到指定的文件 具体解决方案是：使用文本编辑器打开 pytesseract 库的 pytesseract.py 文件，一般路径如下：C:\\Program Files (x86)\\Python35-32\\Lib\\site-packages\\pytesseract\\pytesseract.py 将 tesseract_cmd 修改成你电脑本地的 Tesseract-OCR 的安装路径。 12# CHANGE THIS IF TESSERACT IS NOT IN YOUR PATH, OR IS NAMED DIFFERENTLYtesseract_cmd = 'C:/Program Files (x86)/Tesseract-OCR/tesseract.exe' 最后执行字符识别的实例代码 123456def main(): img = convert_Image(getImage(fileName)) print('识别的结果：', change_Image_to_text(img))if __name__ == '__main__': main() 运行结果如下： 12未转化前的: RGB JPEG识别的结果： 9834 总结Tesseract-ORC 对于这种弱验证码识别率还是可以，大部分字符能够正确识别出来。只不过有时候会将数字 8 识别为 0。如果图片验证码稍微变得复杂点，识别率大大降低，会经常识别不出来的情况。我自己也尝试收集 500 张图片来训练 Tesseract-ORC，识别率会有所提升，但识别率还是很低。 如果想要做到识别率较高，那么需要使用 CNN (卷积神经网络)或者 RNN (循环神经网络)训练出自己的识别库。正好机器学习很火爆很流行，学习一下也无妨。","link":"/1228.html"},{"title":"Scrapy 框架插件之IP代理池","text":"现在很多网站都是对单个 IP 地址有访问次数限制，如果你在短时间内访问过于频繁。该网站会封掉你 IP，让你在一段时间内无法正常该网站。突破反爬虫机制的一个重要举措就是代理 IP。拥有庞大稳定的 IP 代理，在爬虫工作中将起到重要的作用,但是从成本的角度来说，一般稳定的 IP 池都很贵。因此，我为 Scrapy 爬虫编写个免费 IP 代理池插件。 特点该插件适用的程序是基于 Scrapy 框架编写的爬虫程序。插件通过爬取免费代理地址，然后过滤掉无效 IP 代理后存放到 Mysql 数据库。另外，它会每 10 分钟轮询数据库中的 IP 代理数量。如果代理地址因为连接失败次数超过 3 次被删除，从而导致代理不够，它会后台重新爬取新的 IP 代理。 收集的代理网站 无忧代理(data5u) ip181 代理 快代理 西刺代理 项目说明 startrun.py项目的主入口。它负责启动 Scrapy 爬虫和代理池。 your_scrapy_project该目录下主要存放两个文件：config.py 和 settings.py。config.py 是代理池的项目配置信息。而 settings.py 是你的 Scrapy 爬虫项目的配置参考代码。 ProxyPoolWorker.pyProxyPoolWorker.py 是 IP代理池模块的管理类，负责启动和维护 IP 代理池。 proxyDBManager.pyproxyDBManager.py 位于 dbManager 包下。它是数据库操作类。主要工作是创建数据库表、往数据库中插入 IP 代理、查询数据库中剩余的 IP 代理总数、从数据库中随机查询一个 IP 代理、对连接超时或失败的 IP 代理做处理。 proxyModel.pyproxyModel.py 在 model 包下。它是 IP 代理对象类。 requestEnginer.pyrequestEnginer.py 位于 requester 目录下。requestEnginer 是整个爬虫代理池的网络引擎。它采用 Session 的形式来发起 HTTP 请求。同时，它还负责验证代理地址有效性, 达到过滤掉无用 IP 代理的目的。 scrapyscrapy 目录是一些 Scrapy 框架的自定义中间件。RandomUserAgentMiddleware.py 是为 HTTP 请求随机设置个 User-agent。middlewares.py 有两个职责。一是为 HTTP 请求随机设置个 IP 代理。二是负责捕获并处理 HTTP 异常请求。 spiders该包主要是爬取各大代理网站的爬虫。 使用方法安装依赖使用本插件，你需要通过 pip 安装以下依赖： requests apscheduler pymysql 修改配置1) 将 startrun.py 放到你的 Scrapy 项目的主目录下。例如你项目名为 demo，那么你需要放到 demo 的目录下。 2) 修改 config.py 里面的 Mysql 相关配置信息。然后将其放到你的 Scrapy 项目的二级目录下。假如你项目名为 demo，那么你需要放到 demo /demo 的目录下。 3) 参考 setting.py，修改你的 Scrapy 项目中的 setting.py 文件。主要是在你项目中增加以下代码： 1234567891011121314151617# 默认使用 IP 代理池if IF_USE_PROXY: DOWNLOADER_MIDDLEWARES = { # 第二行的填写规则 # yourproject.myMiddlewares(文件名).middleware类 # 设置 User-Agent 'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None, 'proxyPool.scrapy.RandomUserAgentMiddleware.RandomUserAgentMiddleware': 400, # 设置代理 'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': None, 'proxyPool.scrapy.middlewares.ProxyMiddleware': 100, # 设置自定义捕获异常中间层 'proxyPool.scrapy.middlewares.CatchExceptionMiddleware': 105, } 写在最后本项目目前是第一版，可能有些不完善的地方。如果你有宝贵的完善建议或者有更多的代理网站，可以反馈给我。我会持续完善本项目。最后，本项目也在 github 上开源了。 如果你想获取网站的源码, 点击☞☞☞Github 仓库地址","link":"/1229.html"},{"title":"回顾 2017,展望 2018","text":"望着桌上的日历，我发现只剩下几张纸。自己蓦然意识到 2017 年已经即将离去，2018 年即将到来。已经到了年底，我们需要总结和回顾今年的历程。让我们盘点 2017 年涉及 Python 重要事件。 2017 年最热门的话题莫过于人工智能。人工智能是一块崭新的研究领域，所以很多公司都很注重人工智能的研究。走在前沿的，当属谷歌。谷歌不仅完善用于开源人工智能项目 TensorFlow 文档，建立相关社区，而且还在中国成立 AI 中国中心。虽然 TensorFlow 虽然是用 C++ 编写的，但是提供了一套 Python 的接口。另外，吴恩达教授又开设人工智能课程。这种种让 Python 的人气一路高涨， 成为今年世界上最流行的编程语言。地址: https://github.com/tensorflow/tensorflow Python 社区在几年前就一直在讨论是否迁移到 GitHub 以改进开发流程。就在今年 2 月份，Brett Cannon 在 Python 官方邮件组发消息，确定迁移到 GitHub 的日期。这让 Python 正式迁移到源码托管平台 GitHub，拥抱了 Git 版本控制系统。地址： https://github.com/python/cpython 由于历史原因，Python 2 和 Python 3 是互不兼容。所以 Python 核心团队是这几年来同时维护这两个版本。然而，Python 核心团队计划在 2020 年停止支持 Python 2。NumPy 项目团队也宣布停止支持 Python 2。 Django 作为 Python Web 流行的开发框架。它凭借文档资料丰富，开发迅速，内置辅助组件颇多等特点，一直深受人们的喜爱。国内很多知名网站也是基于 Django 来做开发的，不妨有知乎，果壳网，虎扑等。Django 官方在今年推出了 2.0 版本，其中最大的变化是停止支持 Python 2 系列。 微软也考虑将 Python 列为 Excel 官方脚本语言。如果获得批准，Excel 用户将能够像目前使用 VBA 脚本一样，使用 Python 脚本与 Excel 文档、数据以及一些 Excel 核心函数进行交互。同时，微软官网也是积极做出回应，通过发起投票来收集更多用户的反馈信息，在线调查用户是否想要在 Excel 中使用 Python。 全国计算机等级考试经过教育部批准，对全国等级考试做出调整。在二级考试中，取消“Visual FoxPro 数据库程序设计项目”科目，新增“Python 语言程序设计项目”。另外，Python 将被纳入高考内容，浙江省信息技术课程改革方案出台，确认 Python 进入浙江省信息技术高考。除此之外，山东省最新出版的《小学信息技术六年级教材》也加入了 Python 内容，设计开源硬件、人工智能、3D 创意设计等。 写在最后，我相信 Python 还会在 2018 年继续保持热度，甚至会更加火爆。所以学习 Python 是不会吃亏的。掌握多一项技能就多一条求生之路。","link":"/1230.html"},{"title":"pustil-获取系统信息库","text":"运维工程师经常使用 Python 编写脚本程序来做监控系统运行的状态。如果自己手动使用 Python 的标准库执行系统命令来获取信息，会显得非常麻烦。既要兼容不同操作系统，又要自己处理解析信息。为了解决的痛点问题，psutil 就横空出世。它的出现无疑是运维工程师的福音。运维小伙伴通过它执行一两行代码即可实现系统监控。 简介psutil全称是process and system utilities。psutil 是一个跨平台的应用于系统监控、分析、以及对系统进程进行一定管理的 Python 第三方库。它不仅能够轻松获取系统中正常运行的进程和系统利用率（例如 CPU、内存、磁盘、网络等）信息，还实现了跟 UNIX 系统命令行工具类似的功能。可以说是运维工作的“必备品”。 它功能强大，操作简单。这也促使很多开源项目都集成它到自己项目中，不妨有谷歌的 GRR 项目、脸书的 osquery 项目等。 github 地址：https://github.com/giampaolo/psutil 安装安装 psutil 是有多种办法：通过 pip 安装，通过源码方式安装，通过下载 tar 压缩包来安装。其中通过 pip 的方式是最简单的。 123pip install psutil# 如果出现因下载失败导致安装不上的情况，建议使用代理pip --proxy http://代理ip:端口 install psutil 使用前面说到 psutil 能监听到 CPU、内存、磁盘、网络、传感器、进程等，现在跟着我来学习下。 获取 CPU 信息1）我先获取自己电脑 CPU 的核心数，我电脑的 CPU 型号是 I5 4590。我通过搜索引擎得知该型号 CPU 是四核四线。 12345678import psutilpsutil.cpu_count() # 获取 CPU 的逻辑核心数，默认logical=Truepsutil.cpu_count(logical=False) # 获取 CPU 的物理核心数&gt;&gt; 4&gt;&gt; 4# 这说明该 CPU 型号是真四核。 2）统计 CPU 的时间： 1234import psutilpsutil.cpu_times() &gt;&gt; scputimes(user=9276.365234375, system=5034.5390625, idle=96077.0703125, interrupt=181.78796863555908, dpc=298.227108001709) cpu_times() 返回的是带有系统所有逻辑 CPU 运行时间的元组，单位是秒。返回元组的字段中有这几个常用字段： user：执行用户进程的时间，Linux 系统还包括访客的时间 system：执行内核进程时间 idle：闲置时间 iowait（Linux 特有）：等待 I/O 操作的时间 irp（Linux 特有）：打断服务硬件的时间 interrupt（Windows 特有）：跟 irp 字段类似 dpc（Windows 特有）：服务延迟程序调用（DPCs）的时间 如果增加参数 percpu=True， cpu_times() 会以列表的形式输出每个逻辑 CPU 的时间。 3）获取当前 CPU 的利用率的百分比： 1234import psutilpsutil.cpu_percent()&gt;&gt; 16.5 cpu_percent() 默认的参数是 interval=None, percpu=False。当 interval 为0或者None时，表示的是 interval 时间内的sys的利用率。当 percpu 为 False 表示所有逻辑 CPU 的使用率。 如果你想统计 15 秒中内，间隔 5 秒每个逻辑 CPU 的使用率，你可以这么做： 12345678import psutilfor i in range(3): psutil.cpu_percent(interval=5, percpu=True)&gt;&gt; [9.1, 7.2, 7.2, 6.2]&gt;&gt; [5.9, 3.8, 9.0, 8.4]&gt;&gt; [12.3, 8.4, 3.4, 4.1] 4）获取 CPU 频率信息： 1234import psutilpsutil.cpu_freq()&gt;&gt; [scpufreq(current=931.42925, min=0.0, max=3301.0)] 可知 cpu_freq() 返回的带有所有逻辑 CPU 频率的元组，包括当前、最小和最大频率。 获取内存信息1）获取物理内存信息： 12345import psutilpsutil.virtual_memory()&gt;&gt; svmem(total=8509177856, available=1692307456, percent=80.1, used=6816870400, free=1692307456) virtual_memory() 返回一个记载当前电脑设备中可用的物理内存信息的元组，单位是字节。从返回结果得知，当前内存总大小为 8509177856 Byte = 8 GB，可用内存（闲置内存） 1692307456 Byte = 1.6 GB，当前内存使用率为 80.1%。值得注意的是，内存总大小不等于 Used 和 available 两者的总和 available 字段在 Linux 系统下，计算方式则不同。available = free + buffers +cached。buffers 指的是 Linux 系统下的 Buffers 内存, 表示块设备(block device)所占用的缓存页; 而 cached 指的是 Linux 系统下的 Cache 内存，顾名思义为高速缓存。 2）获取交换内存信息： 1234import psutilpsutil.swap_memory()&gt;&gt; sswap(total=17016451072, used=7407996928, free=9608454144, percent=43.5, sin=0, sout=0) swap_memory() 获取的是系统的交换内存的信息，也就是我们常说的虚拟内存。前面四个字段跟物理内存含义一样。而 sin 表示从磁盘调入是 swap 的大小， sout 表示从swap调出到 disk 的大小。这两个字段在 Windows 系统下是没有意义。因此，获取结果为 0。 获取磁盘信息1）获取磁盘分区信息： 123456789import psutilpsutil.disk_partitions()&gt;&gt; [sdiskpart(device='C:\\\\', mountpoint='C:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='D:\\\\', mountpoint='D:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='E:\\\\', mountpoint='E:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='F:\\\\', mountpoint='F:\\\\', fstype='NTFS', opts='rw,fixed')] disk_partitions(all=False) 返回所有挂载磁盘分区信息的列表。有点类似 Linux 的 df 命令。各个字段的含有分别为： device：分区 mountpoint：挂载点 fstype：文件系统格式 opts：挂载参数 大部分人都对 Windows 系统的分区信息了解比较多，对 Linux 系统所知甚少。因此，我给出虚拟机 Ubuntu 系统的磁盘信息, 方便大家学习。 12[sdiskpart(device='/dev/sda1', mountpoint='/', fstype='ext4', opts='rw,errors=remount-ro'), sdiskpart(device='/dev/sda2', mountpoint='', fstype='swap', opts='rw')] 2）获取磁盘使用率：disk_usage() 统计参数中路径目录的磁盘使用情况。它需要传入一个路径参数；我传入的参数是 “/“，意味着获取当前整个硬盘的使用率。 12345import psutilpsutil.disk_usage(&apos;/&apos;)&gt;&gt; sdiskusage(total=128033574912, used=81997942784, free=46035632128, percent=64.0) 3）获取磁盘 IO 信息： 12345import psutilpsutil.disk_io_counters()&gt;&gt; sdiskio(read_count=510749, write_count=505110, read_bytes=13353246720, write_bytes=8962015232, read_time=275, write_time=238) disk_io_counters() 获取当前磁盘的 I/O 数据信息情况，也就是读取和写入信息。 获取网络信息1）获取整个系统的网络信息 12345import psutilpsutil.disk_io_counters()&gt;&gt; snetio(bytes_sent=77587966, bytes_recv=1113555204, packets_sent=500638, packets_recv=1048467, errin=0, errout=0, dropin=0, dropout=0) disk_io_counters() 返回整个系统所有网卡（包括有线网卡、无限网卡）的进行网络读写数据、发包数等信息。个人认为可以使用该方法来抓包。各个字段含义如下： bytes_sent：发送的字节数 bytes_recv：收的字节数 packets_sent：发送到数据包的个数 packets_recv：接受的数据包的个数 errout：发送数据包错误的总数 dropin：接收时丢弃的数据包的总数 dropout：发送时丢弃的数据包的总数( OSX 和 BSD 系统总是 0 ) 如果增加参数 pernic=True，disk_io_counters() 则会分别输出各个网卡的网络信息数据。 2）获取当前网络连接信息net_connections() 的作用跟系统命令 netstat -an 是一样的。输出当前系统中所有类型的网络连接数据。 1234import psutilpsutil.net_connections()# 数据过多，我就不打印输出结果。 3）获取网络接口信息我们能通过 net_if_addrs() 函数来获取到各个网卡的 IP 地址、网关等信息 12import psutilpsutil.net_if_addrs() 4）获取网络接口状态net_if_stats() 获取的是各个网卡的状态信息，例如网卡是否处于激活状态、当前网速等 12import psutilpsutil.net_if_stats() 获取系统相关信息1) 获取当前登录用户信息 12345import psutilpsutil.users()&gt;&gt; [suser(name='monkey', terminal=None, host='0.125.2.117', started=1515585444.0, pid=None)] users() 是返回当前登录用户的信息。例如用户的名称 name、运行的终端 terminal， 在 Windows 系统下就是我们常说的 CMD 窗口、登录的 IP 地址 host、登录的时长 started以及登录的进程 pid，在 Windows 和 OpenBSD 系统中，该字段为 None。 2）获取系统启动时间psutil.boot_time() 获取的是系统启动的时间点，而不会启动消耗时长。 获取进程信息如果查看当前系统的所有进程信息，你可以使用 test() 方法。跟 Windows 系统下的 tasklist 命令作用类似。 12import psutilpsutil.test() psutil 还提供一列的方法来查看系统进程的信息。 1234567891011121314151617181920212223242526272829303132333435import psutilpsutil.pids() # 列出所有进程的PID&gt;&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 40, 41, 71, 199, 203, 218, 219, 220, 325, 327, 344, 345, 364, 417, 569, 764, 765, 817, 1042, 1058, 1070, 1110, 1186, 1194, 1197, 1209, 1211, 1213, 1215, 1217, 1219, 1228, 1229, 1231, 1298, 1643, 1745, 1794, 1796, 1829, 1846, 2004, 2039, 2154, 2175]p = psutil.Process(2154) # 实例化一个Process对象，参数为一个进程的PIDp.name() # 获取进程名&gt;&gt; 'python'p.exe() # 获取进程 bin 路径, 即安装路径&gt;&gt;'/usr/bin/python'p.cwd() # 获取进程的工作目录&gt;&gt; '/root'p.status() # 获取进程的运行状态&gt;&gt;'running'p.create_time() # 获取进程创建时间点&gt;&gt; 1427469528.49p.cpu_times() # 获取进程使用 CPU 时间信息&gt;&gt; pcputimes(user=0.081150144, system=0.053269812, children_user=0.0, children_system=0.0)p.memory_info() # 获取进程使用的内存&gt;&gt; pmem(rss=8310784, vms=2481725440, pfaults=3207, pageins=18)p.open_files() # 获取进程打开的文件&gt;&gt; []p.connections() # 获取进程相关网络连接&gt;&gt; []p.num_threads() # 获取进程的线程数量&gt;&gt; 1","link":"/131.html"},{"title":"Python 中各种时间类型的转换","text":"我们编码过程中经常需要获取当前时间。当然， 这也离不开对时间类型进行转换运算。本文主要讲解 Python 各种时间类型之间的转换。 处理时间的库Python 标准库中有两个处理时间的库。其中一个名为 datetime，另一个是time。 在 Python 官网文档中，datetime 是被定义为数据类型(Data Types)。由此可见，datetime 是主要提供处理日期和时间的数据类型的模块。它其中有几个常用的类型，例如：datetime.datetime、datetime.time、datetime.date 等，其中最主要的类是datetime.datetime。因为它携带了 datetime.time 和 datetime.date 这两个所带的信息，能够比较齐全地输出，即能一次性就输出年、月、日、时、分、秒等日期和时间信息。 time 模块是归属于通用操作系统服务（Generic Operating System Services）类目中。time 模块主要提供各种时间转换的函数。它服务于系统层次，Python 又是跨平台的，所以有些 API 只能在某些操作系统上使用。 时间类型对象在进行时间转换之前，我们要确认下时间对象是属于哪种数据类型。只有做到对症下药，根治病因。在 Python 中，涉及时间对象有 4 种：1）datetime2）timestamp3）time tuple4）string datetimedatetime 对象属于 datetime 模块。它的构造方式是 datetime(year, month, day, hour=0, minute=0, second=0, microsecond=0, tzinfo=None)。我们了解下它的构造方法即可，一般很少直接使用它的构造方法。我们一般使用它的 now() 函数来获取本地当前日期和时间。 12345678import datetimenow = datetime.datetime.now()print(now)print(type(now))&gt;&gt; 2018-01-17 16:49:24.314323&gt;&gt; &lt;class 'datetime.datetime'&gt; string在某些场景，我们可能需要使用到字符串类型的时间。我们在 now() 函数的基础上再调用 strftime() 函数即可。strftime() 返回的是一个表示日期和时间的字符串。最后显示结果由指定样式的参数决定。 1234567import datetimenow = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")print(now)print(type(now))&gt;&gt; 2018-01-17 17:30:45&gt;&gt; &lt;class 'str'&gt; timestamptimestamp 指的是 Unix 时间戳。它是格林威治时间 1970 年 01 月 01 日 00 时 00 分 00 秒(北京时间 1970 年 01 月 01 日 08 时 00 分 00 秒)起至现在的总秒数。我们使用 time 对象中的 time() 函数能获取到时间戳 。 12345678import timenow = time.time()print(now)print(type(now))&gt;&gt; 1516179935.351417&gt;&gt; &lt;class 'float'&gt; time tupletime tuple 表示时间元组。time tuple 是 time.struct_time 对象类型。获取本地当前时间，一般是使用 time 中的 localtime() 函数。localtime() 返回的是将时间戳经过格式化的本地时间。 1234567import timenow = time.localtime()print(now)print(type(now))&gt;&gt; time.struct_time(tm_year=2018, tm_mon=1, tm_mday=17, tm_hour=17, tm_min=20, tm_sec=34, tm_wday=2, tm_yday=17, tm_isdst=0)&gt;&gt; &lt;class 'time.struct_time'&gt; 时间类型转换上述四种时间类型是如何转换，我本打算以代码的形式加以说明。当后来我看到互联网上已经有前辈整理的关系转换图，我意识到自己这种方式不够简洁明了。所以，我直接献上前辈的宝图。","link":"/132.html"},{"title":"几个Python编程小技巧","text":"本文，猴哥分享几个 Python 编程的小技巧。 编码问题我们在爬取网站是，会经常抓取网页文本，但是打印文本会出现是一堆乱码。这是为什么呢？原因是 Python 中字符对象分为两种，一种是 Unicode 对象，另一种是 str 对象。字符在 Python 中又以 Unicode 对象为基础，所以我们定义的字符串在内存中以 Unicode 编码的形式存储。另外，str 对象又可以有多种编码形式，如 UTF-8、GBK-2312 等。虽然不同编码的 str 对象能被解码成 unicode 对象，但是不同编码的 str 对象直接不能直接转换。因此，如果字符串编码是 GB2312，将其存储到 list 中，再打印出来看到乱码是 Unicode 编码。 解决这个问题其实很简单，Unicode 作为中间编码。我们只要将一种字符编码（如 GB2312）的字符串解码为 Unicode 编码，再编码为另外一种字符编码（如 UTF-8）。 1234# 数据库以 UTF-8 形式保存字符串，而获取到的字符串是 GB2312str = getFromNetWork() # 获取网络字符串，字符编码为 GB2312str.encode('GB2312').decode('UTF-8') print(str) 值交换在 C 或者 Java 中，要将两个变量的值进行交换。我们需要一个临时变量来存储其中一个值。 1234int a=10, b=20,temp;temp = a;a = b;b = temp; 但是在 Python中，有个简单的办法能直接一步到位。 12345678a = 10b = 20print('a = ', a, ' b = ', b)b, a = a, bprint('a = ', a, ' b = ', b)&gt;&gt; a = 10 b = 20&gt;&gt; a = 20 b = 10 单例模式Python 不仅是面向过程的编程语言，而且是面向对象的编程语言。在 Python 中，一个类被初始化，那么 __new__() 函数一定会先被调用，然后再调用__init__()。我们可以采用 hasattr() 函数来判断对象是否包含对应的属性，也就是判断类是否被初始化。 123456789101112131415class Singleton(object): def __new__(cls): # 关键在于这，每一次实例化的时候，我们都只会返回这同一个instance对象 if not hasattr(cls, 'instance'): cls.instance = super(Singleton, cls).__new__(cls) return cls.instanceobj1 = Singleton()obj2 = Singleton()obj1.name = 'GeeMonkey'print(obj1.name, obj2.name)print(obj1 is obj2)&gt;&gt; GeeMonkey GeeMonkey&gt;&gt; True","link":"/133.html"},{"title":"计划分享Python Web学习心得","text":"前段时间，有个读者留言跟我说，有空出使用 Python 实现 RESTful API 的教程。我一看，这正合我意。自己很早就想学习 Python web。之前有简单过了解些 Django 框架基础知识。但对于 Python Web，我还是研究不够深入。 因此，打算接下来一段时间。自己学习 Python Web，并将学习心得分享出来。自己在 Web 方面是只菜鸟，所以请老鸟轻喷。 回到刚才话题，RESTful API 是个什么东西呢？不妨我们先看下平常的网页是怎么回事。我们平时浏览的网站，一般分为前端和后端。我们用浏览器观看页面的内容就是前端的工作。前端采用 Html + CSS + JavaScript 技术来呈现页面内容以及页面效果。后端主要负责维护数据库并返回前端请求数据库的数据。如果我们有个需求，不需要那么华丽、炫酷的页面，只需要后端返回的数据。我们把这样的网络请求称为 RESTful API。再者，REST 描述的是在网络中 Client（PC 浏览器、手机 APP 等） 和 Server的一种交互形式；REST本身不实用，实用的是 RESTful API（REST 风格的网络接口）。 后端已经比较成熟的 Web 框架，我们没有必要重复造轮子。Python Web 主流框架有 Flask、Django、Tornado等 FlaskFlask 是一个使用 Python 编写的轻量级 Web 应用框架。它基于 Werkzeug WSGI 工具箱和 Jinja2 模板引擎。Flask 学习成本比较低，花很少的时间成本就能开发出一个简单的博客网站。如果你时间比较充裕，又想学习 Web 开发。可以学习 Flask ，再以 Flask 做跳板学习其他 Web 框架。 DjangoDjango 是以 Python 编写的高级，MVC 风格的开源库。 Django 也被称为“完美主义者的最后框架”。它最初是为新闻网站设计的，并且允许开发人员编写数据库驱动 Web 应用程序。它算是一个全能型框架。它内置了很多模块，能快速解决大量 Web 痛点问题。另外再加上云平台的支持，这使Django 成为 Web 开发者最受欢迎的选择。大名鼎鼎的 Instagram 网站就是基于 Django 开发的。 TornadoTornado 是传说中性能高高的框架。它支持异步处理的功能，这是它的优势。因为其他框架不具备该功能。但 Tornado 也有致命缺点，那就是扩展库资源比较少。Tornado 除了提供了网站基本需要使用的模块外，剩下的则需要开发者自己进行扩展。 所以，综合以上几点，我就决定深入学习 Django。朋友们，敬请期待我的分享吧。","link":"/134.html"},{"title":"Django 学习笔记之环境搭建","text":"古人云：功遇善其事，必先利其器。在正式学习 Django Web 框架之前，我们要把准备工作做好。准备工作主要是搭建开发环境，具体工作是安装 Python、创建虚拟环境 venv、安装 Django、安装 IDE 工具（Pycharm）。 安装 Python如果你使用的桌面系统是 Windows，你需要到 Python 官网下载安装包。 Linux 和 Mac 系统都自带了 Python 运行环境。Python 分为 2 和 3 版本，目前 Python 团队即将停止维护 Python 2 版本，所以建议安装 Python 3。目前最新版本是 3.6.4。另外，本系列文章适合具备 Python 基础的同学。如果你对 Python 基本语法还是很懵懂，建议你先把基础知识夯实。 创建虚拟环境 venv一提到 Python 虚拟环境，你会惊叹说为什么不用 virtualenv？如果你生产或开发环境需同时支持 Python 2 和 Python 3 ，那就需要 virtualenv。我们是从零开始学习 Django，所以可以直接使用 venv。简单来说，venv 模块是 Python 3.3 版本之后，标准库自带的虚拟环境创建和管理工具，在 Python 3 版本是代替 virtualenv。 为什么要创建虚拟环境呢？你或许会从网上下载一些安全工具或者软件。你害怕这些程序带有后门，甚至是木马程序。所以不敢在自己当前的系统中直接运行。你会使用 VMware 创建一个虚拟机，然后在虚拟机中运行该程序。不管程序是否是病毒，都不会对我当前的系统造成影响。因为虚拟机和当前的系统相互隔离，互不影响。虚拟机出现问题，只要删除即可，不会影响到当前系统。使用 venv 创建虚拟环境也是同样的道理。在当前系统中创建出一个环境，该环境可以跟当前系统互不影响，你可以随意折腾。另外，有了 virtualenv 虚拟环境之后，我们就可以把那个文件夹整体拷贝了，部署起来方便很多。 venv 使用创建 Python 虚拟环境，其实是“创建” 一个文件夹。假如我们需要在 D 盘中创建一个名为 web_dev 的虚拟环境。打开终端，执行以下命令。 123// venv 后面接上创建虚拟环境的绝对路径，建议文件名不要事先存在。// Windows、Mac、Linux 执行命令都是一样，只不过路径不一样python -m venv D://web_dev 执行创建命令之后，你会发现多出了一个名为 web_dev 文件夹，这说明已经创建成功。进入目录，里面有四个文件夹。 创建虚拟环境的完成，只是完成了一半工作。革命还尚未成功，我们还需要激活虚拟环境。依然是打开终端，进入 Scripts 文件夹，运行 activate.bat 来激活虚拟环境。 Linux 下没有 Scripts 这个目录，取而代之的是 bin目录。而激活脚本名则是activate。激活完毕，我们下一步就是安装 Django 库。 安装 Django还是上述的虚拟环境中，我们通过 pip 方式来安装 Django。如果你把终端关闭了，这也意味着把虚拟环境给关闭了。你按照上述激活步骤重新进入虚拟环境即可。 123D:\\web_dev&gt;D://web_dev/Scripts/activate.bat(web_dev) D:\\web_dev&gt;pip install djangoCollecting django 安装 IDE 工具（Pycharm）我们需要到 Pycharm 官网下载安装包。安装版本一定要选择 Professional ！安装版本一定要选择 Professional ！安装版本一定要选择 Professional ！重要的话说三遍~ 因为这个版本集成了很多 Web 开发组件，无须手动。 另附上下载地址：https://www.jetbrains.com/pycharm/download/ Pycharm 是收费版本。如果你有条件的话，可以选择购买正版。或者到网上选择激活码。 初始 Django我之前写了 Django 初始系列文章。你可以先阅读下，这样你对 Django 有整体的认识以及掌握些基本知识（创建项目，运行项目等）。 写在最后我新建一个 Python Web 学习交流 QQ 群，群号：701534112。欢迎大家加群，一起交流，一起学习。","link":"/135.html"},{"title":"Django 学习笔记之初识","text":"上篇文章讲述 Django 环境搭建， 在文章最后部分还有 6 篇 Django 简单入门的文章。后来我自己以一个新手的角度来阅读文章，发现前面三篇文章能被够消化吸收。但是后三篇文章理解起来可能会有点费劲，可能是我漏写了很多细节。因此，本文先将前三盘文章的内容做一些补充说明，降低学习成本。 MVC 与 MTV在 Web 服务器开发领域，MVC 模式可以算是家喻户晓。有些书籍或者文章说 Django 是一个 MVC 开发框架，另一些文章或者博客则说 Django 是 MTV 模式。那么 MTV 模式是什么？Django 又究竟是哪种模式？ Django 是一个遵循 MVC 开发模式的框架 。我们先看下 MVC 的数据流向，了解 MVC 的工作流程。 M 是 Model 的意思 ，它是一个抽象层，用来构建和操作 Web 应用中的数据。同时，Model 层跟数据库打交道的层次，执行数据库数据的增删改查操作。在 Django 项目中，Model 层逻辑是体现在 models.py 中，models.py中定义的各种类代表数据模型 model 。每个 model 是对应数据库中唯一的一张表，每个 model 中的字段也对应表中的字段。 V 指的是 View 层。在 Django 项目中，templates 文件夹中各个模版文件代表视图（View），负责数据内容的显示。templates 文件夹中文件其实就是 HTML、CSS、Javascript 文件。但在 HTML 中使用一些 Django 中特定的特殊语法，就可以实现动态内容插入，从而实现动态页面。 C 全称是 Controller 。它通常是负责从视图读取数据，控制用户输入，并向模型发送数据。在 Django 项目中，urls.py （文件路由）中定义的各种 url 访问入口 和 view.py 中定义的各种处理函数（被称为 Django 视图函数）代表控制器（Controller ）。urls.py 接受用户在浏览器中输入不同 url 地址的请求，然后分发给 view.py 。view.py 再根据文件中对应的函数与数据模型和视图交互，响应用户的请求。即将数据填充到模板（templates）中，呈现给用户。 在实际开发过程中，开发者主要操作对象是 models.py、view.py、templates 文件夹中各个模版文件。这就弱化 C 层的概念， 更加注重关注的是模型（Model）、模板(Template)和视图（Views），所以 Django 也被称为 MTV 框架 。 Django 工作流程了解 Django 的模式，我们来了解 Django 程序是处理一个 HTTP 请求的流程。 图中显示 Django 程度接受到一个 HTTP 请求到返回请求内容的过程。各个路径的含义如下： 用户使用浏览器浏览网页，浏览器向 Web 服务器发起 HTTP 请求。 Web服务器（比如，Nginx）把这个请求转交到一个WSGI（比如，uWSGI），或者直接地文件系统能够取出一个文件（比如返回一个视频文件）。 不像 web 服务器那样，WSGI服务器可以直接运行Python应用。请求生成一个被称为 environ 的 Ptyhon 字典。而且可以选择传递过去几个中间件的层，最终达到 Django 应用。 Django 根据请求的路径，URLconf 将请求分配对应的视图文件。这个请求被封装到 HttpRequest 中。URLconf 可以理解为 URL 以及该 URL所调用的视图函数之间的映射表，通常是记录到 urls.py 中。 被选择的那个视图(Views.py 中的类)会根据页面的需求执行一些操作。例如通过模型（Model）与数据库进行通信；使用模板渲染 HTML或者任何格式化过的响应；访问页面出错，抛出一个异常等。在处理过程中，视图处理的对象主要是 HttpResponse。 当 HttpResponse 对象离开 Django 后，被压缩成二进制流传输给浏览器（HTTP 请求的传输的内容是二进制数据）。 浏览器收到 HTTP 的响应头，呈现给用户。","link":"/236.html"},{"title":"Django 学习笔记之视图与URL配置","text":"本文章是自己学习 Django 框架的第三篇。前面两篇文章主要记录 Django 理论相关知识。从本篇文章开始，将以理论和实战方式讲述 Django 框架的知识。让我们一起来 coding 吧~ 新建项目我们开发 Web 程序是基于 Django 框架，所以要想创建 Django 项目。创建项目有两种方式，一种是使用 Django 管理任务 django-admin.py，另一种是借助 IDE 工具 Pycharm。 使用 django-admin.py 1）新建project在你准备存放项目的目录下，打开终端，执行新建命令。 123django-admin.py startproject Django_demo // Django_demo 为 project 的名称，你可随意命名// 如果执行失败，可以改用下面命令django-admin startproject Django_demo // Django_demo 为 project 的名称，你可随意命名 新建 Project 成功之后，你会发现目录下会多出一些文件。这些文件主要跟工程配置有关系，跟具体业务逻辑没啥关系。 2）新建 application新建了项目，为什么还要新建 application ？简单来说 project **可以理解为一个容器，application** 可以理解为 project 中的一个网站应用。对于每个Django项目有且只有一个 project, 而一个 project 可以包含多个 application。 到最外层的 Django_demo 目录下新建我们的 application 1python manage.py startapp test // test 为 application 的名称，你可随意命名 新建成功后，会发现多了一个名为 test 目录。看到里面有些文件，是否心中有种似曾相识的感觉？ 3）配置应用最后一步，我们需要把刚才新建的 application 加到 settings.py 中的 INSTALL_APPS 中 。具体操作是修改 Django_demo/Django_demo/settings.py 文件， 12345678910INSTALLED_APPS = ( 'django.contrib.admin', // 管理站点 'django.contrib.auth', // 认证系统 'django.contrib.contenttypes', // 用于内容类型框架 'django.contrib.sessions', // 回话框架 'django.contrib.messages', // 消息框架 'django.contrib.staticfiles', // 管理静态文件的框架 // Djaogo 默认包含上面的应用 'test', // 刚才新建的 application，如果有多个 application，依次在后面添加即可。) 借助 Pycharm 有些小伙伴可能使用命令工具方式比较复杂，是否有比较简便的方式？当然，那借助 Pycharm，直接一步搞定。打开 PyCharm IDE 工具，点击 File -&gt; New Project， 左边选择Django。新建如下图所示： 新建成功之后会看到这样的目录结构 ，Pycharm 已经帮我们搞定了大部分工作。 视图与URL配置第一个页面URL配置(URLconf.py) 是文件路由，是 URL 和 URL 对应视图的映射表 。换句话说，就是由它来分发网络请求，将每个 Web 请求根据 URL 地址来调用视图来显示。URL 模式的语法是： 12345678910urlpatterns = [ ''' url(路径匹配, view 函数, 可选参数, 可选别名), 路径匹配： 一个正则表达式字符串。一般写法是 **r'正则表达式'**。如果你对正则表达式不太熟悉，建议你先补习补习下。 view 函数： 一个可调用对象，通常为一个视图函数或一个指定视图函数路径的字符串 可选参数： 可选的要传递给视图函数的默认参数（字典形式） 可选别名： 可选参数，一般结合模板方便管理 ''' url(r'^admin/', admin.site.urls),] 一般在 URLconf.py 中新增一个 URL 表达式，就需要在 view.py 中增加一个视图函数。 我们来根据上述规则创建下首个页面。首先在 view.py 中增加首页视图函数。 12345# Create your views here.from django.http import HttpResponsedef index(request): return HttpResponse(\"Hello ！这是我第一个 Django 项目\") 视图中的函数名 index 对应是 URL 地址中的 path 部分。 那什么是 path 呢？URL 地址定义是 协议://host:port/path 。假如有个地址是 http://127.0.0.1:8000/music。http 就是协议；127.0.0.1 是主机号，主机号可以是 IP 地址或者域名，不过域名是比较常见，因为容易记；port 是端口号，如果端口号是 80 可以忽略不写，其他则要填写完整；music 就是前面说的 path，端口号后面第一个 “/“ 符号后面的字符串都是路径（path）。index 比较特殊，我们通常把 index 设定为首页，所以访问首页的时候，path 不用填写。如果你还不很了解，我们等会运行下程序，你就会涣然大悟。 在创建视图函数之后，我们需要在 urls.py 中配置好 url 匹配规则。 12345678from django.conf.urls import urlfrom django.contrib import adminfrom demo import views # 导入我们的视图urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^$', views.index), # 添加首页 url 匹配规则。index 就是刚才创建的函数。] 配置了 url 匹配规则，剩下最后一步运行验证了。点击 pycharm 工具上的运行按钮（绿色的播放键），然后打开浏览器，在地址栏输入 http://127.0.0.1:8000。你会看到我们 index 函数返回的内容。 如果你访问的地址是 http://127.0.0.1:8000/，同样也是能正常看到页面内容。 pycharm 能启动一个 web 服务器，内部是使用到 manage.py 脚本。因此，我们也可以使用命令行的形式来启动一个 web 本地测试服务器。在最外层的 Django_demo 目录下，打开终端命令行工具，执行 python manage.py runserver 即可启动服务器。 返回 HTML 页面视图返回结果是一串字符串，我们只是用于做测试用的。但是实际开发中，返回结果通常是 html 页面。view 函数想要返回 html 页面，使用 render() 携带一个 html 页面即可。render() 内部返回的也是一个 HttpResponse 对象。 我们在 view.py 中增加名为 content 的视图函数，用来返回一个 html 页面。 12345# Create your views here.from django.http import HttpResponsedef content(request): return render(request, 'content.html') 接着在 application 目录下新建名为 templates 的文件夹，再创建一个 content.html 文件。 12345678910111213&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;content&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;这是 content 页面。&lt;/h1&gt; &lt;h2&gt;这是 content 页面。&lt;/h2&gt; &lt;h3&gt;这是 content 页面。&lt;/h3&gt;&lt;/body&gt;&lt;/html&gt; 还要配置下 url 路由规则。 12345678from django.conf.urls import urlfrom django.contrib import adminfrom demo import views # 导入我们的视图urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^content/$', views.content) # 添加 content 页面的 url 匹配规则。] 最后一步，使用浏览器浏览 content 的页面。 url 路由的命名组url 路由有另种命名组，一种是无名名组，另一种是有名分组。 无名分组是使用简单的、没有命名的正则表达式组（通过圆括号）来捕获 URL 中的值。上述两个例子和以下的一些都是都是无命名分组。 123456789from django.conf.urls import urlfrom demo import viewsurlpatterns = [ url(r'^content/2018/$', views.special_case_2018), url(r'^content/([0-9]{4})/$', views.year_content), url(r'^content/([0-9]{4})/([0-9]{2})/$', views.month_content), url(r'^content/([0-9]{4})/([0-9]{2})/([0-9]+)/$', views.detail_content)] 如果返回的 URL 的 path 为 /content/2018 时，是无法匹配到上面任何一个模式，因为每个模式要求 URL 以一个斜线结尾。paht 为 /content/2018/ 则能匹配到两个模式，是匹配到哪个呢？因为模式按照顺序来匹配的，所以第一个模式会被优先匹配。path 为 /content/2018/02/ 将匹配到第三个模式。Django 调用的是 views 文件中的 month_content(request, ‘2018’, ‘02’)。 无名分组的视图只能接受 python 中传入的固定值参数，如值 2018 等。但是无法获取到存放值的变量，而有名分组恰恰能解决这个痛点。有名分组只是在无名分组的正则表达式上增加一个参数即可。语法是(?Ppattern)，其中 name 是组的名称，pattern 是要匹配的模式。具体改造如下： 123456789from django.conf.urls import urlfrom demo import viewsurlpatterns = [ url(r'^content/2018/$', views.special_case_2018), url(r'^articles/(?P&lt;year&gt;[0-9]{4})/$', views.year_content), url(r'^articles/(?P&lt;year&gt;[0-9]{4})/(?P&lt;month&gt;[0-9]{2})/$', views.month_content), url(r'^articles/(?P&lt;year&gt;[0-9]{4})/(?P&lt;month&gt;[0-9]{2})/(?P&lt;day&gt;[0-9]{2})/$', views.detail_content)] 经过改造之后，当访问 path 为 /content/2018/02/ 时，请求将调用views.content(request, year=’2018’, month=’02’)函数，而不是views.month_content(request, ‘2018’, ‘02’)。 路由转发器（include）如果一个项目下有很多的 application，那么在 urls.py 里面就要写巨多的 urls 映射关系。这样看起来很不灵活，而且杂乱无章。这时候就要根据不同的 application 来分发不同的url请求。 假如在上述的 project 中，我又新建了一个新的 application，名为 app02。配置路由转发器操作如下：首先，在 urls.py 里写入 urls 映射条目。值得注意是，要导入Django 的 include 方法。 1234567from django.conf.urls import url, includefrom django.contrib import adminurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^app02/', include('app02.urls')), // 将 url 为”app02/“的请求都交给 app02 下的 urls 去处理] 其次，在 app02 目录下创建一个 urls.py 文件，用来处理请求的url，使之与 views 建立映射。 1234567from django.conf.urls import include, urlfrom app02 import views urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'index/$', views.index),]","link":"/237.html"},{"title":"Django 学习笔记之模板","text":"本文是自己 Django 学习笔记系列的第四篇原创文章。主要接着篇文章的视图内容，讲解模板的用法。另外也说下 Django 学习笔记系列的安排。自己计划大概 15 篇文章的输出自己学习 Django 框架的内容，再用大概 10 篇文章进行实战开发，最后可能用少量的篇幅进行补充。废话不多说，切回主题。 模板是什么通过之前文章，我们学会使用 render(request, 'content.html') 方法来返回静态页面。但在一些页面中，页面需要根据不同场景（例如时间，角色）显示不同的数据。这就需要使用到模板（Template）。模板通常是 HTML 文件，只不过其中带有特定的语句。这些语句是用来存储并显示数据库中返回的数据。另外，除了 HTML 文件外，Django的模板也能产生任何基于文本格式的文档。 我们就以一个简单的例子来开始学习模板。该模板是一段添加了些变量和模板标签的 html 文件。如果你暂时看不懂其中的内容，没有关系，下面会逐步说明。 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;模板（Template）&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p&gt;Dear {{ person_name }},&lt;/p&gt; &lt;p&gt;It is scheduled to ship on {{ ship_date|date:\"F j, Y\" }}.&lt;/p&gt; &lt;ul&gt; {% for item in item_list %} &lt;li&gt;{{ item }}&lt;/li&gt; {% endfor %} &lt;/ul&gt; {% if ordered_warranty %} &lt;p&gt;Your warranty information will be included in the packaging&lt;/p&gt; {% else %} &lt;p&gt;You did not order a warranty.&lt;/p&gt; {% endif %} &lt;p&gt;Sincerely,&lt;br /&gt;{{ company }}&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 根据上面的代码，让我们逐步分析一下其中的内容： 用两个大括号括起来的文字（例如 ）和 称为 变量 。这里是插入一个变量的值。赋值是在 视图 （views.py）中操作的。 被大括号和百分号包围的文本(例如 {百分号 if ordered_warranty 百分号} )是 模板标签(template tag) 。标签的作用是通知模板系统执行某些操作。 {百分号 for item in item_list 百分号} 是 模板的 for 标签。有点类似 Python 中的 for 语句，能够让你循环遍历序列中的内容。 {百分号 if ordered_warranty 百分号} 则是 if 标签，用于执行逻辑判断。 中用到了 filter 过滤器。这是一种最方便的转换变量输出格式的方式。它的用法跟 Linux 的管道一样，使用管道符 “ | ” 。在这里，我们将变量 ship_date 传递给 date 过滤器，同时指定参数 F j,Y 常用标签从上面的例子中可知，模板中有些常用的标签。让我们来深入了解各个标签的用法。 if/else 标签{百分号 if 百分号} 标签检查一个变量，如果这个变量为真（即，变量存在，非空，不是布尔值假），系统会显示在 {百分号 if 百分号} 和 {百分号 endif 百分号} 之间的任何内容。在每一个 {百分号 if 百分号} 标签后面， 一定要用 {百分号 endif 百分号} 标签来关闭。如： 123{% if is_rain %} &lt;p&gt;外面正在下雨&lt;/p&gt;{% endif %} 如果需要 else 操作， 使用 {百分号 else 百分号} 标签。 12345{% if is_rain %} &lt;p&gt;外面正在下雨。&lt;/p&gt;{% else %} &lt;p&gt;今天是阴天。&lt;/p&gt;{% endif %} {百分号 if 百分号} 标签用法跟 Python 中的 if 语法有些差别。因此需要重点关注下。它不支持用圆括号来组合操作，但支持接受 and ， or 或者 not 关键字来对多个变量做判断。如： 1234567891011{% if sunny and hot %} ...{% endif %}{% if sunny or rainy %} ...{% endif %}{% if not rainy %} ...{% endif %} 另外{百分号 if 百分号} 并没有 {百分号 elif 百分号} 标签， 请使用嵌套的{百分号 if 百分号} 标签来达成同样的效果。 for 标签{百分号 for 百分号} 标签允许我们遍历一个序列上的每一项。在每一次循环中，模板系统会渲染在 {百分号 for 百分号} 和 {百分号 endfor 百分号} 之间的所有内容。 例如，给定一个 图片列表 image_list，我们可以使用下面的代码来显示这个列表： 12345&lt;ul&gt;{% for img in image_list %} &lt;li&gt;{{ img.url }}&lt;/li&gt;{% endfor %}&lt;/ul&gt; 如果你想将列表反方向输出，可以使用 reversed 关键字。 12345&lt;ul&gt;{% for img in image_list reversed %} &lt;li&gt;{{ img.url }}&lt;/li&gt;{% endfor %}&lt;/ul&gt; 在执行循环之前，通常会先检测列表的大小。模板提供了一个标签 {百分号 empty 百分号} 来输出列表为空的提示。 123456&lt;ul&gt;{% for img in image_list %} &lt;li&gt;{{ img.url }}&lt;/li&gt;{% empty %} &lt;p&gt;图片列表不能为空.&lt;/p&gt;&lt;/ul&gt; 除此之外，{百分号 for 百分号} 标签还支持嵌套使用。 1234567{% for page in page_list %}&lt;ul&gt;{% for img in image_list %} &lt;li&gt;{{ img.url }}&lt;/li&gt;{% endfor %}&lt;/ul&gt;{% endfor %} 跟 {百分号 if 百分号} 标签类似，{百分号 for 百分号} 标签的用法不能等同 Python 中的 for 语句。它不支持退出循环操作，即 break 语句；同样，它也不支持 continue 语句。 在每个 {百分号 for 百分号}循环中有一个被称为 ** forloop ** 的模板变量。这变量提供一些带有循环进度信息的属性。 forloop.counter 表示当前循环的执行次数的总数。这个计数器是从 1 开始记录，所以在第一次循环操作是，forloop.counter 会被设置为 1。 forloop.counter0 类似于 forloop.counter ，但是它是从0计数的。 第一次执行循环时这个变量会被设置为0。 forloop.revcounter 是记录循环中还没有被遍历项的总数。循环初次执行时 forloop.revcounter 将被设置为序列的长度。 最后一次循环执行中，这个变量将被置1。 forloop.revcounter0 类似于 forloop.revcounter ，但它以0做为结束索引。因此，第一次循环执行的时候，该变量的值为 序列的长度减 1。 forloop.first 是一个布尔值。如果你需要在第一次循环时，执行一些操作。可以利用该属性。 forloop.last也是布尔类型。用法跟 forloop.first 类似。它的运行场景是最后一个循环。 ifequal 标签比较两个变量的值是在是太常见了，所以 Django 模板提供了 {百分号 ifequal 百分号} 标签提供我们使用。 {百分号 ifequal 百分号} 标签比较两个值，当它们相等时，显示在 {百分号 ifequal 百分号} 和 {百分号 endifequal 百分号} 之中所有的值。如下面的例子是比较两个模板变量 name 和 currentname： 12345{% ifequal name curentname %}&lt;p&gt;名字是一样。&lt;/p&gt;{% else %}&lt;p&gt;名字是不一样。&lt;/p&gt;{% endifequal %} 除了判断两个变量的值，该标签还支持字符串，整数和小数做为参数，但是不支持 Python 的列表类型、布尔类型和字典类型。 模板还有一个比较不相等的 ifnotequal 标签，它的用法跟 ifequal 标签类似。 注释标签如果是需要对单行进行注释操作，使用 # # 标签： 1{# 单行注释 #} 如果要实现多行注释，需用到 {百分号 comment 百分号} 模板标签，就像这样： 123{% comment %}多行注释{% endcomment %} 上下文(context)对象context 对象视图和模板文件的承接桥梁。 context 对象携带视图中需要填充的数据，然后在模版渲染的时候，将数据赋值给模板的变量。模板进而可以渲染显示。 让我们通过下面的例子来了解 context 的用法。在 views.py 中，我们创建一个 current_time 视图，然后用 Django 模板系统修改视图。其内容如下： 12345678from django.template import Template, Contextfrom django.http import HttpResponsedef current_time(request): now = '2018-2-27 20:01:56' t = Template(\"&lt;html&gt;&lt;body&gt;It is now {{ time }}.&lt;/body&gt;&lt;/html&gt;\") html = t.render(Context({'time': now})) return HttpResponse(html) context 和 Python 字典很类似，它以键值对的形式传递参数。context 不仅能传递字符穿和 datetime.date 这样的简单参数值，还能处理更加复杂的数据结构，例如列表、字典和类的对象。模板遍历复制数据结构是用到句点符号(.)。 下面是向模板传递一个 Python 字典的例子。 12345678from django.template import Template, Contextfrom django.http import HttpResponsedef case_dir(request): person = {'name': '极客猴', 'age': '18'} t = Template('{{ person.name }} is {{ person.age }} years old.') html = t.render(Context({'person': person})) return HttpResponse(html) 向模板传递一个类的对象的列子： 12345678910111213# 在其它目录有一个实体类class Person(object):def __init__(self, name, age):self.name, self.age = name, age# 在 views.py 文件中from django.template import Template, Contextfrom django.http import HttpResponsedef case_class(request): t = Template('{{ person.name }} is {{ person.age }} years old.') html = t.render(Context({'person': Person('极客猴','18')})) return HttpResponse(html) 默认情况下，如果一个变量不存在，模板系统会把它展示为空字符串，不会引发一个异常。 加载模板Django 提供模板功能目的是为了让视图和前端页面内容隔开来。同时，前端设计师可能对 HTML 编码比较熟悉，但完全不懂 Python。Python 工程是不一定都熟悉前端的知识。因此，不提倡直接在 视图中混入模板内容。 views.py 中的视图函数只负责加载模板文件，模板一般存放到 templates 文件夹中。 Django 提供了一种使用方便且功能强大的 API，用于从本地中加载模板。当你新建一个新的 Django 项目时，在 setting.py 配置文件中有个 TEMPLATES 选项。TEMPLATES 的 DIRS 属性是记录存放模板文件的绝对路径。 123456789101112131415TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },] 我们无需手动去配置 DIRS，当我们创建目录时，Django 会自动帮我们填充好路径。如果你在 application 目录中创建名为 templates 目录，你会发现 setting.py 中的 TEMPLATES 选项发生变化。 12345678910111213141516TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates')] , 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },] 了解 Django 的机制之后，我们就可以加载模板文件了。因为 Django 已经帮我们搞定模板文件搜索工作。加载模板，我们使用函数 django.template.loader.get_template()，而不是手动从文件系统中加载。调用 get_template() 函数，需要传入模板文件名称的参数。get_template() 函数帮我们实现了打开模板文件，关闭模板文件，异常处理等工作。这大大减轻了我们重复的工作量。 123456789from django.template.loader import get_templatefrom django.template import Contextfrom django.http import HttpResponsedef current_time(request): now = '2018-2-27 20:01:56' t = get_template('current_time.html') html = t.render(Context({'time': now})) return HttpResponse(html)","link":"/238.html"},{"title":"Django 学习笔记之模型(上)","text":"上片文章讲解模板。你本文将讲解 “MTV” 中 M 层次，即模型层（数据存取层）。模型这内容比较多，我将其拆分为 3 个部分来讲解。同时，文章也配套了例子，你可以通过 阅读原文 来查看。 编程环境因为 Django 近期推出 Django 2.0 版本， 所以有必要再说明下。如果你是按照本系列来学习 Django 框架的话，按照前面安装 Django 的方式，你安装 Django 版本应该是最新版本，即 2.0。 那么使用最新 Django 版本来学习可以吗？如果是学习的话，不用太在意版本。当然学习最新的较好，因为可以学习新的 API。同时，Django 2.0 不再兼容 Python 2 了，现在学习 Python 都建议采用 Python 3版本了。另外 Django 1.8 官方只维护到 2018 年的 4 月，1.11 是最后一个兼容 Python 2 的 Django版本。如果是项目需要升级 Django版本，需要兼容到 Python 2，那么要考虑用 1.11 版本了。 顺便补充下本文用的一些工具的版本：Python 版本是 3.6，Mysql 版本是 5.5. 模型是什么在 Web 应用中，数据一般存储到数据库中。Django 中的模型层是跟数据库打交道的层次。模型层中可能会有多个模型，每个模型（每个 app 中的 models.py 中每个类都是一个模型）都对应着数据库中的唯一一张表。 配置数据库在我们探索 Django 的模型层之前，我们需要配置下数据库；告诉 Django 视野什么数据库以及如何连接数据库。这一步要确保配置无误，不然后面难以执行。我们找到新项目中的 setting.py, 里面有个 DATABASES 选项。Django 默认是使用 sqlite 数据库，所以你会看到里面 sqlite 数据库的配置信息。 123456DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), }} 上面的代码中的 ENGINE 是选择哪个数据库引擎, NAME 是数据库的名字。至于选择哪种引擎，要看项目中使用了什么数据库。Django 目前支持以下这 MySQL、PostgreSQL、Oracle 等数据库，它们的数据库引擎设置如下： 设置字段 数据库 设配器(驱动) postgresql PostgreSQL psycopg 1.x版 postgresql_psycopg2 PostgreSQL psycopg 2.x版 mysql MySQL MySQLdb sqlite3 SQLite 不需要 oracle Oracle cx_Oracle 其中设置字段是填充 ENGINE 的值。如果你使用的 MySQL 数据库，那么你需要填写 django.db.backends.mysql。数据库驱动表示需要使用 pip 安装该库。如果你使用的 MySQL 数据库，那么你需要安装 MySQLdb 设配器。 但是这里有个坑，MySQLdb 在支持 Python 2 版本，不支持 Python 3 版本。所以你安装该设配器之后，运行项目会报出错误。Django 官网建议使用替代品 mysqlclient。mysqlclient 是 MySQLdb 的一个分支，最主要是它支持 Python 3。 本文中使用到 Mysql 数据库，那么 DATABASES 的配置如下： 12345678910DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'django', 'HOST': '127.0.0.1', 'PORT': '3306', 'USER': 'root', 'PASSWORD': '123456', }} 第一个模型我们先新建名为 Django_demo 的 projeact, 再新建名为 demo 的 app。最后，别忘记在 setting.py 中将新创建的 app 激活。 12345678910# Django 2.0 的初始配置内容INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'demo', # 我们新创建的 app] 创建模型打开我们刚才创建的 app 中的 models.py 文件，我们以后就主要在这里编写模型。假设现在有个需求：某个作者创作了一本书，本书由出版社出版了。我们可以设定三者的字段以及关系。 假设作者有姓名、Email 邮箱这两个数据属性。 假设出版社有出版社名称、地址这两个属性。 书籍有两四个属性：书名、出版日期、作者、出版社。有一个或多个作者（和作者是多对多的关联关系[many-to-many]）， 只有一个出版商（和出版商是一对多的关联关系[one-to-many]，也被称作外键[foreign key]） 所以我们编写代码如下： 1234567891011121314151617from django.db import modelsclass Author(models.Model): name = models.CharField(max_length=40) email = models.EmailField()class Publisher(models.Model): name = models.CharField(max_length=30) address = models.CharField(max_length=50)class Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author) # publisher = models.ForeignKey(Publisher) # Django 2.0 需要加上 on_delete=models.CASCADE publisher = models.ForeignKey(Publisher, on_delete=models.CASCADE) publication_date = models.DateField() 我们定义每个模型，即类（如 Author 和 Book）都需要继承 django.db.models.Model。Model 是 Django 做了一层包装以便我们更加方便地使用的类， 它其中包含了所有数据库交互的方法。上面代码中的每个类相当于单个数据库表，每个属性也是这个表中的一个字段。 属性名就是字段名，它的类型（例如 CharField ）相当于数据库的字段类型 （例如 varchar ）。例如， Publisher 模块等同于下面这张表 12345CREATE TABLE \"Publisher\" ( \"id\" serial NOT NULL PRIMARY KEY, \"name\" varchar(30) NOT NULL, \"address\" varchar(50) NOT NULL,); 所以我们在为属性命名的时候，要避免使用数据库的关键字。另外，我们只需要关心每个类的属性以及长度限制，不用关心怎么创建数据库表。Django 可以自动生成这些 CREATE TABLE 语句的。 创建数据表我们上面的创建了几个模型还处于定义上，Django 还没有正真创建数据库中的表。因此，我们需要执行两个命令来同步一下数据库。 在我们刚才创建的工程 Django_demo 目录下，我们打开终端，执行以下命令： 1python manage.py makemigrations 运行成功效果图如下： 这一步相当于 在该app下建立 migrations目录，并记录下你所有的关于modes.py的改动，比如0001_initial.py， 但是这个改动还没有作用到数据库文件 第二步，在之前的终端上继续执行 migrate 命令。 1python manage.py migrate 运行成功效果图如下： 这一步表示将该改动（当makemigrations之后产生了0001_initial.py 文件）作用到数据库文件，比如 create table 之类。 如果你用到 Pycharm 的 Database 功能，你会看到我们刚才创建的定义的几个模型。 字段类型在前面的例子中，我们需要对每个属性设置一个字段，例如 CharField。Django 内置了几十种内置字段类型。常用的类型如下： 1) AutoField：一个根据实际ID自动增长的 IntegerField 。如果表中没有设置主键时，将会自动添加一个自增主键。 2）IntegerField：一个整数。在 Django 所有支持的数据库中，-2147483648 到 2147483647 范围才是合法的。 3）BigIntegerField：一个64位整数, 和 IntegerField 类似，不过它的范围比较大。 4）BooleanField：一个 true/false 字段。这个字段的默认表单部件是 CheckboxInput。 5）CharField：字符字段。对于比较大的文本内容,请使用 TextField 类型。这个字段的默认表单部件是 TextInput。它有个参数 max_length。max_length 表示字段允许的最大字符串长度。这将在数据库中和表单验证时生效 6）TextField：大文本字段。默认的表单部件是一个 Textarea。 7）DateField：日期。它带有两个可选参数：auto_now 和 auto_now_add。auto_now 表示当对象保存时，该字段会自动设置成当前时间。一般用于记录“修改时间” 。auto_now_add 记录字段首次被创建的时间。 8）DateTimeField：时间和日期。它也带有两个可选参数，名字和用法跟 DateField 一样。 9）TimeField：时间字段, 类似于Python datetime.time 实例. 和 DateField 具有相同的选项。 10）URLField：一个 CharField 类型的URL，默认长度是200；默认的表单部件是一个 TextInput。 11）EmailField：一个检查输入的email地址是否合法的 CharField 类型。 12）FileField：上传文件字段。 13）ImageField：图片字段，它继承了 FileField 所以属性和方法。 关系字段关系字段(Relationship fileds) 也是属于字段，只不过三个字段比较特殊，所以单独拿出来说。我们按照上述的创建模型的例子来继续讲解。它们三者之间的关系应该这样：一本书由一家出版社出版，一家出版社可以出版很多书。一本书由多个作者合写，一个作者可以写很多书。 1）ForeignKey表示属于模型间关系中的多对一关系。在我们的范例模型中，一家出版社 publisher 可以出版很多书 Book。在数据库中, Django 使用 ForeignKey 字段名称＋ “_id” 做为数据库中的列名称。在上面的例子中, 书籍 model 对应的数据表中会有一个 publisher_id 列。你可以通过显式地指定 db_column 来改变该字段的列名称,不过，除非你想自定 义 SQL ，否则没必要更改数据库的列名称。 它第一个参数必须传入该模型关联的类。on_delete 现在可以用作第二个位置参数(之前它通常只是作为一个关键字参数传递). 在Django 2.0中，这将是一个必传的参数。 2）OneToOneField它属于 ForeignKey 中的特例。当 ForeignKey 中有个字段 unique 被设置为 True 时， 就表示一对一关系。 3）ManyToManyField属于模型间关系中的多对多关系。在我们的范例模型中， Book 有一个 多对多字段 叫做 authors。因为他们的关系是一本书由多个作者合写，一个作者可以写很多书。在数据库中Django 创建一个中间表来表示 ManyToManyField 关系。默认情况下，中间表的名称由两个关系表名结合而成。所以刚才我们创建数据库表的途中，会有四张表，而不是三表。 字段选项有些字段会有些特殊参数，但所有字段类型都又些通用的可选选项。先是常用的可选选项。1）null ：如果该参数设置为 True，Django将会把数据库中的空值保存为 NULL。不填写就默认为 False。 2）blank：如果为 True ，该字段允许为空值，不填写默认为 False。这个字段是用于处理表单数据输入验证。 3）primary_key：如果为 True，那么这个字段就是模型的主键。 4）unique：如果该值设置为 True, 这个数据字段在整张表中必须是唯一的。 5）default：设置该字段的默认值。 6）由二项元组构成的一个可迭代对象（列表或元组），用来给字段提供选择项。 如果设置了 choices，默认的表单将是一个选择框。具体使用例子如下： 12345678910from django.db import modelsclass Person(models.Model): SHIRT_SIZES = ( ('S', 'Small'), ('M', 'Medium'), ('L', 'Large'), ) name = models.CharField(max_length=60) shirt_size = models.CharField(max_length=1, choices=SHIRT_SIZES) 下篇文章，我们将讲解如果对这些模型（表）进行操作。","link":"/239.html"},{"title":"Django 学习笔记之模型(下)","text":"上篇文章讲解了 Django 如何创建模型，本文将继续讲解如何对模型进行增删改查操作。 前言当我们建立好数据模型，Django 会自动为我们生成一套数据库接口相关的接口。这套接口称为 QuerySet API。为什么叫 QuerySet ? 因为从数据库中查询出来的结果一般是一个集合，这个集合叫做 QuerySet。 为了方便理解，我继续使用上篇文章的例子。另外方便我们在打印对象信息时，能得到对象的信息。所以我们需要对之前的代码做下修改。分别为每个模型类添加一个方法 unicode()。 unicode() 方法告诉 Python 如何将对象以 Unicode 的方式显示出来。 为以上三个模型添加 unicode() 函数后，就可以看到效果了： 123456789101112131415161718192021222324from django.db import modelsclass Author(models.Model): name = models.CharField(max_length=40) email = models.EmailField() def __unicode__(self): return self.nameclass Publisher(models.Model): name = models.CharField(max_length=30) address = models.CharField(max_length=50) def __unicode__(self): return self.nameclass Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author) publisher = models.ForeignKey(Publisher, on_delete=models.CASCADE) publication_date = models.DateField() def __unicode__(self): return self.title unicode() 方法可以进行任何处理来返回一个 unicode 对象。总所周知，Python 内部对字符串都是使用 Unicode 来保存的。不像字符串那样，有什么 UTF-8、GB2312 等编码。所以我们在Python 中处理 Unicode 对象的时候，你可以直接将它们混合使用和互相匹配而不必去考虑编码细节。 创建对象为了更加直观的操作数据库，我使用 Django 的 API 来讲解。在项目的目录下，打开终端执行以下命令： 1python manage.py shell 然后在终端中依次输入以下代码： 1234# 前面的 &gt;&gt;&gt; 是终端自带的&gt;&gt;&gt; from demo.models import Publisher&gt;&gt;&gt; p = Publisher(name='清华大学出版社', address='北京')&gt;&gt;&gt; p.save() 接着使用 Pycharm 的 Database 功能，查询 demo_publisher 表，你会发现新增一条数据。 现在来说说刚才发生了什么。第二行代码，即初始化一个 Publisher 实例， 这个实例并没有对数据库做修改。只有调用了 save() 函数，记录才会提交到数据库。所以， 使用这种方法创建实例，最后一定要调用 save() 函数。 另外上述方法来创建实例，另外还有 3 种方法：1）方法2这种办法可以算是方法 1 的变形。 123456from demo.models import Publisherp = Publisher()p.name='北京大学出版社'p.address='北京'p.save()# 别忘记最后调用 save() 函数 2）方法3这种办法是用到 objects.create() 函数 123from demo.models import PublisherPublisher.objects.create(name='人民邮电出版社', address='北京')# 这种办法无需调用 save() 3）方法4这种办法是用到 objects.get_or_create() 方法。使用这种办法有好处也有坏处。好处是可以防止重复插入；那么坏处就是插入速度要相对慢些，因为它要先查询。 12345from demo.models import PublisherPublisher.objects.get_or_create(name='清华大学出版社', address='北京')# 这种办法无需调用 save()# 执行结果如下：(&lt;Publisher: Publisher object (1)&gt;, False) 返回结果跟其他方法返回结果有点不同，它返回是一个元组。第一个是 Publisher 对象；第二个为一个布尔值，如果能新建成功为 True，已经存在则为 Flase。 如果模型中存在有一对多，多对一，多对多的关系，先把相关的对象查询出来或者创建出来，才能创建该模型。例如我们要创建 Book 对象，首先要创建 Author 和 Publisher 对象。具体代码如下： 12345678910111213import datetimefrom demo.models import Publisher, Author, Book# 获取 Publisher 对象pub = Publisher.objects.get(name='清华大学出版社')# 创建并获取 Publisher 对象Author.objects.create(name='令狐冲', email='makeTheFoxRush@xx.com')aut = Author.objects.get(name='令狐冲')# 创建 Book 对象, 要注意添加 id 字段book = Book(id=None,title='令狐传', publisher=pub, publication_date=datetime.date.today())# 一定要先保存数据到数据库，才能添加多对多关系的对象 authorbook.save()book.authors.add(aut)book.save() 查询对象Django 提供在查询数据功能方面做了很多优化工作， 这让我们查询数据有千万种方法。 查询单条数据其实在上面的例子， 我们已经运用到单条数据功能。没错，就是使用 get() 方法来获取单条数据。 1pub = Publisher.objects.get(name='清华大学出版社') 查询多条数据如果现在我们需要查询符合某个条件的数据，get() 只能返回一条数据，无法满足我们的需求。所以我们需要用到过滤器 filter。 12345from demo.models import PublisherPublisher.objects.filter(address='北京')# 运行结果&lt;QuerySet [&lt;Publisher: Publisher object (1)&gt;, &lt;Publisher: Publisher object (2)&gt;, &lt;Publisher: Publisher object (3)&gt;]&gt; 查询多条数据的返回结果为 QuerySet，这部分等会继续讲解。 另外 filter 还支持其他过滤条件，例如 123456789101112131415# 正则表达式Publisher.objects.filter(address__regex='^北京')# 部分查询# 查询地址中包含'北'字的出版社Publisher.objects.filter(address__contains='北')# 还有很多不一一例举# 如果上述条件的前面有个字母 'i', 表示不区分大小写# 正则表达式，但不区分大小写Publisher.objects.filter(address__iregex='^beijing')# 部分查询，但不区分大小写Publisher.objects.filter(address__icontains='bei') 如果要查询全部数据或者一段连续区间的数据，可以使用 all() 函数 12345# 查询所有数据Publisher.objects.all()# 查询一段连续区间的数据Publisher.objects.all()[:3] [:3] 是用到 Python 中的切片操作。因为上限从 0 开始可以忽略不写，所以它等同于 [0:3]。查询出来结果没有包含上限的值，即下标为 3 的值。[:3] 只查询下标为 0, 1 ,2 的数据。 但是这里比较特殊，QuerySet 对象的 id 是从 1 开始的，所以 [:3] 表示 [1:3], 返回 id 为 1, 2, 3 的对象。 另外，这种切片操作时可以节约内存的。 更新数据更新数据操作，一般是在查询数据后才执行。 更新单条数据更新单条数据也有两种方法，其中一种的用法跟使用方法 2 创建对象类似，另一种则是使用 update_or_create() 。具体代码如下： 12345678910# 方法1from demo.models import Publisherp = Publisher.objects.get(name=\"北京大学出版社\")p.name='上海大学出版社'p.address='上海'p.save()# 别忘记最后调用 save() 方法# 方法2p = Publisher.objects.update_or_create(name='北京大学出版社', address='上海') update_or_create() 方法是以模型的其中一个属性去匹配，如果数据库中有匹配数据就更新后面的值，否则则创建新的数据。 更新多条数据批量更新多条数据，一般是在 all()，filter() 后面执行 update() 函数 12from demo.models import Publisherp = Publisher.objects.filter(address='北京').update(address='北京海淀') 删除数据删除单条数据或多条数据，用法跟更新数据类似。具体就不逐一展开讲解了，大概说下用法即可。删除单条数据，获取数据，然后调用 delete() 函数。删除多条数据，同样在获取数据后调用 delete() 函数。 QuerySet 用法前面讲到，使用 all()，filter() 查询多条数据，返回的结果是一个 QuerySet 对象。它不是个列表，但是可以使用** list() 将其转变为列表**。 123from demo.models import Publisherpubs = Publisher.objects.all()pub_list = list(pubs) 可跌代性QuerySet 是一个可迭代的对象。因此，可以使用 for 循环来遍历。代码如下： 1234567891011# 在 views.py 中from django.http import HttpResponsefrom django.shortcuts import renderfrom demo.models import Publisher# Create your views here.def index(request): pubs = Publisher.objects.all() for p in pubs: print(p.name) return HttpResponse(\"Hello\") 你会在 Pycharm 的控制台上看到查询的数据。 支持排序QuerySet 支持对查询结果排序。例如将出版社按照名称来排序， 123456789101112131415161718# 在 views.py 中from django.http import HttpResponsefrom django.shortcuts import renderfrom demo.models import Publisher# Create your views here.def index(request): pubs = Publisher.objects.all().order_by('name') print(=== 正序排序 ===) for p in pubs: print(p.name) # 在 column name 前加一个负号，可以实现倒序 repubs = Publisher.objects.all().order_by('-name') print(=== 反序排序 ===) for p in repubs: print(p.name) return HttpResponse(\"Hello\") 组合查询QuerySet 还支持跟 SQL 语法中的组合查询，具体用法如下： 123456# 查询结果中同时满足 name=清华大学出版社 和 address=上海, 这两个条件Publisher.objects.filter(name=\"清华大学出版社\").filter(address=\"上海\")# 查询结果中同时满足 name=清华大学出版社 和 address 不是上海, 这两个条件# exclude() 函数排除指定的内容Publisher.objects.filter(name=\"清华大学出版社\").exclude(address=\"上海\") 其他前面说到切片操作的区间是从整数到无穷大的，在Python 语法中还有负查询，即区间是从负无穷大到 0。可惜的是 QuerySet 是不支持负查询。 但是也有替代方法 123# 使用 reverse() 将 QuerySet 的顺序倒置下# 再使用正查询， 下面的例子表示查询最后两条数据。Publisher.objects.all().reverse()[:2] 如果还要获取 QuerySet 里面存放对象的个数，可以使用 count() 函数来查询数量。内部实现是用执行 SELECT COUNT(*) SQL 语句。 12count = Publisher.objects.all().count()print('count == ', count)","link":"/340.html"},{"title":"Django 学习笔记之后台管理","text":"本文是 Django 学习笔记系列的第七篇。前面 6 篇文章，我们已经了解了 Django MTV 模型中三个层的内容。这部分内容算是最基础，也是最重要。本文的内容相对简单，阅读起来会比较轻松些。主要是介绍下 Django 默认管理后台以及一些实用后台管理系统的第三方应用。 前言每个网站无论大小，大型电商网站也好，个人博客也罢，它们都是一个管理后台。管理后台可以看做一个窗口，管理员通过它来管理以及维护网站。 Django 作为一个全能型的框架，当然也自带了一个后台管理系统。登录后台管理希望能对前端或者数据库数据进行增加、修改、删除等工作。我们现在就激动该系统来学习。 激活管理界面其实 Django 默认帮我们激活 admin 管理后台。不知你还记得上次的操作？ 当新建创建应用的，需要将刚创建的 app 加入到 setting.py 文件中。在 setting.py 文件中，你会看到前面有很多应用。 123456789INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'demo', # 我们新创建的 app] 其中 django.contrib.admin 就是管理后台。因此，我们可以知道 admin 其实也是一个应用，只不过它是 Django 自带的。 我们只需要做一件事，那就是创建用户。在项目目录下，打开终端，执行以下命令来创建超级管理员。 1python manage.py createsuperuser 打开终端之后，依次输入用户名，邮箱和密码即可创建。成功创建换管理员结果如下： Django 在密码校验这块做的还不错。我使用弱密码 ‘admin’和’1234678’，Django 都不让我通过。 成功创建超级用户之后，使用浏览器访问后台管理系统： 127.0.0.1:8000/admin。 输入用户名和密码并进行登录，会看到以下界面。因为是空项目，所以没有数据，只有显示用户账号的信息。你可以对 admin 进行二次开发，这样你就能在后台对数据库数据进行操作。 常用的 admin 应用我推荐这几个应用都是第三方开源项目，都支持 Django 2.0 版本。具体安装方法可以阅读各个插件的 Github 仓库地址。 1) Xadmin Xadmin 在 Django 后台管理系统应用库中算是名声显赫。它算是一款内置功能比较丰富的框架，提供了基本的CRUD功能，还内置了丰富的插件功能；还包括数据导出、书签、图表、数据添加向导及图片相册等多种扩展功能。 最重要的是，它使用起来非常方便。我们只需要定义数据的字段等信息，即可获取一个功能全面的管理系统。 推荐指数：5 星github 地址: 传送门 2) django-grappelli django-grappelli 可以算是一个功能强大的应用。它使用网格状的形式来呈现数据，这个给人简洁大方的感觉。因此，django-grappelli 更加适合需要对数据频繁操作的场景。 推荐指数：4 星github 地址：传送门 3) django-material django-material 是采用谷歌 Material Design 来设置 UI 。自己比较喜欢 MD 这种风格的界面，所以推荐给大家。 推荐指数：3 星半github 地址：传送门 4) django-admin-bootstrap django-admin-bootstrap 是一个能自动隐藏侧面菜单栏的响应式管理后台。它跟 Xadmin 一样，都是基于 bootstrap 开发的。个人觉得比较适合初学者来学习和研究。 推荐指数：3 星github 地址：传送门","link":"/341.html"},{"title":"Django 学习笔记之表单","text":"本文是自己 Django 学习笔记系列中第 8 篇，算是基础知识篇章中最后一篇笔记。后续的笔记内容会相对比较综合。所以建议大家要把前面的内容，包括本篇笔记掌握。而本篇内容主要是讲解表单。 表单是什么？表单英文单词是 Froms, 它其实属于 HTML 的知识范畴。HTML 表单可以实现用户和 Web 站点之间数据交互。表单允许用户将数据发送到 Web 站点。 但在大多数情况下，Froms 携带的数据发送到 Web 服务器，Web 页面会将其拦截并自己使用它。举个栗子，用户使用浏览器访问一个页面，在页面的搜索框中输入图书的名称，想获取所有销售该图书的商店。Web 站点需要获取图书名称的信息作为数据库查询条件，所以将数据拦截并获取图书的名称。然后通关查询数据库，最后将查询到的所有商店信息返回给浏览器进行渲染显示。另外，博客系统中的评论模块也是这个原理。因此，在一些站点上会爆出 XSS 漏洞。原因可能是编码者没有对用户提交的数据进行过滤或者过滤不严，直接存储到数据库中。 HTML 表单这部分是给不熟悉 HTML 表单同学准备的，如果你已经掌握这部分知识。可以选择直接跳过。 HTML 表单在页面中表现是一个可以填写数据的区域。表单中会根据页面显示需求，采用不同的表单元素来呈现，比如：文本域(textarea)、下拉列表、单选框(radio-buttons)、复选框(checkboxes)等等 它可能长得这个样子 表单使用标签 form 来设置区域范围，它携带常用的属性如下： 123456&lt;form action=\"search.html\" method=\"GET\" target=\"_blank\" &gt; &lt;!-- ... 表单元素 --&gt;&lt;/form&gt; action 属性：指定表单数据提交到哪个页面。例子中是提交到 search.html 页面，这个也会跳转到 search.html 页面。如果你想把数据提交到原来的页面，action 的值为空就行，即 action=”” method 属性：规定提交表单时所用的 HTTP 方法，一般选择** GET 或者 POST**。 target 属性：规定 action 属性中地址的目标（默认：_self）。如果填写值** _blank** ，当点击按钮提交数据时，在新窗口中打开新的页面。 常用表单元素有以下这些： 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt; &lt;form action=&quot;&quot; method=&quot;GET&quot; &gt; &lt;!-- label 标签用于显示内容，不可以输入 --&gt; &lt;label &gt;我是 label 标签&lt;/label&gt;&lt;br&gt; &lt;!-- input 元素 --&gt; &lt;!-- 单行的文本输入框 --&gt; &lt;input type=&quot;text&quot; name=&quot;lastname&quot;&gt;&lt;br&gt; &lt;!-- 单选按钮 --&gt; &lt;input type=&quot;radio&quot; name=&quot;sex&quot; value=&quot;female&quot;&gt;female&lt;br&gt; &lt;!-- 提交按钮, 用于提交表单数据 --&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;&gt; &lt;!-- 还有其他的表单元素, 就不一一列举 --&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 对于每个输入字段 ，必须设置一个 name 属性，数据才会被正确提交。因为页面是通过 name 属性中的值来获取用户输入的内容的。以 GET 方式请求为例，有个单行输入框定义 **name=”q”。当你在输入框中填写值 moneky 然后提交。你会发现浏览器地址发生变化了，从之前的 127.0.0.1:8000 变为 127.0.0.1:8000/?q=monkey Django Form功能Django 的表单针对 HTML 表单实现了一层封装，这使得 Django 的 Form 表单功能更加强大。它具有以下功能：1）自动生成HTML表单元素2）检查表单数据的合法性3）如果验证错误，重新显示表单（数据不会重置）4）数据类型转换（字符类型的数据转换成相应的Python类型） Form 对象Objects Form对象封装了一系列 Field 和验证规则，Form 类都必须直接或间接继承自 django.forms.Form，定义 Form 有两种方式： 方法一：根据 Model 自动生成 Form如果你的需求比较简单，只想将模型的字段全部以表单的形式展示出来，你可以采用这种方法。 123456from django import formsclass AuthorFormOne(forms.Form): name = forms.CharField(max_length=40,label='名称') email = form.EmailField() message = form.CharField(widget=forms.TextInput) Form 表单除了定义属性跟模型差不多，它还具有一些特有的属性。 1) Widget用来渲染成 HTML 元素的工具，如：forms.TextInput 对应 HTML中的 input标签2) Form一系列 Field 对象的集合，负责验证和显示 HTML 元素。3) Form Media用来渲染表单的 CSS 和 JavaScript 资源。 方法二：自定义 Form自定义表单是比较高级用法，有时候通过 Model 自动创建的 Form 无法满足自己需求。譬如：Model 中的某些属性我不需要显示在页面上，或数据处理方式比较复杂，这个时候你就需要自定义 Form。自定义 Form 是直接继承 Form 12345678910# 在 models.py 定义模型 Authorclass Author(models.Model): name = models.CharField(max_length=40) email = models.EmailField()# 新建 forms.py 文件, 用于定义 Formclass AuthorFormTwo(ModelForm): class Meta: model = Author fields = ('name',) # 只显示 model 中指定的字段 视图层的处理在视图文件 view.py 中， 可以获取、过滤到用户提交的数据。 123456789101112131415161718192021222324from django.http import HttpResponseRedirectfrom django.shortcuts import render# Create your views here.from demo_form.form.forms import AuthorFormOnedef formView(request): # 过滤 POST 方法的请求 if request.method == \"POST\": form = AuthorFormOne(request.POST) # 验证表单 if form.is_valid(): # 一般过滤数据 name = form.cleaned_data['name'] email = form.cleaned_data['email'] information = form.cleaned_data['information'] # 处理业务, 如查询数据库信息 return HttpResponseRedirect('/') else: # 不是 GET 请求则显示表单 form = AuthorFormOne() templateView = 'author.html' return render(request, templateView, {'form':form}) form.is_valid() 返回 true 后，表单数据都被存储在 form.cleaned_data 对象中（字典类型，意为经过清洗的数据）。而且数据会被自动转换为 Python 对象，如：在 form 中定义了 DateTimeField ，那么该字段将被转换为 datetime 类型。 而模板文件内容则比较简单，使用几个 HTML 标签以及模板标签就轻松搞定。 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;author 表单&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action='/forms/' method='POST'&gt; {% csrf_token %} {{ form }} &lt;input type=\"submit\" value=\"提交\" /&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 我在 url.py 定义表单的路径是： 123456789from django.contrib import adminfrom django.urls import pathfrom demo_form import viewsurlpatterns = [ path('admin/', admin.site.urls), path('forms/', views.formView),] 所以使用浏览器访问 http://127.0.0.1:8000/forms/， 你会发现页面会自动渲染出表单的信息。 美化模板我们虽然成功把表单内容渲染到页面上，但是页面有点丑陋。你可能会无法忍受，想把页面修改得美观一点，顺便也秀秀自己的 Bootstrap 知识。 Django 默认提供几种显示表单的方式。例如 form.as_p、form.as_table、form.as_ul，在 html 文件中会被渲染成 p 标签，table 标签和 ul 标签。除此之外，还可以 form 还支持自定义。具体实现是你获取到 form 中每个属性，然后逐一渲染指定样式。 所以 author.html 经过调整之后的代码如下： 123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;author 表单&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action='/forms/' method='POST'&gt; &lt;div class=\"\"&gt; {{ form.name.errors }} &lt;label for=\"name\"&gt;姓名：&lt;/label&gt;&lt;br&gt; {{ form.name }} &lt;br&gt; &lt;/div&gt; &lt;div class=\"\"&gt; {{ form.email.errors }} &lt;label for=\"email\"&gt;邮箱：&lt;/label&gt;&lt;br&gt; {{ form.email }} &lt;br&gt; &lt;/div&gt; &lt;div class=\"\"&gt; {{ form.information.errors }} &lt;label for=\"information\"&gt;个人信息：&lt;/label&gt;&lt;br&gt; {{ form.information }} &lt;br&gt; &lt;/div&gt; &lt;p&gt;&lt;input type=\"submit\" value=\"提交\" /&gt;&lt;p&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt;","link":"/342.html"},{"title":"Django 学习笔记之使用旧数据库","text":"如果你按照顺序，从第一篇文章读到本文。恭喜你，你已经将 Django 大部分基础知识掌握了。后续的文章是在之前的基础上添砖加瓦或常用的应用。本文将的内容是一个场景应用，新项目使用旧数据库。 可能以前项目是使用其他语言，如 Java 或 PHP 开发的，后面迁移到 Python 上。虽然应用程序改变了，但是数据缺不是丢弃。因此，存在这样的问题。那就是使用 Django 开发的 Web 应用程序如何使用旧的数据库？ 我就使用旧的 SqLite 数据库作为例子进行讲解，MySQL 等其他数据库也是操作类似。 导入数据库旧的数据库名为 MyDataBase.db, 我将其导入到新项目的 db 目录。 然后将 settings.py 文件中的数据库名称修改下。 12345678DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', # 系统自动生成 # 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), 'NAME': os.path.join(BASE_DIR, './db/MyDataBase.db'), }} 创建模型我们知道 Models 层是跟数据库打交道的层次，需要创建数据库表对应的 models。但对应使用旧数据库，我们不需要手动创建，可以使用 Django 提供的 API 反向生成 models。 假设数据有一张表，其创建表的 SQL 语句如下： 123456789101112Create table if not exists `Users` ( `id` INTEGER PRIMARY KEY, `username` varchar(64), `password` varchar(64), `email` varchar(64) , `QQ` varchar(15) , `weibo` varchar(300), `cell_phone` varchar(20), `college` varchar(60), `living_place` varchar(200), `remarks` varchar(400) ); 我们进入到项目的目录下，打开终端，执行以下命令： 1python manage.py inspectdb 目的是针对已有数据库自省生成新的 models。 然后执行导出命令，将模型导出到 models.py 文件中。 1python manage.py inspectdb &gt; models.py 你会看到项目根目录下多了一个名为 models.py 文件。 将其内容迁移到 app 的 models.py 中。默认配置下生成不可修改或删除的 models，所以我们修改 meta class 中的 managed 属性。如果 managed 被设置为 True，则告诉 Django 可以对数据库进行操作。 最后一步，同步 model 的改动到数据库中。在项目目录下，使用终端执行以下命令。 1python manage.py migrate 如果没有报错的话，证明成功导入。有时候可能会报出以下的错误： 原因是表中定义了 id 字段，同时这个字段被设定为主键。具体的解决方式是：修改 model.py 中 id 字段的定义; 将其中的 null=True 修改为 primary_key=True。 12345# 修改前id = models.IntegerField(blank=True, null=True)# 修改后id = models.IntegerField(blank=True, primary_key=True) 保存修改之后，重新执行同步数据库命令即可。","link":"/443.html"},{"title":"Django 实战1：搭建属于自己社工查询系(上)","text":"前面的文章已经把模板、模型、视图、表单等知识点逐一讲解，大家已经熟悉它们具体用法。但如何将其串联起来还一筹莫展。本篇文章分享我之前做过的一个小项目，帮助大家抹开这一层迷雾。 想做什么？我分享的项目是一个社工库查询系统。大家不要一拿到源码，就直接去阅读。建议先思考下，如果让我来设计社工库查询系统，我要如何实现？ 我就先阐述我思考的内容。既然项目是一个查询系统，那么重点在于查询。众所周知，查询数据则是执行 SQL 语句来从数据库中获取数据。SQL 查询语句一般使用表中的字段作为查询条件。哪些能作为查询条件呢？先让我们来看看数据库表是怎么定义的？ 数据库表数据库中只有一个表，名为 Socialusers。它的创建语句是： 12345678910111213141516Create table if not exists `SocialUsers` ( `id` INTEGER PRIMARY KEY, `username` varchar(64), `password` varchar(64), `chineseName` varchar(64) , `email` varchar(64) , `QQ` varchar(15) , `weibo` varchar(300), `identity_number` varchar(25), `cell_phone` varchar(20), `ip_address` varchar(100), `college` varchar(60), `living_place` varchar(200), `source` varchar(50), `remarks` varchar(400) ); 表中一共有 13 个字段，其中有 9 个字段的内容属于敏感字段，涉及到个人账号安全问题。因此，我将这 9 个字段，分别是 username、password、chineseName、email、QQ、identity_number、cell_phone、college、source 作为查询条件。 那么系统最终的效果是根据查询条件和用户输入的内容来查询数据。如果数据库匹配到数据，就通过表格形式打印数据。反之则提醒用户查询不到数据（查询不到数据应该值得庆幸的，说明你的账号数据没有被泄露）。 我将查询条件以下拉框的形式显示，让用户可以自行选择查询条件。查询内容以文章输入框展示，提供用户输入数据的接口。到这里可以回答之前的想要做什么的问题了。我们最终要实现的页面效果如下： 模型建立数据库已经有了表，我直接使用 Django 自带工具生成 Socialusers 模型。这一步就省略不讲，如果你对这步操作不是很熟悉，可以阅读上篇文章的内容。 最终生成的模型如下： 123456789101112131415161718192021class Socialusers(models.Model): id = models.IntegerField(blank=True, primary_key=True) username = models.CharField(max_length=64, blank=True, null=True) password = models.CharField(max_length=64, blank=True, null=True) # Field name made lowercase. chinesename = models.CharField(db_column='chineseName', max_length=64, blank=True, null=True) email = models.CharField(max_length=64, blank=True, null=True) # Field name made lowercase. qq = models.CharField(db_column='QQ', max_length=15, blank=True, null=True) weibo = models.CharField(max_length=300, blank=True, null=True) identity_number = models.CharField(max_length=25, blank=True, null=True) cell_phone = models.CharField(max_length=20, blank=True, null=True) ip_address = models.CharField(max_length=100, blank=True, null=True) college = models.CharField(max_length=60, blank=True, null=True) living_place = models.CharField(max_length=200, blank=True, null=True) source = models.CharField(max_length=50, blank=True, null=True) remarks = models.CharField(max_length=400, blank=True, null=True) class Meta: managed = True db_table = 'SocialUsers' 注意下 chinesename 和 qq 这两个属性。我们在数据库中的定义是 chineseName 和 QQ。而 Django 在生成的模型时，自动将大写转为化小写。因此，当我们在使用 Socialusers 这两个属性时，要注意它们的定义已经改变，属性的定义全是小写的。 表单实现表单的实现由两种方式:一种是根据 model 生成 Form，另一种是自定义 Form。我分别把这两种方式实现。你可以对比下这两种写法的差异，以及如何给表单指定 bootstrap CSS 样式的。 根据 model 生成 Form在 app 目录下新建文件夹 forms 以及文件 forms.py 。forms.py 主要存放表单的定义，实现代码如下： 1234567891011121314151617181920212223242526272829303132# forms.py CONDITION_CHOICES = ( ('username', '用户名'), ('password', '密码'), ('chineseName', '姓名'), ('email', '邮箱'), ('QQ', 'QQ'), ('identity_number', '身份证'), ('cell_phone', '电话'), ('college', '大学'), ('source', '来源'),)class QueryUserForm(forms.Form): condition = forms.CharField( # 也可以定义为 ChoiceField max_length=20, widget=forms.Select(choices=CONDITION_CHOICES, attrs={'class':\"form-control\", 'title':\"query condition\", 'name':'condition', }), localize=('username', '用户名') ) queryContent = forms.CharField( max_length=100, widget=forms.TextInput(attrs={'class': 'form-control is-invalid', 'name': 'queryContent', 'placeholder': '请输入需要要查询的内容...' }), error_messages={'required': '查询内容不能为空 !'} ) 其中， widget 是指定字段呈现样式，如 condition 被指定呈现下拉框 Select；localize 是设置初始化值；error_messages 是错误提示形式。attrs 是为呈现的组件指定一些属性，如 CSS 样式、name 等。这部分内容，后面的文章会做详细讲解。 自定义 Form为了更好区分模型，我在 models.py 中新建一个模型来代替之前的 Socialusers 模型。 1234567891011121314151617181920# models.py # 用于表单查询的 modelCONDITION_CHOICES = ( ('username', '用户名'), ('password', '密码'), ('chineseName', '姓名'), ('email', '邮箱'), ('QQ', 'QQ'), ('identity_number', '身份证'), ('cell_phone', '电话'), ('college', '大学'), ('source', '来源'),)class QueryUser(models.Model): condition = models.CharField(max_length=20, choices=CONDITION_CHOICES) queryContent = models.CharField(max_length=100) def __str__(self): # __unicode__ on Python 2 return self.condition Form 的实现代码如下： 12345678910111213141516171819202122232425262728# forms.py class QueryUserForm(ModelForm): class Meta: model = QueryUser fields = ['condition', 'queryContent', ] # 指定呈现样式字段、指定 CSS 样式 widgets = { 'condition': Select(attrs={'class':\"form-control\", 'title':\"query condition\", 'name':'condition', }), 'queryContent':TextInput(attrs={'class': 'form-control is-invalid', 'name': 'queryContent', 'placeholder': '请输入需要要查询的内容...' }) } localized = { 'condition':('username', '用户名'), 'queryContent':123 } # 自定义错误信息 error_messages = { 'queryContent':{ 'required': '查询内容不能为空 !', } } 各个字段的含义跟第一种实现方式类似，我就不重复说明。 模板创建我创建名为 templates 文件夹来存放 HTML 文件。其中 index.html 是主页面，也是我们查询数据的页面。因为我前端框架使用的是 bootstrap，所以需要加载一些库。我为了满足在电脑无网络的状态也能使用的需求。我将其 bootstrap 所用到的库到打包到 static 目录下。 代码比较多，我只把重点内容贴出来。详细代码可以通过点击【阅读原文】查看完整代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657{% load staticfiles %}&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt; &lt;div class=\"container\" id=\"container\"&gt; &lt;div id=\"page-header\"&gt; &lt;h1 class=\"text-center\"&gt; 社工库查询系统 &lt;/h1&gt; &lt;/div&gt; &lt;div class=\"row\"&gt; &lt;form action=\"\" method=\"GET\" class=\"form-horizontal\" role=\"form\"&gt; &lt;div id=\"checkbox\" class=\"text-center\"&gt; &lt;label class=\"checkbox-inline text-success\"&gt;默认采用完整匹配&lt;/label&gt; &lt;/div&gt; &lt;div class=\"col-md-10 col-md-offset-1\"&gt; &lt;div class=\"col-md-2 col-md-offset-0\"&gt; {{ form.condition }} {% comment %} {{ form.condition }} 在 html 中将被渲染成以下代码 &lt;select name='condition' title=\"query condition\" class=\"form-control\"&gt; &lt;option &gt;用户名&lt;/option&gt; &lt;option&gt;密码&lt;/option&gt; &lt;option&gt;姓名&lt;/option&gt; &lt;option&gt;邮箱&lt;/option&gt; &lt;option&gt;QQ&lt;/option&gt; &lt;option&gt;身份证&lt;/option&gt; &lt;option&gt;电话&lt;/option&gt; &lt;option&gt;大学&lt;/option&gt; &lt;option&gt;来源&lt;/option&gt; &lt;/select&gt; {% endcomment %} &lt;/div&gt; &lt;div class=\"input-group col-md-10 col-md-offset-1\"&gt; {{ form.queryContent.field.errors }} {{ form.queryContent }} {% comment %} {{ form.queryContent }} 在 html 中将被渲染成以下代码 &lt;input type=\"text\" class=\"form-control is-invalid\" name=\"q\" placeholder=\"请输入内容...\" value=\"\"&gt; {% endcomment %} &lt;div class=\"input-group-btn\"&gt; &lt;button type=\"submit\" class=\"btn btn-primary\" required &gt;Search&lt;/button&gt; &lt;div class=\"invalid-feedback\"&gt; Please provide a valid value. &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;br&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 在 html 代码中，我直接将 form 标签直接写出来，里面的 Select 和 Input 标签通过 Django Form 来填充。下拉框使用表单的 condition 属性，即 来填充 ，输入框也是使用 来填充。当它们渲染的时候，会自动被解析为 Select 和 input 控件。 视图我设定是用户提交表单地址不改变。所以表单数据会被提交到原来的页面。因此，在视图的工作是拦截表单，并截取里面的内容。最后将查询结果返回给 HTML 页面。 1234567891011121314151617181920# views.pydef index(request): templateView = 'index.html' if request.method == 'GET': form = QueryUserForm(request.GET) # 验证表单 if form.is_valid(): # 过滤需要的数据, 获取查询条件和查询内容 condition = form.cleaned_data['condition'] keywords = form.cleaned_data['queryContent'] print('condition == ' + condition) print('keywords == ' + keywords) # 查询数据... return render(request, templateView, {'form': form}) # 直接访问主页, 显示的内容 else: return render(request, templateView, {'form': form}) 系统的基本框架已经搭建差不多。因为文章篇幅关系，一部分内容下篇文章讲解。下篇文章主要是如何查询数据、如何根据查询结果显示不同内容、如何将数据呈现出来。 如果你想获取网站的源码, 点击☞☞☞Github 仓库地址","link":"/444.html"},{"title":"Django 实战1：搭建属于自己社工查询系(下)","text":"上篇文章已经完成框架搭建，本文接着上篇的内容继续讲解。本片主要的说三点内容，分别是：根据条件查询数据、根据查询结果显示不同内容、将查询数据填充到页面上。 逻辑优化在上篇文章，我在原来的 url 地址中处理用户提交的表单数据。Url 值改变了，但是页面没有刷新。同时，表单数据没有进行分类。这会导致用户不管提交什么数据，页面就呈现什么数据。 而正常的逻辑应该是这样。如果用户访问的是首页，那么不需要填充任何数据以及展示提示框。 如果用户提交了表单数据，但是数据库中没有查询到数据，则提醒下用户没有查询到相关数据。 如果用户提交了表单数据，数据库也能查询到数据。页面需要提醒用户查询到数据，并将查询结果展示出来。 为了解决这个问题，我通过定义一个变量 countNum 来区分。 根据查询结果显示不同内容视图改造按照解决方案，我们需要对 V 层进行改造。定义全局变量 countNum，初始化为 -1。如果用户访问的是首页，就直接返回。如果用户提交表单数据，数据库查询不到数据。则将 countNum 赋值为 0 ，然后返回。如果用户提交表单数据，数据库查询不到数据，就返回数据总条。 1234567891011121314151617181920212223242526272829303132333435363738# views.pydef index(request): templateView = 'index.html' countNum = -1 keywords = '' if request.method == 'GET': form = QueryUserForm(request.GET) # 验证表单 if form.is_valid(): # 过滤需要的数据 condition = form.cleaned_data['condition'] keywords = form.cleaned_data['queryContent'] print('condition == ' + condition) print('keywords == ' + keywords) countNum = 0 # 查询结果 # 假设经过查询, 一共获取到 3 条数据 countNum = 3 if countNum != 0: return render(request, templateView, { 'countNum': countNum, 'keywords': keywords, 'form': form, }) # 查询不到数据, 显示没有数据的浮窗 if countNum == 0: return render(request, templateView, { 'countNum': countNum, 'keywords': keywords, 'form': form, }) # 直接访问主页, 显示的内容 else: return render(request, templateView, {'countNum': countNum, 'form': form}) 模板改造T 层（模板）需要根据 V 层（视图）透传过来的 countNum 的值进行判断，然后渲染相应的内容。 123456789101112131415161718192021222324252627282930313233343536373839404142# index.html&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt; &lt;div class=\"container\" id=\"container\"&gt; &lt;!-- 前面代码不变 --&gt; {% if countNum == -1 %} &lt;!-- 显示首页内容, 不需要做处理 --&gt; {% else %} &lt;!-- 查询不到数据, 提醒用户更换 关键词或者类型 --&gt; {% if countNum == 0 %} &lt;div class=\"alert alert-warning alert-dismissible col-md-10 col-md-offset-1\" role=\"alert\"&gt; &lt;button type=\"button\" class=\"close\" data-dismiss=\"alert\"&gt; &lt;span aria-hidden=\"true\"&gt;&amp;times;&lt;/span&gt; &lt;span class=\"sr-only\"&gt;Close&lt;/span&gt;&lt;/button&gt; 找不到与&lt;b&gt;&amp;nbsp{{ keywords }}&amp;nbsp&lt;/b&gt;相关的结果。请更换其他&lt;b&gt;&amp;nbsp关键词或类型&amp;nbsp&lt;/b&gt;试试。&lt;/div&gt;&lt;/div&gt; {% else %} &lt;!-- 显示总数以及查询耗时 打印表格、说明头部行、查询到的数据--&gt; &lt;div class=\"row\"&gt; &lt;div class=\"alert alert-success alert-dismissible col-md-10 col-md-offset-1\" role=\"alert\"&gt; &lt;button type=\"button\" class=\"close\" data-dismiss=\"alert\"&gt; &lt;span aria-hidden=\"true\"&gt;&amp;times;&lt;/span&gt; &lt;span class=\"sr-only\"&gt;Close&lt;/span&gt;&lt;/button&gt; 找到与&lt;b&gt;&amp;nbsp {{ keywords }} &amp;nbsp&lt;/b&gt;相关的结果 {{ countNum }} 个。用时 {{ time }} 秒。&lt;/div&gt; &lt;div class=\"table-responsive col-md-12\"&gt; &lt;table class=\"table table-striped table-hover\"&gt; &lt;tr&gt; &lt;th class=\"text-center\"&gt;用户名&lt;/th&gt; &lt;th class=\"text-center\"&gt;密码&lt;/th&gt; &lt;th class=\"text-center\"&gt;姓名&lt;/th&gt; &lt;th class=\"text-center\"&gt;邮箱&lt;/th&gt; &lt;th class=\"text-center\"&gt;QQ 号码&lt;/th&gt; &lt;th class=\"text-center\"&gt;数据来源&lt;/th&gt; &lt;/tr&gt; &lt;!-- 填充查询到的数据 --&gt; &lt;/table&gt;&lt;/div&gt;&lt;/div&gt; {% endif %} {% endif %} &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 根据条件查询数据模型层主要跟数据库打交道， 数据库存储数据的地方。所以查询数据内容其实是模型知识地运用。 考虑到数据库中可能存在多条关键字（queryContent）相关的数据，所以需要使用 filter() 来匹配。我使用的匹配模式是精确匹配，所以无须使用正则表达式来匹配。直接把关键字作为模型过滤条件就可以了，实现代码如下： 12345# 把 Socialusers 的 username 属性作为 condition 来查询数据。# 查询内容是用户输入的内容 queryContentuser_list = Socialusers.objects.filter(username=keywords)# 获取总条数countNum = user_list.count() condition 记录用户选择下拉框的条目值。该变量的值决定模型 Socialusers 要使用哪个属性来查询。由于 python 没有 switch 语句，只能写多个 elif 来处理多个条件。那么代码可以这么实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# views.pydef index(request): templateView = 'index.html' countNum = -1 keywords = '' time = 0 if request.method == 'GET': form = QueryUserForm(request.GET) # 验证表单 if form.is_valid(): # 过滤需要的数据 condition = form.cleaned_data['condition'] keywords = form.cleaned_data['queryContent'] print('condition == ' + condition) print('keywords == ' + keywords) countNum = 0 # 查询结果 if condition == 'username': user_list = Socialusers.objects.filter(username=keywords) countNum = user_list.count() # 获取查询耗时 time = (connection.queries)[0].get('time') print('user_list size=== ', user_list.count()) print('time === ', time) # 显示分页操作, 每页显示 20 条 paginator = Paginator(user_list, 20) page = request.GET.get('page') try: users = paginator.page(page) except PageNotAnInteger: # 如果请求的页数不是整数，返回第一页。 users = paginator.page(1) except EmptyPage: # 如果请求的页数不在合法的页数范围内，返回结果的最后一页。 users = paginator.page(paginator.num_pages) return render(request, templateView, { 'countNum': countNum, 'condition': condition, 'keywords': keywords, 'form': form, 'users': users, 'time': time, }) elif condition == 'password': # 后面的代码逻辑跟前面类似, 只不过 filer() 的内容改变了。 # 查询不到数据, 显示没有数据的浮窗 if countNum == 0: return render(request, templateView, { 'countNum': countNum, 'keywords': keywords, 'form': form, }) # 直接访问主页, 显示的内容 else: return render(request, templateView, {'countNum': countNum, 'form': form}) 代码中使用到了分页 Paginator，这部分后面会继续讲解。 填充数据最后一步工作就是在模板中填充查询到的数据。因为我们在视图中将查询到 user_list 集合传递给模板。这里要注意的是，user_list 其实是一个查询集 QuerySet，不是真正意义上的列表，只不过命名为列表而已。在模板中，使用一个 for 循环逐一解析每个模型的值，就能完成数据填充工作。 12345678910111213# index.html&lt;!-- 填充查询到的数据 --&gt; # 在下面的注释下面，打印查询到的数据{% for user in users %}&lt;tr&gt;&lt;td class=\"text-center\"&gt;{{ user.username }}&lt;/td&gt;&lt;td class=\"text-center\"&gt;{{ user.password }}&lt;/td&gt;&lt;td class=\"text-center\"&gt;{{ user.chinesename }}&lt;/td&gt;&lt;td class=\"text-center\"&gt;{{ user.email }}&lt;/td&gt;&lt;td class=\"text-center\"&gt;{{ user.qq }}&lt;/td&gt;&lt;td class=\"text-center\"&gt;{{ user.source }}&lt;/td&gt;&lt;/tr&gt;{% endfor %} 第一个实战项目到这里就结束了。主要目的是让大家明白如何将模型、表单、模板、视图串联起来。后面的文章会讲解高级用法以及前面遗漏的细节内容。 如果你想获取网站的源码, 点击☞☞☞Github 仓库地址","link":"/445.html"},{"title":"该如何学习 Python？","text":"在我的 QQ 学习交流群中，有位读者问我一个很有代表性的问题，在这里和大家分享下。 猴哥，能说下自学的学习经验吗？我都自学 Python 一个月了。 在我看来，这个问题的本质就是找到属于自己的学习方法。我把自己的经验分享给大家，希望对你们有帮助。在这之前，我要先说下学习效率的问题。因为我觉得掌握知识 = 学习方法 + 学习效率。 美国著名学习专家爱德家·戴尔曾提出一种学习方式的理论。它其实是一张图，名为学习金字塔效率图。 在国内，我们最经常接触的三种学习方式分别是听讲，阅读，实践。听讲指的是是上学时听老师讲课或者观看别人的教学视频。由图可知，这是一种学习方式效率最低的。随着时间地流逝，遗忘的内容最多。阅读书籍与听讲相比，则相对好点，但还是不高。而实践以及教授给他人，这两个种方式往往是被大家所推崇。 因此，我们学习新领域的知识时，可以先通过看别人的教学视频或者阅读相关书籍来入门。但一定要去实践，这能确保自己学到大部分知识。例如你正在看别人的 Python 入门课程，在课后自己要手动去敲代码。值得注意的是，不要照着课程的代码来敲打，那是没有效果。如果没有实践机会，也可以将知识讲给其他人。如果很不巧没有观众，那就自问自答 说完学习效率，现在来说下学习方法。观看别人的教学视频这种方式，我就不详细说了。只要你跟着讲师的节奏就可以。我就重点说下自己的阅读方式。因为我比较喜欢阅读纸质书籍，所以推荐都是书籍。如果你喜欢电子书，也可以阅读电子书或者技术博客。 学习新的东西，我首先统计需要学习知识的范围。划出这个范围很简单，每本书籍都有目录，目录的内容就是大致学习范围。然后翻翻几本书籍的目录。如果几本书籍同时出现的内容，这些内容就是重点。接着根据学习内容以及自己时间安排，指定学习计划。最后把学习内容分割为小内容到每天当中，每天坚持学习。 最后，我推荐一些个人觉得不错的 Python 书籍。目前这些书籍都是最新版本的，所以你不用担心过时问题。 { 编程入门 }《笨办法学 Python》《Python 基础教程（第3版·修订版）》《Head First Python (第2版)》《Python 编程：入门到实践》 { 开发进阶 }《Python 项目开发实战（第2版）》《精通 Python 设计模式》《Python 核心编程第3版》《Python 源码剖析》《Python 学习手册（第4版）》《Python Web 开发实战》《Python 3网络爬虫开发实战》","link":"/446.html"},{"title":"Python 编码规范","text":"软件行业现在基本上是协同工作，不再是以前个人单打独斗的年代了。在一个团队中，每个人可能负责一个或者多个模块。如果团队没有统一的编程规范，会增加代码理解难度，从而增加维护成本。所以遵循良好的编码风格，可以有效的提高代码的可读性，降低出错几率和维护难度。另外，使用（尽量）统一的编码风格，还可以降低沟通成本。 网上有许多的编码规范，我介绍分享几个知名编码规范给大家参考学习。 *PEP 8 *PEP 8 可以算是 Python 官方的标准编码规范。它是用于规范 Python 主发行版中的标准库的代码。所以这个编码规范是值得一看。 传送门：文档地址 Google 的 Python 风格指南总所周知，Google 是开源大户。Google 会将项目托管到 Github 上面，任何人都可以 fork、 修改、提交。如果代码贡献者的编程风格与 Google 的不一致, 会给代码阅读者和其他代码提交者造成不小的困扰。Google 因此发布了这份自己的编程风格指南, 使所有提交代码的人都能获知 Google 的编程风格。Google 的 Python 风格也是遵循 PEP8 规范。 传送门：文档地址 Pocoo 风格指南估计大家对 Pocoo 比较陌生，但大家一定对小型 Web 开发框架 Flask 很熟悉。没错，Flask 是 Pocoo 团队开发的项目。 除了 Flask 之外，Pocoo 团队还有开发出很多广受欢迎的项目，例如 Jinja2（模板引擎）、Pygments（语法高亮包）、Sphinx（文档处理器）、Werzeug（WSGI工具集）。Poco o团队编码风格指南适用于所有 Pocoo 团队的项目。总体来说，Pocoo 团队编码风格指南严格遵循了 PEP8 的要求，但略有一些不同之处，并进行了一定的扩展延伸。 传送门：文档地址 PyCharm 目前开发 Python 的主流 IDE 工具，我介绍下如何在 PyCharm 配置 PEP 8 代码提示、将代码格式化符合 PEP 8 规范。 配置 PEP 8 代码提示一般安装 PyCharm 都默认配置了规范提示。直接在右下角调整 Highlighting Level 为 Inspections 就能自动 PEP 8 提示。 在我之前 Django 学习笔记系列的第一个 demo 中，有不符合规范的地方。在代码编辑框的右边会有一个浅黄色的标记，你将鼠标悬停在光标上，PyCharm 会发现有提示。 对于这种提示，只要在第 7 行增加一个回车就搞定了，之后PyCharm 也没有提示。 当然，你也可以修改提示框的配色。 将代码格式化符合 PEP 8 规范这里我们需要使用到一个第三方库 Autopep8。Autopep8 是一个将 Python 代码自动排版为 PEP 8 风格的小工具。它使用 PEP 8 工具来决定代码中的哪部分需要被排版。Autopep8 可以修复大部分PEP 8 工具中报告的排版问题。 打开终端，使用 pip 命令来安装 Autopep8： 1pip install autopep8 autopep8 是一个命令行工具，所以我们在终端下对某个文件进行格式化。命令行如下： 1autopep8 --in-place --aggressive --aggressive &lt;filename&gt; Pycharm 配置 Autopep8 方法如下：1）选择菜单「File」–&gt;「Settings」–&gt;「Tools」–&gt;「External Tools」–&gt;点击加号添加工具 2）填写如下配置项，点击「OK」保存 图片中需要配置信息Name：Autopep8 (可随意填写)Tools settings: Programs：autopep8 Parameters：--in-place --aggressive --ignore=E123,E133,E50 $FilePath$ Working directory：$ProjectFileDir$ 3） 选择菜单「Tool」–&gt;「Extern Tools」–&gt;「Autopep8」或在某个文件中右键选择「Extern Tools」–&gt;「Autopep8」，即可使用autopep8自动格式化你的python代码了。 或","link":"/447.html"},{"title":"想做 Python Web 开发，需要掌握哪些技能？","text":"在 Web 开发领域，Java 凭借企业级支持以及世界丰富的生态环境成为绝对霸主，PHP 紧随其后。有些公司考虑效率问题而采用 C++ 做后台开发语言， 也有人使用 Node.js 开发后台。 另外 Python 也是能够做后台开发的。Python 具有语言简洁、开发效率高等特点。还有成熟且不断更新的开源框架，例如 Flask、Django、Tornado等。所以很多大公司都使用其开发后台。比如人人皆知的 YouTube、Instagram、Reddit、Quora、知乎、豆瓣、果壳等。 因此，市场有 Python Web 开发的岗位需求。那么我想往这方面发展，我需要掌握哪些技能？ Python 语言想使用 Python 做后台开发，第一步就是就是学习 Python 语言。首先要将学习 Python 基本语法，再学习 Python 高级用法（例如闭包，面向对象等），接着再了解各个标准库的用法，最后熟悉下 PEP8 编码规范。 开发框架大部分后台业务逻辑都会使用 Web 框架来开发，目的是提高开发效率。常用的 Python web框架有 Django、Flask、Tornado 等。个人推荐熟练掌握 Django，因为 Django 是一个全能型框架。另外需要了解 REST，学习如何编写 RESTful APIs。 数据库现在网站业务后端用得比较多的有三种类型的数据库，关系型数据库（mysql等），文档型数据库（mongodb等），和内存型数据库（redis等）。因为三种数据库各有优势和其使用场景，所以需要了解下不同类型数据库的使用方法和应用场景，灵活应用到后端代码中。所以要学习如何使用、设计、优化数据库。 前端知识需要了解基本的 HTML、CSS、JavaScript。通常前后端开发是分离的，了解前端知识是有帮助的。能知道自己需要将传递哪些内容给前端，从而提供团队合作效率。如果对前端知识感兴趣，可以了解下 Bootstrap、Vue 等。 Web 服务器Nginx 目前很流行，使用也是很广泛。因为其占用内存少，稳定性高、并发能力强。所以需要掌握 Web 应用部署以及如何使用 Nginx 实现负载均衡。 Linux 操作系统Nginx 通常运行在 Linux 服务器上，所以需要学习 Linux 系统。了解一些常见的 Linux 命令、文件与目录管理、账号与身份管理、程序与资源管理等。推荐阅读 《鸟哥的Linux私房菜基础学习篇》，这本书爽是最具知名度的 Linux 入门书全面。它能详细地介绍了 Linux 操作系统。 计算机网络后台开发经常要跟网络打交道，所以熟悉对网络协议 TCP/IP 和 HTTP。学习 TCP/IP 可以阅读《TCP/IP详解卷1：协议》，学习 HTTP 可以阅读《图解Http》和《Http权威指南》 算法与数据结构我记得读大学时使用的教程是严蔚敏的《数据结构》（C语言版）。其中有一句很经典的话：『程序 = 算法 + 数据结构』。所以需要了解常用的算法和数据结构。推荐阅读《算法图解》、《枕边算法书》 以上信息是自己抓取大量数据，然后分析总结出来的，希望对你有所帮助。","link":"/448.html"},{"title":"Django 学习笔记之模型高级用法(上)","text":"前面有两篇文章简单介绍 Django 的模型，这一部分算是基础知识。我自己近期也总做了下总结，将花大概两篇的篇幅来分享下模型的一些高级用法。 如果想熟悉 Django 的用法，我认为应该一开始要熟悉一些细节用法，后面再了解 Django 的实现原理。而细节用法往往体现在一些差别用法，难以理解的知识点上。 复杂的字段类型经过前面的学习，我们知道模型的字段类型一方面是指定数据库表的列名称和数据类型，另一方面决定 HTML 中的表单标签类型。 整数类型的区别Django 的整数类型有三个，分别是 IntegerField、BigIntegerField 和 SmallIntegerField。这三个字段区别在于取值范围。IntegerField 在 Django 所有支持的数据库中，合法取值范围是 -2147483648 到 2147483647。而 BigIntegerField 是一个 64 位整数，它允许的值范围是 -9223372036854775808 到 9223372036854775807。所以在数据库迁移的时候，特别数据库中有 Sqlite 时，要更加注意数字的取值范围。SmallIntegerField 取值范围是 -32768 到 32767。 自增类型的区别AutoFiled 和 BigAutoFiled 都是自增类型，它们都是由整数类型演化而来。AutoFiled 是一个根据实际 ID 自动增长的 IntegerField。通常不需要直接使用它,如果表中没有设置主键时，Django 将会自动添加一个自增主键。BigAutoField 其实也是一个 BigIntegerField，但它支持 ID 自动增长。所以它的取值范围不能为负数和零了。 时间类型DateField 和 DateTimeField 中的两个重要属性 auto_now 和 auto_now_add 默认值都是 Flase。 设置 auto_now 或者 auto_now_add 的值为 True，间接给该字段设置了 editable=False 和 blank=True 。给参数赋值需要传递一个 datetime.date 对象。如果时间是一串字符串，则转化为 date 对象。 DateField 支持输入值的形式如下： 123['%Y-%m-%d', # '2006-10-25' '%m/%d/%Y', # '10/25/2006' '%m/%d/%y'] # '10/25/06' DateTimeField 支持输入值的形式如下： 12345678['%b %d %Y', # 'Oct 25 2006' '%b %d, %Y', # 'Oct 25, 2006' '%d %b %Y', # '25 Oct 2006' '%d %b, %Y', # '25 Oct, 2006' '%B %d %Y', # 'October 25 2006' '%B %d, %Y', # 'October 25, 2006' '%d %B %Y', # '25 October 2006' '%d %B, %Y'] # '25 October, 2006' FilePathField该字段是用于保存文件路径信息的。默认最大长度为 100，当可通过 max_length 参数自定义。它包含几个重要的参数： path：必传参数。记录目录或者文件的绝对路径。例如：/home/monkeymatch：可选参数，它是一个正则表达式，主要用于匹配过滤出文件名。recursive：可选参数，表示是否包含子目录。默认值为 Flase。allow_files：可选参数，表示是否将文件名包括在内，默认值为 True。allow_folders：可选参数，表示是否将目录名包括在内默认值为 Flase。 Django 规定 allow_files 和 allow_folders 两者之间必须有一个值为 True。 FileField上传文件字段，常见于表单中。一般而言，文件都是保存在服务器的硬盘中。因此，该字段在数据库中其实是一个字符串类型，默认最大长度100，可以通过max_length参数自定义。 FileField 有两个重要的可选参数：upload_to 和 storage upload_toupload_to 是指定文件上传的目录。用法如下： 123456class MyModel(models.Model): # 文件上传到 MEDIA_ROOT/uploads upload = models.FileField(upload_to='uploads/') # 或者 # 文件上传到 MEDIA_ROOT/uploads/2015/01/30 upload = models.FileField(upload_to='uploads/%Y/%m/%d/') 其中 MEDIA_ROOT 是在 settings.py 中设置，表示上传文件的根目录。另外还需要设置 MEDIA_URL, 它表示上传文件对外能访问的 url 地址。 StorageStorage 是一个文件操作对象。它提供 size(path)、open(path).read()、delete(path)、exists(path)等方法来操作文件。 ImageField保存图像文件的字段。ImageField 用法跟 FileField 类似。除了需要在 seeting.py 中增加相关配置，还都拥有共同的 upload_to 字段选项。 它还有额外的可选参数：一个是 height_field，表示保存图片的高度。 另一个是 width_field，表示保存图片的宽度。 关系字段之前文章讲了三种关系字段的类型、定义、作用。今天讲下其中的一些字段选项。 ForeignKey1） on_delete在 Django 2.0 中，设置外键时需要添加一个 on_delete 选项。外键本身涉及到两个表的数据，况且外键在数据库中是有约束行为。所以 on_delete 参数是 Django 模拟 SQL 约束的行为。 on_delete 有几个可选值： CASCADE：这就是默认的选项，级联删除，你无需显性指定它。 PROTECT: 保护模式，如果采用该选项，删除的时候，会抛出 ProtectedError 错误。 SET_NULL: 置空模式，删除的时候，外键字段被设置为空，前提就是blank=True, null=True,定义该字段的时候，允许为空。 SET_DEFAULT: 置默认值，删除的时候，外键字段设置为默认值，所以定义外键的时候注意加上一个默认值。 SET(): 自定义对应的实体的值。 2）limit_choices_to该参数用于限制外键所能关联的对象，只能用于 Django 的 ModelForm（Django的表单模块）和 admin 后台，对其它场合无限制功能。该值接受是一个字典、返回一个字典的函数 3) db_constraint默认情况下，这个参数被设为 True，表示遵循数据库约束。如果设为 False，那么将无法保证数据的完整性和合法性。 4） related_name用于关联对象反向引用模型的名称。主要用于反向查询，即外键源模型实例通过管理器返回第一个模型的所有实例。 默认情况下，这个管理器的名字为 foo_set，其中 foo 是源模型名字的小写。例如： 123456# 在终端下使用 Django&gt;&gt;&gt;b = Book.objects.get(id=1)# 其中 entry_set 为默认的 related_name&gt;&gt;&gt;b.entry_set.all() &gt;&gt;&gt;b.entry_set.filter(headline__contains='天龙八部')&gt;&gt;&gt;b.entry_set.count() 如果我们设置 related_name=’novels’，那么上面的代码将变为： 123456# 在终端下使用 Django&gt;&gt;&gt;b = Book.objects.get(id=1)# 其中 entry_set 为默认的 related_name&gt;&gt;&gt;b.novels.all() &gt;&gt;&gt;b.novels.filter(headline__contains='天龙八部')&gt;&gt;&gt;b.novels.count() 5）related_query_name反向查询的关系查询集名称。用于从目标模型反向过滤模型对象的名称。具体用法如下： 1234567891011class Tag(models.Model): article = models.ForeignKey( Article, on_delete=models.CASCADE, related_name=\"tags\", related_query_name=\"tag\", ) name = models.CharField(max_length=255)# 现在可以使用 tag作为查询名Article.objects.filter(tag__name=\"important\") 字段选项字段选项是给每个 Field 指定一些属性。 db_column： 指定当前数据库表中该字段的列名。如果没有指定，Django 默认将 Field 名作为字段名。 db_index： 如果赋值为 True, 将会为这个字段创建数据库索引。 db_tablespace：如果该字段已经设置了索引，db_tablespace 用于指定字段索引的数据库表空间的名字。另外还需要看使用的数据库支不支持表空间。如果不支持，该参数设置没有效果。 editable：设置该字段是否能被编辑，默认是 True。如果设为 False , 这个字段将不会出现在 admin 或者其他 ModelForm 中。 同时也会跳过 模型验证 。 error_messages：用于自定义错误提示信息。参数接受的是字典类型的值。字典的 key 可以是 null, blank, invalid, invalid_choice, unique, 和 unique_for_dat 其中的一个。 help_text：用于前端页面上显示提示信息。要确保页面不存在 XXS 漏洞，需要使用django.utils.html.escape() 对内容进行转义。 unique_for_date：设置为 DateField 或者 DateTimeField 字段的名字，表示要求该字段对于相应的日期字段值是唯一的。例如，字段 title 设置了 unique_for_date=”pub_date” ，那么Django将不会允许在同一 pub_date 的两条记录的 title 相同。 unique_for_month：用法跟 unique_for_date 类似。 unique_for_year：用法跟 unique_for_date 类似。 verbose_name：为字段设置别名。对于每一个字段类型，除了 ForeignKey、ManyToManyField和 OneToOneField 这三个特殊的关系类型，其第一可选位置参数都是 verbose_name。如果用户没有定义该选项， Django会自动将自动创建，内容是该字段属性名中的下划线转换为空格的结果。 比如这个例子中描述名是 person's first name: 1first_name = models.CharField(\"person's first name\", max_length=30) 而没有主动设置时，则是 first name: 1first_name = models.CharField(max_length=30) 对于外键、多对多和一对一字字段，由于第一个参数需要用来指定关联的模型。因此必须用关键字参数 verbose_name 来明确指定。如下： 1234567891011poll = models.ForeignKey( Poll, on_delete=models.CASCADE, verbose_name=\"the related poll\",)sites = models.ManyToManyField(Site, verbose_name=\"list of sites\")place = models.OneToOneField( Place, on_delete=models.CASCADE, verbose_name=\"related place\",) 另外 verbose_name 不用大写首字母，在必要的时候 Django 会自动大写首字母。 validators：该字段将要运行的一个验证器的列表。例如 RegexValidator、EmailValidator。","link":"/449.html"},{"title":"Django 学习笔记之模型高级用法(下)","text":"接着上篇文章内容，本文分享自己对模型一些用法的总结。 模型的元数据Meta除了抽象模型，在模型中定义的字段都会成为表中的列。如果我们需要给模型指定其他一些信息，例如排序方式、数据库表名等，就需要用到 Meta。Meta 是一个可选的类，具体用法如下： 1234567class Author(models.Model): name = models.CharField(max_length=40) email = models.EmailField() class Meta: managed = True db_table = 'author' 不知你是否对上述代码有影响。通过 Django 将数据库表反向生成模型时，Django 会默认带上 managed 和 db_table 信息。 我主要说下 Meta 一些重要的属性，其他属性你可以通过文档信息进行学习。 abstract: 如果 abstract = True，模型会指定为抽象模型。它相当于面向对象编程中的抽象基类。 proxy：如果设置了proxy = True，表示使用代理模式的模型继承方式。 db_table：指定当前模型在数据库的表名。 managed：该属性默认值为 True，表示能创建模型和操作数据库表。 ordering：指定该模型生成的所有对象的排序方式。默认按升序排列，如果在字段名前加上字符 “-” 则表示按降序排列，如果使用字符问号 “？” 表示随机排列。 123ordering = ['pub_date'] # 表示按'pub_date'字段进行升序排列ordering = ['-pub_date'] # 表示按'pub_date'字段进行降序排列ordering = ['-pub_date', 'author'] # 表示先按'pub_date'字段进行降序排列，再按`author`字段进行升序排列。 verbose_name：给模型设置别名。如果不指定它，Django 会使用小写的模型名作为默认值。 12verbose_name = \"book\"verbose_name = \"图书\" verbose_name_plural：因为英语单词有单数和复数两种形式，这个属性是模型对象的复数名。中文则跟 verbose_name 值一致。如果不指定该选项，那么默认的复数名字是 verbose_name 加上 ‘s’ 。 12verbose_name_plural = \"books\"verbose_name_plural = \"图书\" indexes：为当前模型建立索引列表。用法如下： 1234567891011from django.db import modelsclass Customer(models.Model): first_name = models.CharField(max_length=100) last_name = models.CharField(max_length=100) class Meta: indexes = [ models.Index(fields=['last_name', 'first_name']), models.Index(fields=['first_name'], name='first_name_idx'), ] 模型的继承根据模型的 Meta 信息设置，模型继承方式可以分为三种：1）抽象模型模型的 Meta 类中含有 abstract = True 属性。抽象模型一般被当作基类，它持有子类共有的字段。值得注意的是，抽象模型在数据库中不会生成表。 12345678910111213from django.db import models# 抽象模型class Person(models.Model): name = models.CharField(max_length=500) age = models.PositiveIntegerField() class Meta: abstract = True# 子模型class Student(Person): school_name = models.CharField(max_length=20) 子模型如果没有定义 Meta 类，那么会继承抽象模型的 Meta 类。但是 abstract 属性不会被继承。 2）多表继承这种方式继承方式，子模型的父模型可以一个或者多个。 当父类模型是正常的模型，即不是抽象模型，在数据库中有对应表。 虽然在 Model 层不推荐使用多重继承，但 Django 的 ORM 还是支持这样的使用方式。如果使用多表继承，子模型跟每个父模型都会添加一个一对一的关系。 12345678910111213from django.db import models# 父模型 oneclass Model_One(models.Model): attr1 = models.CharField(max_length=10)# 父模型 twoclass Model_Two(models.Model): attr2 = models.CharField(max_length=10)# 子模型class Multiple(Model_One, Model_Two): attr3 = models.CharField(max_length=10) 多重继承的时候，子类的 ORM 映射会选择第一个父类作为主键管理，其他的父类作为一般的外键管理。 3）代理模型使用多表继承时，父类的每个子类都会创建一张新数据表。但是我们只是想扩展一些方法，而不想改变模型的数据存储结构。我们可以将在 Meta 类中增加约束proxy=True 来实现。此时子模型称为父模型的代理类，子类中只能增加方法，而不能增加属性。 1234567891011121314151617from django.db import modelsfrom django.contrib.auth.models import Userclass Person(User): name = models.CharField(max_length=10) class Meta: proxy = True def do_something(self): passclass Man(Person): job = models.CharField(max_length=20)class Woman(Person): makeup = models.CharField(max_length=20)","link":"/450.html"},{"title":"Django 实现分页功能","text":"当页面因需要展示的数据条目过多，导致无法在一个页面全部显示。这时，页面经常会采用分页形式进行展示，然后每页显示 20 或者 50 等条数据。分页经常在网站上随处可见， 它大概是这样子：这样的实现不仅提高了用户体验，还是减轻数据库读取数据的压力。Django 自带名为 Paginator 的分页工具， 方便我们实现分页功能。本文就讲解如何使用 Paginator 实现分页功能。 PaginatorPaginator 类的作用是将我们需要分页的数据分割成若干份。当我们实现化一个 Paginator 类的实例时，需要给 Paginator 传入两个参数。第一个参数是数据源，可以是一个列表、元组、以及查询结果集 QuerySet。第二个参数需要传入一个整数，表示每页显示数据条数。具体写法如下： 12345book_list = []for x in range(1, 26): # 一共 25 本书 book_list.append('Book ' + str(x))# 将数据按照规定每页显示 10 条, 进行分割paginator = Paginator(book_list, 10) 上面代码中，我们传入一个名为 book_list 的列表，该列表中含有 25 本书，然后我们给 Paginator 设定每页显示 10 条数据，最后得到一个 Paginator 实例。 另外 Paginator 类中有三个常用的属性，它们分别是： count：表示所有页面的对象总数。 num_pages： 表示页面总数。 page_range： 下标从 1 开始的页数范围迭代器。 Page 对象Paginator 类提供一个** page(number) **函数，该函数返回就是一个 Page 对象。参数 number 表示第几个分页。如果 number = 1，那么 page() 返回的对象是第一分页的 Page 对象。在前端页面中显示数据，我们主要的操作都是基于 Page 对象。具体用法如下： 12# 使用 paginator 对象返回第 1 页的 page 对象books = paginator.page(1) Page 对象有三个常用的属性： object_list: 表示当前页面上所有对象的列表。 numberv: 表示当前页的序号，从 1 开始计数。 paginator： 当前 Page 对象所属的 Paginator 对象。 除此之外，Page 对象还拥有几个常用的函数： has_next()： 判断是否还有下一页，有的话返回True。 has_previous()：判断是否还有上一页，有的话返回 True。 has_other_pages()：判断是否上一页或下一页，有的话返回True。 next_page_number()： 返回下一页的页码。如果下一页不存在，抛出InvalidPage 异常。 previous_page_number()：返回上一页的页码。如果上一页不存在，抛出InvalidPage 异常。 运用下面是自己编写的 demo 程序，介绍 Paginator 和 Page 如何一起使用。 视图在 views.py 获取需要展示的全部数据，然后使用 Paginator 类对数据进行分页，最后返回第 1 页面的 page 对象。page 对象的作用巨大，一方面展示当前分页数据，还提供获取后续页面数据的接口。 123456789101112131415161718192021222324252627282930313233from django.core.paginator import Paginator, PageNotAnInteger, EmptyPage, InvalidPagefrom django.http import HttpResponsefrom django.shortcuts import renderdef paginator_view(request): book_list = [] ''' 数据通常是从 models 中获取。这里为了方便，直接使用生成器来获取数据。 ''' for x in range(1, 26): # 一共 25 本书 book_list.append('Book ' + str(x)) # 将数据按照规定每页显示 10 条, 进行分割 paginator = Paginator(book_list, 10) if request.method == \"GET\": # 获取 url 后面的 page 参数的值, 首页不显示 page 参数, 默认值是 1 page = request.GET.get('page') try: books = paginator.page(page) # todo: 注意捕获异常 except PageNotAnInteger: # 如果请求的页数不是整数, 返回第一页。 books = paginator.page(1) except InvalidPage: # 如果请求的页数不存在, 重定向页面 return HttpResponse('找不到页面的内容') except EmptyPage: # 如果请求的页数不在合法的页数范围内，返回结果的最后一页。 books = paginator.page(paginator.num_pages) template_view = 'page.html' return render(request, template_view, {'books': books}) 模板模板的工作就是在 HTML 页面中填充数据。当拿到视图传递过来的 books（books 是一个 Page 对象）， 就在 for 循环中打印数据。最后使用 books 根据页面情况展示上一页按钮，当前页数，总页数，下一页按钮。 12345678910111213141516171819202122232425262728293031323334353637{% load staticfiles %}&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns=\"http://www.w3.org/1999/html\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;link href=\"{% static 'css/bootstrap.min.css' %}\" rel=\"stylesheet\"&gt; &lt;script src=\"{% static 'js/jquery.min.js' %}\"&gt;&lt;/script&gt; &lt;script src=\"{% static 'js/bootstrap.min.js' %}\"&gt;&lt;/script&gt; &lt;title&gt;分页&lt;/title&gt;&lt;/head&gt;&lt;br&gt; &lt;div class=\"text-center\" &gt; {% for book in books %} &lt;span&gt;书名： {{ book }} &lt;br /&gt;&lt;/span&gt; {% endfor %} &lt;/div&gt; {# 实现分页标签的代码 #} {# 这里使用 bootstrap 渲染页面 #} &lt;div id=\"pages\" class=\"text-center\" &gt; &lt;nav&gt; &lt;ul class=\"pagination\"&gt; &lt;li class=\"step-links\"&gt; {% if books.has_previous %} &lt;a class='active' href=\"?page={{ books.previous_page_number }}\"&gt;上一页&lt;/a&gt; {% endif %} &lt;span class=\"current\"&gt; Page {{ books.number }} of {{ books.paginator.num_pages }}&lt;/span&gt; {% if books.has_next %} &lt;a class='active' href=\"?page={{ books.next_page_number }}\"&gt;下一页&lt;/a&gt; {% endif %} &lt;/li&gt;&lt;/ul&gt;&lt;/nav&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 运行结果我在 urls.py 中配置当前的 app 访问路径是 paginator/。所以使用浏览器访问地址 http://127.0.0.1:8000/paginator/， 看到访问结果如下：","link":"/551.html"}],"tags":[{"name":"HTTP","slug":"HTTP","link":"/tags/HTTP/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"思念","slug":"思念","link":"/tags/%E6%80%9D%E5%BF%B5/"},{"name":"鸡汤","slug":"鸡汤","link":"/tags/%E9%B8%A1%E6%B1%A4/"},{"name":"网络爬虫","slug":"网络爬虫","link":"/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"name":"正则表达式","slug":"正则表达式","link":"/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"beautifulSoup","slug":"beautifulSoup","link":"/tags/beautifulSoup/"},{"name":"爬虫实战","slug":"爬虫实战","link":"/tags/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98/"},{"name":"当当","slug":"当当","link":"/tags/%E5%BD%93%E5%BD%93/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"多进程","slug":"多进程","link":"/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"name":"Requests","slug":"Requests","link":"/tags/Requests/"},{"name":"阅读","slug":"阅读","link":"/tags/%E9%98%85%E8%AF%BB/"},{"name":"方法论","slug":"方法论","link":"/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"},{"name":"认知","slug":"认知","link":"/tags/%E8%AE%A4%E7%9F%A5/"},{"name":"Xpath","slug":"Xpath","link":"/tags/Xpath/"},{"name":"lxml","slug":"lxml","link":"/tags/lxml/"},{"name":"电影","slug":"电影","link":"/tags/%E7%94%B5%E5%BD%B1/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Scrapy","slug":"Scrapy","link":"/tags/Scrapy/"},{"name":"反爬虫","slug":"反爬虫","link":"/tags/%E5%8F%8D%E7%88%AC%E8%99%AB/"},{"name":"数据分析","slug":"数据分析","link":"/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"generator","slug":"generator","link":"/tags/generator/"},{"name":"Iterator","slug":"Iterator","link":"/tags/Iterator/"},{"name":"Iterable","slug":"Iterable","link":"/tags/Iterable/"},{"name":"定时任务","slug":"定时任务","link":"/tags/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"name":"sched","slug":"sched","link":"/tags/sched/"},{"name":"APScheduler","slug":"APScheduler","link":"/tags/APScheduler/"},{"name":"OCR","slug":"OCR","link":"/tags/OCR/"},{"name":"验证码","slug":"验证码","link":"/tags/%E9%AA%8C%E8%AF%81%E7%A0%81/"},{"name":"IP代理池","slug":"IP代理池","link":"/tags/IP%E4%BB%A3%E7%90%86%E6%B1%A0/"},{"name":"pustil","slug":"pustil","link":"/tags/pustil/"},{"name":"datetime","slug":"datetime","link":"/tags/datetime/"},{"name":"time","slug":"time","link":"/tags/time/"},{"name":"技巧","slug":"技巧","link":"/tags/%E6%8A%80%E5%B7%A7/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"Django","slug":"Django","link":"/tags/Django/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"社工库","slug":"社工库","link":"/tags/%E7%A4%BE%E5%B7%A5%E5%BA%93/"},{"name":"自学","slug":"自学","link":"/tags/%E8%87%AA%E5%AD%A6/"},{"name":"编码规范","slug":"编码规范","link":"/tags/%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83/"},{"name":"PEP","slug":"PEP","link":"/tags/PEP/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"后台","slug":"后台","link":"/tags/%E5%90%8E%E5%8F%B0/"}],"categories":[{"name":"计算机网络","slug":"计算机网络","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"Python 必知必会","slug":"Python-必知必会","link":"/categories/Python-%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"思维与认知","slug":"思维与认知","link":"/categories/%E6%80%9D%E7%BB%B4%E4%B8%8E%E8%AE%A4%E7%9F%A5/"},{"name":"网络爬虫","slug":"网络爬虫","link":"/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"name":"数据分析","slug":"数据分析","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"Django 学习笔记","slug":"Django-学习笔记","link":"/categories/Django-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]}