{"pages":[],"posts":[{"title":"深入理解HTTP","text":"HTTP是什么 HTTP全称是HyperText Transfer Protocal，即：超文本传输协议。它主要规定了客户端和服务器之间的通信格式。HTTP还是一个基于请求/响应模式的、无状态的协议；即我们通常所说的Request/Response。 HTTP与TCP的关系TCP协议是位于TCP/IP参考模型中的网络互连层，而HTTP协议属于应用层。因此，HTTP协议是基于TCP协议。 HTTP请求(HTTP Request)HTTP请求由三部分组成，分别是： 请求行 HTTP头 请求体 下面是请求示例： 123456789GET /?tn=90058352_hao_pg HTTP/1.1Host: www.hao123.comConnection: keep-aliveCache-Control: max-age=0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Upgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/5.7.16400.16 Safari/537.36Accept-Encoding: gzip, deflateAccept-Language: zh-CN,zh;q=0.8 请求行同样也是由请求方法（POST/GET）方式、请求的主机、协议版本号三部分组成。下面为请求行的示例：GET /?tn=90058352_hao_pg HTTP/1.1 HTTP头HTTP头又细分为请求头(request header)、普通头(general header)、实体头(entity header)而HTTP头主要关注点是其字段 Accept作用: 浏览器可以接受的媒体类型例如： Accept: text/html 代表浏览器可以接受服务器回发的类型为 text/html 也就是我们常说的html文档通配符 * 代表任意类型例如： Accept: */* 代表浏览器可以处理所有类型,(一般浏览器发给服务器都是发这个) Accept-Language作用： 浏览器申明自己接收的语言。语言跟字符集的区别：中文是语言，中文有多种字符集，比如big5，gb2312，gbk等等；例如： Accept-Language: zh-CN,zh Accept-Encoding作用： 浏览器申明自己接收的编码方法，通常指定压缩方法（gzip，deflate）例如：Accept-Encoding: gzip, Accept-Encoding: deflate User-Agent作用： 告诉HTTP服务器， 客户端使用的操作系统的名称和版本以及浏览器的名称和版本.例如： User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/5.7.16400.16 Safari/537.36 Content-Type作用： 告诉服务器，请求的内容的类型常见的字段： 假设使用POST方式请求 text/xml [请求体为文本] application/json [请求体为JSON数据] application/xml [请求体为xml数据] image/jpeg [请求体为jpeg图片] multipart/form-data [请求体为表单] Cookie作用： 最重要的header，将cookie的值发送给HTTP服务器 Connection例如： Connection: keep-alive 当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接例如： Connection: close 代表一个Request完成后，客户端和服务器之间用于传输HTTP数据的TCP连接会关闭， 当客户端再次发送Request，需要重新建立TCP连接。 Content-Length作用：发送给HTTP服务器数据的长度。例如： Content-Length: 18 Referer:作用： 提供了Request的上下文信息的服务器，告诉服务器我是从哪个链接过来的。 请求体这个只有post方式请求才有，get方式请求没有。 HTTP响应(HTTP Response)HTTP Response的结构跟Request的结构基本一样。同样分为三部分： 响应行 响应头 响应体 下面是响应示例： 1234567891011HTTP/1.1 200 OKCache-Control: max-age=0Content-Encoding: gzipContent-Length: 156474Content-Type: text/html;charset=UTF-8Cxy_all: 90058352_hao_pg+d4fa7f28cefb9b120f868558e440bafaDate: Sun, 20 Nov 2016 05:09:51 GMTExpires: Sun, 20 Nov 2016 05:09:51 GMTLfy: nj02.11Server: BWS/1.0Set-Cookie: __bsi=11619936655404239050_00_60_N_R_126_0303_c02f_Y; max-age=3600; domain=www.hao123.com; path=/ 响应行响应行由协议版本、响应状态构成下面为响应行的示例：HTTP/1.1 200 OK 响应头响应头关注点是字段，常见的字段如下： Cache-Control作用: 非常重要的规则。 这个用来指定Response-Request遵循的缓存机制。例如：Cache-Control:Public 可以被任何缓存所缓存Cache-Control:Private 内容只缓存到私有缓存中Cache-Control:no-cache 所有内容都不会被缓存 Content-Type作用：服务器告诉浏览器，自己响应的对象的类型和字符集例如:Content-Type: text/html; charset=utf-8Content-Type: image/jpeg Expires作用: 浏览器会在指定过期时间内使用本地缓存例如: Expires:Sun, 20 Nov 2016 05:09:51 GMT Connection跟HTTP头中的Connection是同样的原理 Content-Encoding跟HTTP中头的Content-Encoding是同样的原理 Content-Length作用：指明实体正文的长度，以字节方式存储的十进制数字来表示。例如: Content-Length: 156474 Date作用: 生成消息的具体时间和日期例如: Date: Sun, 20 Nov 2016 05:09:51 GMT 响应体响应体包含的内容是网页的内容信息，主要是html代码等","link":"/111.html"},{"title":"每逢佳节倍思亲","text":"回忆童年每逢冬至来临外面寒风凛凛屋内热气腾腾全家人围在一起吃汤圆团团圆圆 如今身为游子在外漂流更思念家更思念父母亲只能发QQ说说、微博、朋友圈给远方家人送上祝福？不！没有比给家人打一电话来得更加直接来的更加亲切拿起你手中的手机给家里打一通电话为爸妈送上节日祝福！祝：冬至快乐~","link":"/124.html"},{"title":"常用Python标准库","text":"众所周知，Python有庞大的库资源，有官方标准库以及第三方的扩展库。每个库都一把利器，能帮助我们快速处理某方面的问题。作为一名python的初学者，当把基本的语法、列表和元组、字典、迭代器、异常处理、I/O操作、抽象等知识点学完之后。我建议把官方常用的标准库也随便学下来。讲真的，你知道这些库之后，你会有种相见恨晚的感觉。 接下来带大家走进python标准库的世界。PS： 使用Python的版本为Python3 字符串 re: 正则表达式。用来判断字符串是否是你指定的特定字符串。在爬虫项目中，经常能捕获到它的身影。 StringIO: 提供以文件为保存形式来读和写字符串。还有个性能更加好的cStringIO版本 struct: 以二进制字节序列来解释字符串。可以通过格式化参数，指定类型、长度、字节序（大小端）、内存对齐等。 数据类型 bisect: 数组二分算法。提供支持按顺序对列表进行排序，而不必每次在列表中插入后再去排序。 heapq: 堆队列算法。最小堆：完全平衡二叉树， 所有节点都小于字节点。 datetime: 提供操作日期和时间的类。其中有两种日期和时间类型： naive和aware collections: 高性能容器数据类型。实现了Python的通用内置容器、字典、列表、集合，和元组专门的数据类型提供替代品 pprint: 提供”整洁低打印”任意Python数据结构的能力。 数学运算 random: 各种分布的伪随机数的生成器 math: 数学函数。提供了由C标准的数学函数访问。该库的函数不适用于复数。 cmath: 为复数提供的数学函数。 operator: 提供了重载操作符 文件和目录 os.path: 常用路径名操作。提供了操作路径名的常用的函数。 filecmp: 文件和目录的比较。提供了比较文件和目录的函数。 shutil: 高级的文件操作。提供了许多文件和文件集上的操作操作。尤其是提供支持文件复制和删除的函数。 数据存储 serialization: Python专用的序列化算法，通常不建议用来存储自定义数据。 pickle: Python对象序列化。提供了一个基本但功能强大的Python对象序列化和反序列化算法。 cPickle: 比pickle快1000倍的对象序列化库， 和pickle可互相替换。 shevle: 将对象pickle序列化，然后保存到anydbm格式文件。anydbm是KV结构的数据库，可以保存多个序列化对象。 sqlite3: SQLite数据库DB-API 2.0接口。 数据压缩 zipfile: 提供了ZIP文件个创建、读取、写入、最佳和列出zip文件的函数。 tarfile: 提供了tar文件的压缩和解压的函数。 文件格式 csv: 提供对CSV文件的读取和写入的函数。 加密 hashlib: 安全哈希和消息摘要。实现了一个通用的接口来实现多个不同的安全哈希和消息摘要算法。包括 FIPS 安全哈希算法 SHA1、SHA224、SHA256、SHA384和 SHA512（定义在 FIPS 180-2），以及 RSA 的 MD5 算法（在互联网 RFC 1321中定义)。 hmac: 用于消息认证的加密哈希算法。实现了RFC 2104 中描述的HMAC 算法。 md5: 实现了MD5加密算法。 sha: 实现了sha1加密算法。 操作系统 time: 时间获取和转换。提供了各种与时间相关的函数。 argparse: 命令行选项、参数和子命令的解析器。使用该库使得编码用户友好的命令行接口非常容易。取代了之前的optparse io: 提供接口处理IO流。 logging: Python的日志工具。提供了日志记录的API。 logging.config: Python日志配置。用于配置日志模块的API。 os: 提供丰富的雨MAC，NT，Posix等操作系统进行交互的能力。这个模块允许程序独立的于操作系统环境。文件系统，用户数据库和权限进行交互。 _thread: 多线程控制。提供了一个底层、原始的操作 —— 多个控制线程共享全局数据空间。 threading: 高级线程接口。是基于_thread模块的，但是比_thread更加容易使用、更高层次的线程API。 sys: 提供访问和维护python解释器的能力。这包括了提示信息，版本，整数的最大值，可用模块，路径钩子，标准错误，标准输入输出的定位和解释器调用的命令行参数。 进程通信 subprocess: 管理子进程。允许用户产生新的进程，然后连接他们的输入/输出/错误/管道，并获取返回值。 socket: 底层网络接口。 signal: 设置异步时间处理handlers。信号是软中断，提供了一种异步事件通知机制。 网络数据处理 json: JSON格式的编码器和解码器。 base64: 提供依据RFC 3548的规定（Base16, Base32, Base64 ）进行数据编码和解码。 htmllib: 提供了一个HTML语法解析器。 mimetypes: 提供了判断给定的URL的MIME类型。 操作因特网网络协议 urllib: 提供了用于获取万维网数据的高层接口。这个是Python2.7版本的，Python3已经将其拆分成多个模块urllib.request，urllib.parse和urllib.error。 urlparse: 提供了用于处理URL的函数，可以在URL和平台特定的文件名间相互转换。 http.client: HTTP协议客户端。 telnetlib: 提供了实现Telnet协议的Telnet类。 poplib: POP3协议客户端。 ftplib: FTP协议客户端。 smtplib: SMTP协议客户端。 webbrowser: 提供控制浏览器行为的函数。","link":"/123.html"},{"title":"你为何要那么拼命？","text":"很多人往往有这样的状态，当完成一个目标之后，就守着这收获的成果沾沾自喜。你觉得考上了大学，就可以整天逃课沉迷于游戏？你觉得找到工作了，就可以准时下班走人，天天潇潇洒洒？答案是否定的。如果你是现在身处这状态，说明你对自己未来人生没有什么规划，是对自己极其不负责任的表示。你试问你自己，是否有在为自己拼命？ 01.何炅, 这个名字已经家喻户晓了。大家都知道他是大名鼎鼎的湖南电视台主持人。平时我们都在享受何老师给我们带来快乐，可知背后辛酸的汗水呢？何老师在读大学三年级时，每天需要应对高难度的阿拉伯语的学习，还担任着学生会的工作，兼职文艺部和宣传部的“要职”。除此之外，他还在央视担任支持，平日要撰写台本以及录影，有时还要出差去外地录制。每天他都很晚才回到学校，同学们可能已经下了晚自习，甚至都已经入睡了。而他只能先在学生会里将自己学生干部的事情都做完后，再回到宿舍开始预习第二天上课要准备的内容。 02.彭宇年轻的时还是街头一个小混混。当他树立人生中第一个梦想————进入电视台工作，生活从此跟之前是天壤之别。他时常对着电视机练习如何应对突发，还报名参加骗子的演员培训班。到了后来，他听说北京机会多，毅然决定北漂，每天在北京电影学院门口等着接活，做群众演员。 为何现在要拼命？只为自己，只为自己生活得更好，只为青春无悔。 你是否感到很震撼？看下你目前的生活状态，如果整天这么懒散。那么你该制定短期目标，并为之拼命一把。","link":"/15.html"},{"title":"学爬虫之道","text":"近来在阅读 《轻量级 Django》,虽然还没有读完，但我已经收益颇多。我不得不称赞 Django 框架的开发人员，他们把 Web 开发降低门槛。Django 让我从对 Web 开发是一无所知到现在可以编写小型 web 应用，这很舒服。 Django 已经算是入门，所以自己把学习目标转到爬虫。自己接下来会利用三个月的时间来专攻 Python 爬虫。这几天，我使用“主题阅读方法”阅读 Python 爬虫入门的文档。制定 Python 爬虫的学习路线。 第一阶段：夯实入门要就是在打基础，所以要从最基础的库学起。下面是几个库是入门最经典的库 urllib它属于 Python 标准库。该库的作用是请求网页并下载数据。在学习该库之前，最好把 HTTP 协议了解下。这会大大提高后面的学习效率。 先学会如何使用 urllib 请求到数据，再学习一些高级用法。例如： 设置 Headers: 某些网站反感爬虫的到访，于是对爬虫一律拒绝请求。设置 Headers 可以把请求伪装成浏览器访问网站。 Proxy 的设置: 某些站点做了反倒链的设置，会将高频繁访问的 IP 地址封掉。所以我们需要用到代理池。 错误解析：根据 URLError 与 HTTPError 返回的错误码进行解析。 Cookie 的使用：可以模拟网站登录，需要结合 cookielib 一起使用。 rere 是正则表达式库。同时也是 Python 标准库之一。它的作用是匹配我们需要爬取的内容。所以我们需要掌握正则表达式常用符号以及常用方法的用法。 BeautifulSoupBeautifulSoup 是解析网页的一款神器。它可以从 HTML 或者 XML 文件中提取数据。配合 urllib 可以编写出各种小巧精干的爬虫脚本。 第二阶段：进阶当把基础打牢固之后，我们需要更进一步学习。使用更加完善的库来提高爬取效率 使用多线程使用多线程抓取数据，提高爬取数据效率。 学习 RequestsRequests 作为 urlilb 的替代品。它是更加人性化、更加成熟的第三方库。使用 Requests 来处理各种类型的请求，重复抓取问题、cookies 跟随问题、多线程多进程、多节点抓取、抓取调度、资源压缩等一系列问题。 学习 XpathXpath 也算是一款神器。它是一款高效的、表达清晰简单的分析语言。掌握它以后介意弃用正则表达式了。一般是使用浏览器的开发者工具 加 lxml 库。 学习 Selenium使用 Selenium，模拟浏览器提交类似用户的操作，处理js动态产生的网页。因为一些网站的数据是动态加载的。类似这样的网站，当你使用鼠标往下滚动时，会自动加载新的网站。 第三阶段：突破学习 ScrapyScrapy 是一个功能非常强大的分布式爬虫框架。我们学会它，就可以不用重复造轮子。 数据存储如果爬取的数据条数较多，我们可以考虑将其存储到数据库中。因此，我们需要学会 MySqlMongoDB、SqlLite的用法。更加深入的，可以学习数据库的查询优化。 第四阶段：为我所用当爬虫完成工作，我们已经拿到数据。我们可以利用这些数据做数据分析、数据可视化、做创业项目原始启动数据等。我们可以学习 NumPy、Pandas、 Matplotlib 这三个库。 NumPy ：它是高性能科学计算和数据分析的基础包。 Pandas : 基于 NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。它可以算得上作弊工具。 Matplotlib：Python中最著名的绘图系统Python中最著名的绘图系统。它可以制作出散点图，折线图，条形图，直方图，饼状图，箱形图散点图，折线图，条形图，直方图，饼状图，箱形图等。","link":"/62.html"},{"title":"详解 python3 urllib","text":"本文是爬虫系列文章的第一篇，主要讲解 Python 3 中的 urllib 库的用法。urllib 是 Python 标准库中用于网络请求的库。该库有四个模块，分别是urllib.request，urllib.error，urllib.parse，urllib.robotparser。其中urllib.request，urllib.error两个库在爬虫程序中应用比较频繁。那我们就开门见山，直接讲解这两个模块的用法。 发起请求模拟浏览器发起一个 HTTP 请求，我们需要用到 urllib.request 模块。urllib.request 的作用不仅仅是发起请求， 还能获取请求返回结果。发起请求，单靠 urlopen() 方法就可以叱咤风云。我们先看下 urlopen() 的 API 1urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) 第一个参数 String 类型的地址或者 data 是 bytes 类型的内容，可通过 bytes()函数转为化字节流。它也是可选参数。使用 data 参数，请求方式变成以 POST 方式提交表单。使用标准格式是application/x-www-form-urlencoded timeout 参数是用于设置请求超时时间。单位是秒。 cafile和capath代表 CA 证书和 CA 证书的路径。如果使用HTTPS则需要用到。 context参数必须是ssl.SSLContext类型，用来指定SSL设置 cadefault参数已经被弃用，可以不用管了。 该方法也可以单独传入urllib.request.Request对象 该函数返回结果是一个http.client.HTTPResponse对象。 简单抓取网页我们使用 urllib.request.urlopen() 去请求百度贴吧，并获取到它页面的源代码。 123456import urllib.requesturl = \"http://tieba.baidu.com\"response = urllib.request.urlopen(url)html = response.read() # 获取到页面的源代码print(html.decode('utf-8')) # 转化为 utf-8 编码 设置请求超时有些请求可能因为网络原因无法得到响应。因此，我们可以手动设置超时时间。当请求超时，我们可以采取进一步措施，例如选择直接丢弃该请求或者再请求一次。 12345import urllib.requesturl = \"http://tieba.baidu.com\"response = urllib.request.urlopen(url, timeout=1)print(response.read().decode('utf-8')) 使用 data 参数提交数据在请求某些网页时需要携带一些数据，我们就需要使用到 data 参数。 123456789101112import urilib.parseimport urllib.requesturl = \"http://127.0.0.1:8000/book\"params = { 'name':'浮生六记', 'author':'沈复'}data = bytes(urllib.parse.urlencode(params), encoding='utf8')response = urllib.request.urlopen(url, data=data)print(response.read().decode('utf-8')) params 需要被转码成字节流。而 params 是一个字典。我们需要使用 urllib.parse.urlencode() 将字典转化为字符串。再使用 bytes() 转为字节流。最后使用 urlopen() 发起请求，请求是模拟用 POST 方式提交表单数据。 使用 Request由上我们知道利用 urlopen() 方法可以发起简单的请求。但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入headers（请求头）、指定请求方式等信息，我们就可以利用更强大的Request类来构建一个请求。按照国际惯例，先看下 Request 的构造方法： 1urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None) url 参数是请求链接，这个是必传参数，其他的都是可选参数。 data 参数跟 urlopen() 中的 data 参数用法相同。 headers 参数是指定发起的 HTTP 请求的头部信息。headers 是一个字典。它除了在 Request 中添加，还可以通过调用 Reques t实例的 add_header() 方法来添加请求头。 origin_req_host 参数指的是请求方的 host 名称或者 IP 地址。 unverifiable 参数表示这个请求是否是无法验证的，默认值是False。意思就是说用户没有足够权限来选择接收这个请求的结果。例如我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，我们就要将 unverifiable 的值设置成 True。 method 参数指的是发起的 HTTP 请求的方式，有 GET、POST、DELETE、PUT等 简单使用 Request使用 Request 伪装成浏览器发起 HTTP 请求。如果不设置 headers 中的 User-Agent，默认的User-Agent是Python-urllib/3.5。可能一些网站会将该请求拦截，所以需要伪装成浏览器发起请求。我使用的 User-Agent 是 Chrome 浏览器。 123456789import urllib.requesturl = \"http://tieba.baidu.com/\"headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}request = urllib.request.Request(url=url, headers=headers)response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) Request 高级用法如果我们需要在请求中添加代理、处理请求的 Cookies，我们需要用到Handler和OpenerDirector。 1） HandlerHandler 的中文意思是处理者、处理器。 Handler 能处理请求（HTTP、HTTPS、FTP等）中的各种事情。它的具体实现是这个类 urllib.request.BaseHandler。它是所有的 Handler 的基类，其提供了最基本的Handler的方法，例如default_open()、protocol_request()等。继承 BaseHandler 有很多个，我就列举几个比较常见的类： ProxyHandler：为请求设置代理 HTTPCookieProcessor：处理 HTTP 请求中的 Cookies HTTPDefaultErrorHandler：处理 HTTP 响应错误。 HTTPRedirectHandler：处理 HTTP 重定向。 HTTPPasswordMgr：用于管理密码，它维护了用户名密码的表。 HTTPBasicAuthHandler：用于登录认证，一般和 HTTPPasswordMgr 结合使用。 2） OpenerDirector对于 OpenerDirector，我们可以称之为 Opener。我们之前用过 urlopen() 这个方法，实际上它就是 urllib 为我们提供的一个Opener。那 Opener 和 Handler 又有什么关系？opener 对象是由 build_opener(handler) 方法来创建出来 。我们需要创建自定义的 opener，就需要使用 install_opener(opener)方法。值得注意的是，install_opener 实例化会得到一个全局的 OpenerDirector 对象。 使用代理我们已经了解了 opener 和 handler，接下来我们就通过示例来深入学习。第一个例子是为 HTTP 请求设置代理有些网站做了浏览频率限制。如果我们请求该网站频率过高。该网站会被封 IP，禁止我们的访问。所以我们需要使用代理来突破这“枷锁”。 1234567891011121314151617import urllib.requesturl = \"http://tieba.baidu.com/\"headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}proxy_handler = urllib.request.ProxyHandler({ 'http': 'web-proxy.oa.com:8080', 'https': 'web-proxy.oa.com:8080'})opener = urllib.request.build_opener(proxy_handler)urllib.request.install_opener(opener)request = urllib.request.Request(url=url, headers=headers)response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) 认证登录有些网站需要携带账号和密码进行登录之后才能继续浏览网页。碰到这样的网站，我们需要用到认证登录。我们首先需要使用 HTTPPasswordMgrWithDefaultRealm() 实例化一个账号密码管理对象；然后使用 add_password() 函数添加账号和密码；接着使用 HTTPBasicAuthHandler() 得到 hander；再使用 build_opener() 获取 opener 对象；最后使用 opener 的 open() 函数发起请求。 第二个例子是携带账号和密码请求登录百度贴吧，代码如下： 123456789101112import urllib.requesturl = \"http://tieba.baidu.com/\"user = 'user'password = 'password'pwdmgr = urllib.request.HTTPPasswordMgrWithDefaultRealm()pwdmgr.add_password(None，url ，user ，password)auth_handler = urllib.request.HTTPBasicAuthHandler(pwdmgr)opener = urllib.request.build_opener(auth_handler)response = opener.open(url)print(response.read().decode('utf-8')) Cookies设置如果请求的页面每次需要身份验证，我们可以使用 Cookies 来自动登录，免去重复登录验证的操作。获取 Cookies 需要使用 http.cookiejar.CookieJar() 实例化一个 Cookies 对象。再用 urllib.request.HTTPCookieProcessor 构建出 handler 对象。最后使用 opener 的 open() 函数即可。 第三个例子是获取请求百度贴吧的 Cookies 并保存到文件中，代码如下： 123456789101112131415import http.cookiejarimport urllib.requesturl = \"http://tieba.baidu.com/\"fileName = 'cookie.txt'cookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open(url)f = open(fileName,'a')for item in cookie: f.write(item.name+\" = \"+item.value+'\\n')f.close() HTTPResponse从上面的例子可知， 使用 urllib.request.urlopen() 或者 opener.open(url) 返回结果是一个 http.client.HTTPResponse 对象。它具有 msg、version、status、reason、debuglevel、closed等属性以及read()、readinto()、getheader(name)、getheaders()、fileno()等函数。 错误解析发起请求难免会出现各种异常，我们需要对异常进行处理，这样会使得程序比较人性化。异常处理主要用到两个类，urllib.error.URLError和urllib.error.HTTPError。 URLErrorURLError 是 urllib.error 异常类的基类, 可以捕获由urllib.request 产生的异常。它具有一个属性reason，即返回错误的原因。 捕获 URL 异常的示例代码： 12345678import urllib.requestimport urllib.errorurl = \"http://www.google.com\"try: response = request.urlopen(url)except error.URLError as e: print(e.reason) HTTPError HTTPError 是 UEKRrror 的子类，专门处理 HTTP 和 HTTPS 请求的错误。它具有三个属性。 1)code：HTTP 请求返回的状态码。 1)renson：与父类用法一样，表示返回错误的原因。 1)headers`：HTTP 请求返回的响应头信息。 获取 HTTP 异常的示例代码, 输出了错误状态码、错误原因、服务器响应头 12345678910import urllib.requestimport urllib.errorurl = \"http://www.google.com\"try: response = request.urlopen(url)except error.HTTPError as e: print('code: ' + e.code + '\\n') print('reason: ' + e.reason + '\\n') print('headers: ' + e.headers + '\\n')","link":"/66.html"},{"title":"Python 正则表达式","text":"我们能够使用 urllib 向网页请求并获取其网页数据。但是抓取信息数据量比较大，我们可能需要其中一小部分数据。对付刚才的难题，就需要正则表达式出马了。正则表达式能帮助我们匹配过滤到我们需要的数据，但它学习起来非常枯燥无味。你可能会说，我还没有开始想学习正则表达式，你就来打击我？ 莫慌！层层递进地学习，一步一个脚印地学习，很快就会学会了。对于爬虫，我觉得学会最基本的符号就差不多了。 正则表达式下面是一张关于正则表达式字符的图，图片资料来自CSDN。先把图中字符了解清楚，基本上算是入门。 re 模块Python 自 1.5 版本起通过新增 re （Regular Expression 正则表达式）模块来提供对正则表达式的支持。使用 re 模块先将正则表达式填充到 Pattern 对象中，再把 Pattern 对象作为参数使用 match 方法去匹配的字符串文本。match 方法会返回一个 Match 对象，再通过 Match 对象会得到我们的信息并进行操作。下面介绍几个 re 常用的函数。 compile 函数compile 是把正则表达式的模式和标识转化成正则表达式对象，供 match() 和 search() 这两个函数使用。它的函数语法如下： 1re.compile(pattern[, flags]) 第一个参数是pattern，指的正则表达式。 第二个参数flags是匹配模式，是个可选参数。可以使用按位或’|’表示同时生效，也可以在正则表达式字符串中指定。匹配模式有以下几种： flag 描述 re.I(全拼：IGNORECASE) 忽略大小写（括号内是完整写法，下同） re.M(全拼：MULTILINE) 多行模式，改变’^’和’$’的行为（参见上图） re.S(全拼：DOTALL) 点任意匹配模式，改变’.’的行为 re.L(全拼：LOCALE) 使预定字符类 \\w \\W \\b \\B \\s \\S 取决于当前区域设定 re.U(全拼：UNICODE) 使预定字符类 \\w \\W \\b \\B \\s \\S \\d \\D 取决于unicode定义的字符属性 re.X(全拼：VERBOSE) 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。 该方法返回的结果是一个 Pattern 对象。 match 函数match()函数只在字符串的开始位置尝试匹配正则表达式，也就是说只有在 0 位置匹配成功的话才有返回。如果不是开始位置匹配成功的话，match() 就返回 none。它的函数语法如下： 1re.match(pattern, string[, flags]) 第一个参数：匹配的正则表达式 第二个参数：要被匹配的字符串 flags 是可选参数，跟 compile 用法相似 匹配成功 re.match 方法返回一个匹配的对象，否则返回None。要想获得匹配结果，既可以使用groups()函数获取一个包含所有字符串的元组（从 1 到 所含的小组号），也可以使用group(组号)函数获取某个组号的字符串。 match 函数用法的示例代码： 123456789import repattern = re.compile('Python')text = 'Python python pythonn'match = re.search(pattern, text)if match: print(match.group())else: print('没有匹配') search 函数 search() 函数是扫描整个字符串来查找匹配，它返回结果是第一个成功匹配的字符串。 1re.search(pattern, string[, flags]) 参数用法以及返回结果跟match函数用法相同。 search 函数用法的示例代码： 123456789import repattern = re.compile('Python')text = 'welcome to Python world!'match = re.search(pattern, text)if match: print(match.group())else: print('没有匹配') findall 函数findall函数在字符串中搜索子串，并以列表形式返回全部能匹配的所有子串。 1re.findall(pattern, string[, flags]) 参数用法以及返回结果跟match函数用法相同。 findall 函数用法的示例代码： 123456789import repattern = re.compile('\\d+')text = 'one1two2three3four4'list = re.findall(pattern, text)if list: print(list)else: print('没有匹配')","link":"/67.html"},{"title":"内容提取神器 beautifulSoup 的用法","text":"上篇文章只是简单讲述正则表达式如何读懂以及 re 常见的函数的用法。我们可能读懂别人的正则表达式，但是要自己写起正则表达式的话，可能会陷入如何写的困境。正则表达式写起来费劲又出错率高，那么有没有替代方案呢？俗话说得好，条条道路通罗马。目前还两种代替其的办法，一种是使用 Xpath 神器，另一种就是本文要讲的 BeautifulSoup。 BeautifulSoup 简介引用 BeautifulSoup 官网的说明： Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work. 大致意思如下: BeautifulSoup 是一个能从 HTML 或 XML 文件中提取数据的 Python 库。它能通过自己定义的解析器来提供导航、搜索，甚至改变解析树。它的出现，会大大节省开发者的时间。 安装 BeautifulSoup目前 BeautifulSoup 最新版本是 4.6.0，它是支持 Python3的。所以可以大胆去升级安装使用。 安装方法有两种： 使用pip比较推荐使用这种方式，既简单又方便管理。 1234pip install beautifulsoup4# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install beautifulsoup4 使用easy_install 1easy_install beautifulsoup4 使用系统包管理 12sudo apt-get install Python-bs4# 适用于 ubuntu 系统以及 Debian 系统 初始 BeautifulSoup首先导入 BeautifulSoup 库，然后创建一个 BeautifulSoup 对象，再利用对象做文章。具体参考示例代码： 1234from bs4 import BeautifulSoupsoup = BeautifulSoup(response)print(soup.prettify()) 上面代码中，response 可以urlllib或者request请求返回的内容，也可以是本地 HTML 文本。如果要打开本地，代码需要改为 12soup = BeautifulSoup(open(\"index.html\"))# 打开当前目录下 index.html 文件 soup.prettify()函数的作用是打印整个 html 文件的 dom 树，例如上面执行结果如下： 123456789101112&lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse's story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt; &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt; &lt;/body&gt;&lt;/html&gt; 解析 BeautifulSoup 对象想从 html 中获取到自己所想要的内容，我归纳出三种办法： 利用 Tag 对象从上文得知，BeautifulSoup 将复杂 HTML 文档转换成一个复杂的树形结构,每个节点都是Python对象。跟安卓中的Gson库有异曲同工之妙。节点对象可以分为 4 种：Tag, NavigableString, BeautifulSoup, Comment。 Tag 对象可以看成 HTML 中的标签。这样说，你大概明白具体是怎么回事。我们再通过例子来更加深入了解 Tag 对象。以下代码是以 prettify() 打印的结果为前提。 例子1 获取head标签内容 123print(soup.head)# 输出结果如下：&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt; 例子2 获取title标签内容 123print(soup.title)# 输出结果如下：&lt;title&gt;The Dormouse's story&lt;/title&gt; 例子3 获取p标签内容 123print(soup.p)# 输出结果如下：&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; 如果 Tag 对象要获取的标签有多个的话，它只会返回所以内容中第一个符合要求的标签。 对象一般含有属性，Tag 对象也不例外。它具有两个非常重要的属性， name 和 attrs。 namename 属性是 Tag 对象的标签名。不过也有特殊的，soup 对象的 name 是 [document] 12345print(soup.name)print(soup.head.name)# 输出结果如下：[document]head attrsattrs 属性是 Tag 对象所包含的属性值，它是一个字典类型。 123print(soup.p.attrs）# 输出结果如下：{'class': ['title'], 'name': 'dromouse'} 其他三个属性也顺带介绍下: NavigableString 说白了就是：Tag 对象里面的内容 123print(soup.title.string) # 输出结果如下：The Dormouse's story BeautifulSoup BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象。它是一个特殊的 Tag。 1234567print(type(soup.name))print(soup.name)print(soup.attrs)# 输出结果如下：&lt;type 'unicode'&gt;[document]{} 空字典 Comment Comment 对象是一个特殊类型的 NavigableString 对象。如果 HTML 页面中含有注释及特殊字符串的内容。而那些内容不是我们想要的，所以我们在使用前最好做下类型判断。例如： 12if type(soup.a.string) == bs4.element.Comment: ... # 执行其他操作，例如打印内容 利用过滤器过滤器其实是一个find_all()函数， 它会将所有符合条件的内容以列表形式返回。它的构造方法如下： 1find_all(name, attrs, recursive, text, **kwargs ) name 参数可以有多种写法： （1）节点名 123print(soup.find_all('p'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] （2）正则表达式 123print(soup.find_all(re.compile('^p')))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] （3）列表如果参数为列表，过滤标准为列表中的所有元素。看下具体代码，你就会一目了然了。 123print(soup.find_all(['p', 'a']))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;, &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 另外 attrs 参数可以也作为过滤条件来获取内容，而 limit 参数是限制返回的条数。 利用 CSS 选择器以 CSS 语法为匹配标准找到 Tag。同样也是使用到一个函数，该函数为select()，返回类型也是 list。它的具体用法如下, 同样以 prettify() 打印的结果为前提： （1）通过 tag 标签查找 123print(soup.select(head))# 输出结果如下：[&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;] （2）通过 id 查找 123print(soup.select('#link1'))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] （3）通过 class 查找 123print(soup.select('.sister'))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] （4）通过属性查找 123print(soup.select('p[name=dromouse]'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;] 123print(soup.select('p[class=title]'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;] （5）组合查找 1234print(soup.select(\"body p\"))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;,&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] 123print(soup.select(\"p &gt; a\"))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 123print(soup.select(\"p &gt; .sister\"))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 处理上下关系从上文可知，我们已经能获取到节点对象，但有时候需要获取其父节点或者子节点的内容，我们要怎么做了？这就需要对parse tree进行遍历 （1）获取子节点利用.children属性，该属性会返回当前节点所以的子节点。但是它返回的类型不是列表，而是迭代器 （2）获取所有子孙节点使用.descendants属性，它会返回所有子孙节点的迭代器 （3）获取父节点通过.parent属性可以获得所有子孙节点的迭代器 （4）获取所有父节点.parents属性，也是返回所有子孙节点的迭代器 （5）获取兄弟节点兄弟节点可以理解为和本节点处在统一级的节点，.next_sibling属性获取了该节点的下一个兄弟节点，.previous_sibling则与之相反，如果节点不存在，则返回 None 注意：实际 HTML 中的 tag 的.next_sibling和 .previous_sibling属性通常是字符串或空白，因为空白或者换行也可以被视作一个节点，所以得到的结果可能是空白或者换行 （5）获取所有兄弟节点通过.next_siblings和.previous_siblings属性可以对当前节点的兄弟节点迭代输出","link":"/78.html"},{"title":"爬虫实战一：爬取当当网所有 Python 书籍","text":"我们已经学习 urllib、re、BeautifulSoup 这三个库的用法。但只是停留在理论层面上，还需实践来检验学习成果。因此，本文主要讲解如何利用我们刚才的几个库去实战。 确定爬取目标任何网站皆可爬取，就看你要不要爬取而已。本次选取的爬取目标是当当网，爬取内容是 以 Python 为关键字搜索出来的页面中所有书籍的信息。具体如下图所示： 本次爬取结果有三项： 图书的封面图片 图书的书名 图书的链接页面最后把这三项内容保存到 csv 文件中。 爬取过程总所周知，每个站点的页面 DOM 树是不一样的。所以我们需要先对爬取页面进行分析，再确定自己要获取的内容，再定义程序爬取内容的规则。 确定 URL 地址我们可以通过利用浏览器来确定URL 地址，为 urllib 发起请求提供入口地址。接下来，我们就一步步来确定请求地址。搜索结果页面为 1 时，URL 地址如下： 搜索结果页面为 3 时，URL 地址如下： 搜索结果页面为 21 时，即最后一页，URL 地址如下： 从上面的图片中，我们发现 URL 地址的差异就在于 page_index 的值，所以 URL 地址最终为 http://search.dangdang.com/?key=python&amp;act=input&amp;show=big&amp;page_index=。而 page_index 的值，我们可以通过循环依次在地址后面添加。因此， urllib 请求代码可以这样写： 123456789101112# 爬取地址, 当当所有 Python 的书籍, 一共是 21 页url = \"http://search.dangdang.com/?key=python&amp;act=input&amp;show=big&amp;page_index=\"index = 1while index &lt;= 21: # 发起请求 request = urllib.request.Request(url=url+str(index), headers=headers) response = urllib.request.urlopen(request) index = index + 1 # 解析爬取内容 parseContent(response) time.sleep(1) # 休眠1秒 确定爬取节点有了 URL 地址，就能使用 urllib 获取到页面的 html 内容。到了这步，我们就需要找到爬取的节点的规则，以便于 BeautifulSoup 地解析。为了搞定这个问题，就要祭出大招 —— Chrome 浏览器的开发者功能（按下 F12 键就能启动）。我们按下 F12 键盘，依次对每本书进行元素检查（在页面使用鼠标右键，点击“检查”即可），具体结果如下： 从上图可以得知解析规则：每本书的节点是一个 a 标签，a 标签具有 title，href，子标签 img 的 src 三个属性，这三者分别对应书名、书的链接页面、书的封图。看到这里也需你不会小激动，感叹这不就是我们要感兴趣的内容吗？得到解析规则，编写BeautifulSoup 解析代码就有了思路，具体代码如下： 1234567891011121314151617# 提取爬取内容中的 a 标签, 例如：# &lt;a# class=\"pic\" dd_name=\"单品图片\"# ddclick=\"act=normalResult_picture&amp;amp;pos=23648843_53_2_q\"# href=\"http://product.dangdang.com/23648843.html\"# name=\"itemlist-picture\"# target=\"_blank\" title=\"# 趣学Python――教孩子学编程 \"&gt;## &lt;img# alt=\" 趣学Python――教孩子学编程 \"# data-original=\"http://img3x3.ddimg.cn/20/34/23648843-1_b_0.jpg\"# src=\"images/model/guan/url_none.png\"/&gt;# &lt;/a&gt;soup = BeautifulSoup(response)books = soup.find_all('a', class_='pic')print(books) 运行结果如下： 这证明刚才制定规则是正确爬取我们所需的内容。 保存爬取信息我写爬虫程序有个习惯，就是每次都会爬取内容持久化到文件中。这样方便以后查看使用。如果爬取数据量比较大，我们可以用其做数据分析。我这里为了方便，就将数据保存到 csv 文件中。用 Python 将数据写到文件中，我们经常中文乱码问题所烦恼。如果单纯使用 csv 库，可能摆脱不了这烦恼。所以我们将 csv 和 codecs 结合一起使用。在写数据到 csv 文件的时候，我们可以通过指定文件编码。这样中文乱码问题就迎刃而解。具体代码如下: 12345678910111213141516171819fileName = 'PythonBook.csv'# 指定编码为 utf-8, 避免写 csv 文件出现中文乱码with codecs.open(fileName, 'w', 'utf-8') as csvfile: filednames = ['书名', '页面地址', '图片地址'] writer = csv.DictWriter(csvfile, fieldnames=filednames) writer.writeheader() for book in books: # print(book) # print(book.attrs) # 获取子节点&lt;img&gt; # (book.children)[0] if len(list(book.children)[0].attrs) == 3: img = list(book.children)[0].attrs['data-original'] else: img = list(book.children)[0].attrs['src'] writer.writerow({'书名': book.attrs['title'], '页面地址': book.attrs['href'], '图片地址': img}) 看到这里，你可能会问为什么不把编码指定为 gb2312 呢，这样用 ecxel 打开就不会乱码了？原因是当书名全部为英文单词时，使用 gb2312 编码，writer.writerow()会出现编码错误的问题。 如果你要用 excel 打开 PythonBook.csv文件, 你则需多执行下面几步： 1) 打开 Excel 2) 执行“数据”-&gt;“自文本” 3) 选择 CSV 文件，出现文本导入向导 4) 选择“分隔符号”，下一步 5) 勾选“逗号”，去掉“ Tab 键”，下一步，完成 6）在“导入数据”对话框里，直接点确定 爬取结果最后，我们将上面代码整合起来即可。这里就不把代码贴出来了，具体阅读原文即可查看源代码。我就把爬取结果截下图： 写在最后这次实战算是结束了，但是我们不能简单地满足，看下程序是否有优化的地方。我把该程序不足的地方写出来。 该程序是单线程，没有使用多线程，执行效率不够高。 没有应用面向对象编程思想，程序的可扩展性不高。 没有使用随机 User-Agent 和 代理，容易被封 IP。","link":"/79.html"},{"title":"Python 多进程与多线程","text":"前言：为什么有人说 Python 的多线程是鸡肋，不是真正意义上的多线程？ 看到这里，也许你会疑惑。这很正常，所以让我们带着问题来阅读本文章吧。 问题：1、Python 多线程为什么耗时更长？2、为什么在 Python 里面推荐使用多进程而不是多线程？ 基础知识现在的 PC 都是多核的，使用多线程能充分利用 CPU 来提供程序的执行效率。 线程线程是一个基本的 CPU 执行单元。它必须依托于进程存活。一个线程是一个execution context（执行上下文），即一个 CPU 执行时所需要的一串指令。 进程进程是指一个程序在给定数据集合上的一次执行过程，是系统进行资源分配和运行调用的独立单位。可以简单地理解为操作系统中正在执行的程序。也就说，每个应用程序都有一个自己的进程。 每一个进程启动时都会最先产生一个线程，即主线程。然后主线程会再创建其他的子线程。 两者的区别 线程必须在某个进行中执行。 一个进程可包含多个线程，其中有且只有一个主线程。 多线程共享同个地址空间、打开的文件以及其他资源。 多进程共享物理内存、磁盘、打印机以及其他资源。 线程的类型线程的因作用可以划分为不同的类型。大致可分为： 主线程 子线程 守护线程（后台线程） 前台线程 Python 多线程GIL其他语言，CPU 是多核时是支持多个线程同时执行。但在 Python 中，无论是单核还是多核，同时只能由一个线程在执行。其根源是 GIL 的存在。 GIL 的全称是 Global Interpreter Lock(全局解释器锁)，来源是 Python 设计之初的考虑，为了数据安全所做的决定。某个线程想要执行，必须先拿到 GIL，我们可以把 GIL 看作是“通行证”，并且在一个 Python 进程中，GIL 只有一个。拿不到通行证的线程，就不允许进入 CPU 执行。 而目前 Python 的解释器有多种，例如： CPython：CPython 是用C语言实现的 Python 解释器。 作为官方实现，它是最广泛使用的 Python 解释器。 PyPy：PyPy 是用RPython实现的解释器。RPython 是 Python 的子集， 具有静态类型。这个解释器的特点是即时编译，支持多重后端（C, CLI, JVM）。PyPy 旨在提高性能，同时保持最大兼容性（参考 CPython 的实现）。 Jython：Jython 是一个将 Python 代码编译成 Java 字节码的实现，运行在JVM (Java Virtual Machine) 上。另外，它可以像是用 Python 模块一样，导入 并使用任何Java类。 IronPython：IronPython 是一个针对 .NET 框架的 Python 实现。它 可以用 Python 和 .NET framewor k的库，也能将 Python 代码暴露给 .NET 框架中的其他语言。 GIL 只在 CPython 中才有，而在 PyPy 和 Jython 中是没有 GIL 的。 每次释放 GIL锁，线程进行锁竞争、切换线程，会消耗资源。这就导致打印线程执行时长，会发现耗时更长的原因。 并且由于 GIL 锁存在，Python 里一个进程永远只能同时执行一个线程(拿到 GIL 的线程才能执行)，这就是为什么在多核CPU上，Python 的多线程效率并不高的根本原因。 创建多线程Python提供两个模块进行多线程的操作，分别是thread和threading，前者是比较低级的模块，用于更底层的操作，一般应用级别的开发不常用。 方法1：直接使用threading.Thread() 1234567891011import threading# 这个函数名可随便定义def run(n): print(\"current task：\", n)if __name__ == \"__main__\": t1 = threading.Thread(target=run, args=(\"thread 1\",)) t2 = threading.Thread(target=run, args=(\"thread 2\",)) t1.start() t2.start() 方法2：继承threading.Thread来自定义线程类，重写run方法 12345678910111213141516import threadingclass MyThread(threading.Thread): def __init__(self, n): super(MyThread, self).__init__() # 重构run函数必须要写 self.n = n def run(self): print(\"current task：\", n)if __name__ == \"__main__\": t1 = MyThread(\"thread 1\") t2 = MyThread(\"thread 2\") t1.start() t2.start() 线程合并Join函数执行顺序是逐个执行每个线程，执行完毕后继续往下执行。主线程结束后，子线程还在运行，join函数使得主线程等到子线程结束时才退出。 1234567891011121314import threadingdef count(n): while n &gt; 0: n -= 1if __name__ == \"__main__\": t1 = threading.Thread(target=count, args=(\"100000\",)) t2 = threading.Thread(target=count, args=(\"100000\",)) t1.start() t2.start() # 将 t1 和 t2 加入到主线程中 t1.join() t2.join() 线程同步与互斥锁线程之间数据共享的。当多个线程对某一个共享数据进行操作时，就需要考虑到线程安全问题。threading模块中定义了Lock 类，提供了互斥锁的功能来保证多线程情况下数据的正确性。 用法的基本步骤： 123456#创建锁mutex = threading.Lock()#锁定mutex.acquire([timeout])#释放mutex.release() 其中，锁定方法acquire可以有一个超时时间的可选参数timeout。如果设定了timeout，则在超时后通过返回值可以判断是否得到了锁，从而可以进行一些其他的处理。 具体用法见示例代码： 123456789101112131415161718192021import threadingimport timenum = 0mutex = threading.Lock()class MyThread(threading.Thread): def run(self): global num time.sleep(1) if mutex.acquire(1): num = num + 1 msg = self.name + ': num value is ' + str(num) print(msg) mutex.release()if __name__ == '__main__': for i in range(5): t = MyThread() t.start() 可重入锁（递归锁）为了满足在同一线程中多次请求同一资源的需求，Python 提供了可重入锁（RLock）。RLock内部维护着一个Lock和一个counter变量，counter 记录了 acquire 的次数，从而使得资源可以被多次 require。直到一个线程所有的 acquire 都被 release，其他的线程才能获得资源。 具体用法如下： 1234567891011#创建 RLockmutex = threading.RLock()class MyThread(threading.Thread): def run(self): if mutex.acquire(1): print(\"thread \" + self.name + \" get mutex\") time.sleep(1) mutex.acquire() mutex.release() mutex.release() 守护线程如果希望主线程执行完毕之后，不管子线程是否执行完毕都随着主线程一起结束。我们可以使用setDaemon(bool)函数，它跟join函数是相反的。它的作用是设置子线程是否随主线程一起结束，必须在start() 之前调用，默认为False。 定时器如果需要规定函数在多少秒后执行某个操作，需要用到Timer类。具体用法如下： 12345678from threading import Timer def show(): print(\"Pyhton\")# 指定一秒钟之后执行 show 函数t = Timer(1, hello)t.start() Python 多进程创建多进程Python 要进行多进程操作，需要用到muiltprocessing库，其中的Process类跟threading模块的Thread类很相似。所以直接看代码熟悉多进程。 方法1：直接使用Process, 代码如下： 123456789from multiprocessing import Process def show(name): print(\"Process name is \" + name)if __name__ == \"__main__\": proc = Process(target=show, args=('subprocess',)) proc.start() proc.join() 方法2：继承Process来自定义进程类，重写run方法, 代码如下： 123456789101112131415161718from multiprocessing import Processimport timeclass MyProcess(Process): def __init__(self, name): super(MyProcess, self).__init__() self.name = name def run(self): print('process name :' + str(self.name)) time.sleep(1)if __name__ == '__main__': for i in range(3): p = MyProcess(i) p.start() for i in range(3): p.join() 多进程通信进程之间不共享数据的。如果进程之间需要进行通信，则要用到Queue模块或者Pipi模块来实现。 Queue Queue 是多进程安全的队列，可以实现多进程之间的数据传递。它主要有两个函数,put和get。 put() 用以插入数据到队列中，put 还有两个可选参数：blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，该方法会阻塞 timeout 指定的时间，直到该队列有剩余的空间。如果超时，会抛出 Queue.Full 异常。如果 blocked 为 False，但该 Queue 已满，会立即抛出 Queue.Full 异常。 get()可以从队列读取并且删除一个元素。同样，get 有两个可选参数：blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，那么在等待时间内没有取到任何元素，会抛出 Queue.Empty 异常。如果blocked 为 False，有两种情况存在，如果 Queue 有一个值可用，则立即返回该值，否则，如果队列为空，则立即抛出 Queue.Empty 异常。 具体用法如下： 1234567891011from multiprocessing import Process, Queue def put(queue): queue.put('Queue 用法') if __name__ == '__main__': queue = Queue() pro = Process(target=put, args=(queue,)) pro.start() print(queue.get()) pro.join() Pipe Pipe的本质是进程之间的用管道数据传递，而不是数据共享，这和socket有点像。pipe() 返回两个连接对象分别表示管道的两端，每端都有send() 和recv()函数。 如果两个进程试图在同一时间的同一端进行读取和写入那么，这可能会损坏管道中的数据。 具体用法如下： 123456789101112from multiprocessing import Process, Pipe def show(conn): conn.send('Pipe 用法') conn.close() if __name__ == '__main__': parent_conn, child_conn = Pipe() pro = Process(target=show, args=(child_conn,)) pro.start() print(parent_conn.recv()) pro.join() 进程池创建多个进程，我们不用傻傻地一个个去创建。我们可以使用Pool模块来搞定。 Pool 常用的方法如下： 方法 含义 apply() 同步执行（串行） apply_async() 异步执行（并行） terminate() 立刻关闭进程池 join() 主进程等待所有子进程执行完毕。必须在close或terminate()之后使用 close() 等待所有进程结束后，才关闭进程池 具体用法见示例代码： 12345678910111213from multiprocessing import Pooldef show(num): print('num : ' + str(num))if __name__==\"__main__\": pool = Pool(processes = 3) for i in xrange(6): # 维持执行的进程总数为processes，当一个进程执行完毕后会添加新的进程进去 pool.apply_async(show, args=(i, )) print('====== apply_async ======') pool.close() #调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束 pool.join() 选择多线程还是多进程？在这个问题上，首先要看下你的程序是属于哪种类型的。一般分为两种 CPU 密集型 和 I/O 密集型。 CPU 密集型：程序比较偏重于计算，需要经常使用 CPU 来运算。例如科学计算的程序，机器学习的程序等。 I/O 密集型：顾名思义就是程序需要频繁进行输入输出操作。爬虫程序就是典型的 I/O 密集型程序。 如果程序是属于 CPU 密集型，建议使用多进程。而多线程就更适合应用于 I/O 密集型程序。","link":"/710.html"},{"title":"详解 Requests 库的用法","text":"如果你把上篇多线程和多进程的文章搞定了，那么要恭喜你了 。你编写爬虫的能力上了一个崭新的台阶。不过，我们还不能沾沾自喜，因为任重而道远。那么接下来就关注下本文的主要内容。本文主要介绍 urllib 库的代替品 —— Requests。 Requests 简介引用 Requests 官网的说明： Requests is the only Non-GMO HTTP library for Python, safe for human consumption. Requests 官方的介绍语是多么霸气。之所以能够这么霸气，是因为 Requests 相比 urllib 在使用方面上会让开发者感到更加人性化、更加简洁、更加舒服。并且国外一些知名的公司也在使用该库，例如 Google、Microsoft、Amazon、Twitter 等。因此，我们就更加有必要来学习 Request 库了。在学习之前，我们来看下它究竟具有哪些特性？ 具体如下： Keep-Alive &amp; 连接池 国际化域名和 URL 带持久 Cookie 的会话 浏览器式的 SSL 认证 自动内容解码 基本/摘要式的身份认证 优雅的 key/value Cookie 自动解压 Unicode 响应体 HTTP(S) 代理支持 文件分块上传 流下载 连接超时 分块请求 支持 .netrc 安装 Requests古人云：“工欲善其事，必先利其器”。在学习 Requests 之前，我们应先将库安装好。安装它有两种办法。 方法1：通过 pip 安装 比较推荐使用这种方式，既简单又方便管理。 1234pip install requests# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install requests 方法2：通过源码安装 先通过 git 克隆源码库： 1git clone git://github.com/kennethreitz/requests.git 或者直接到 github 网页上下载源码压缩包 接着进入到 requests 目录中执行以下命令： 1python setup.py install 发起请求有了前面学习 urllib 库的经验，现在我们学习 Requests 应该会更加容易上手。 简单抓取网页我们使用 Requests 向百度贴吧发起一个 HTTP 请求，并获取到它页面的源代码。 12345import requests# 使用 get 方式请求response = requests.get('https://tieba.baidu.com/')print(response.text) 那么使用 POST 请求网页，代码又该怎么写呢？相信答案已经浮现在你脑海中了。没错，就是将 get 换成 post 即可。 12345import requests# 使用 post 方式请求response = requests.post('https://tieba.baidu.com/')print(response.text) 传递 URL 参数我们在请求网页时，经常需要携带一些参数。Requests 提供了params关键字参数来满足我们的需求。params 是一个字符串字典，我们只要将字典构建并赋给 params 即可。我们也无须关心参数的编码问题，因为 Requests 很人性化，会将我们需要传递的参数正确编码。它的具体用法如下： 123456789import requestsurl = 'http://httpbin.org/get'params = {'name': 'Numb', 'author': 'Linkin Park'}# params 支持列表作为值# params = {'name': 'Good Time', 'author': ['Owl City', 'Carly Rae Jepsen']}response = requests.get(url, params=params)print(response.url)print(response.text) 如果字典为空是不会被拼接到 URL中的。另外，params 的拼接顺序是随机的，而不是写在前面就优先拼接。 123#运行结果如下：http://httpbin.org/get?name=Numb&amp;author=Linkin+Parkhttp://httpbin.org/get?name=Good+Time&amp;author=Owl+City&amp;author=Carly+Rae+Jepsen 你也许会疑问，为什么会有多了个”+”号呢？这个是 Requests 为了替代空格，它在请求时会自动转化为空格的。 构造请求头为了将 Requests 发起的 HTTP 请求伪装成浏览器，我们通常是使用headers关键字参数。headers 参数同样也是一个字典类型。具体用法见以下代码： 12345678910111213141516171819202122232425262728import requestsurl = 'https://tieba.baidu.com/'headers = { 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36' # 还可以设置其他字段。}response = requests.get(url, headers=headers)print(response.url)print(response.text)``` ### 使用 data 参数提交数据data 参数通常结合 POST 请求方式一起使用。如果我们需要用 POST 方式提交表单数据或者JSON数据，我们只需要传递一个字典给 data 参数。- 提交表单数据我们使用测试网页`http://httpbin.org/post`来提交表单数据作为例子进行展示。```pythonimport requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, data=data) print(response.text) 运行结果如下：我们会看到http://httpbin.org/post页面打印我们的请求内容中，有form字段。 12345678910111213141516171819202122{ \"args\": {}, \"data\": \"\", \"files\": {}, \"form\": { \"pass\": \"admin\", \"user\": \"admin\" }, \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Cache-Control\": \"max-age=259200\", \"Connection\": \"close\", \"Content-Length\": \"21\", \"Content-Type\": \"application/x-www-form-urlencoded\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.11.1\" }, \"json\": null, \"origin\": \"14.*.*.*\", \"url\": \"http://httpbin.org/post\"} 提交 JSON 数据 在HTTP 请求中，JSON 数据是被当作字符串文本。所以，我们使用 data 参数的传递 JSON 数据时，需要将其转为为字符串。我们继续使用上文的代码做演示。 12345678910import jsonimport requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, data=json.dumps(data))print(response.text) 你可以拿下面的运行结果和提交表单数据的运行结果做下对比，你会了解更加清楚两者的差异。 123456789101112131415161718192021{ \"args\": {}, \"data\": \"{\\\"pass\\\": \\\"admin\\\", \\\"user\\\": \\\"admin\\\"}\", \"files\": {}, \"form\": {}, \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Cache-Control\": \"max-age=259200\", \"Connection\": \"close\", \"Content-Length\": \"34\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.11.1\" }, \"json\": { \"pass\": \"admin\", \"user\": \"admin\" }, \"origin\": \"14.*.*.*\", \"url\": \"http://httpbin.org/post\"} 那是否有更加简便的方法来传递 JSON 数据？Requests 在 2.4.2 版本新增该功能。我们可以使用 json 参数直接传递，然后它会被自动编码。 123456789import requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, json=data)print(response.text) 使用代理有些网站做了浏览频率限制。如果我们请求该网站频率过高，该网站会被封掉我们的 IP，禁止我们的访问。所以我们需要使用代理来突破这“枷锁”。这里需要用到proxies参数，proxies 也是一个字典类型。具体用法如下： 123456789101112import requestsurl = 'https://tieba.baidu.com/'proxies = { 'http':\"web-proxy.oa.com:8080\", 'https':\"web-proxy.oa.com:8080\" # 若你的代理需要使用HTTP Basic Auth，可以使用 http://user:password@host/ 语法： # \"http\": \"http://user:pass@27.154.181.34:43353/\"}response = requests.get(url, proxies=proxies)print(response.url)print(response.text) 除了支持 HTTP 代理，Requests 在 2.10 版本新增支持 SOCKS 协议的代理。也就是说，Requests 能够使用 ShadowSocks 代理。看到这里，你的内心是不是有点小激动？使用 SOCKS 代理，需要额外安装一个第三方库，我们就使用 pip 来安装。 1pip install requests[socks] 安装成功之后，就可以正常使用了，用法跟 HTTP 代理相关。具体见代码： 1234proxies = { 'http': 'socks5://user:pass@host:port', 'https': 'socks5://user:pass@host:port'} 设置请求超时我们使用代理发起请求，经常会碰到因代理失效导致请求失败的情况。因此，我们对请求超时做下设置。当发现请求超时，更换代理再重连。 1response = requests.get(url, timeout=3) 如果你要同时设置 connect 和 read 的超时时间，可以传入一个元组进行设置。 1response = requests.get(url, timeout=(3, 30)) 使用 Cookie想在响应结果中获取 cookie 的一些值，可以直接访问。 1response.cookies['key'] # key 为 Cookie 字典中键 想发送 cookies 到服务器，可以使用cookies参数。同样该参数也是字典类型 1234567url = 'http://httpbin.org/cookies'# cookies = dict(domain='httpbin.org')cookies = { 'domain':'httpbin.org',}response = requests.get(url, cookies=cookies)print(response.text) 响应结果我们跟Python 打交道，摆脱不了编码的问题。使用 Requests 请求，我们无需担心编码问题。感觉 Requests 真的是太人性化了。请求发出后，Requests 会基于 HTTP 头部对响应的编码作出有根据的推测。当你访问 response .text 之时，Requests 会使用其推测的文本编码。 12response = requests.get(url)print(response.text) 如果你想改变 response 的编码格式，可以这么做： 1response.encoding = 'UTF-8' 二进制响应内容对于非文本请求， 我们能以字节的方式访问请求响应体。Requests 会自动为我们解码 gzip 和 deflate 传输编码的响应数据。例如，以请求返回的二进制数据创建一张图片，你可以使用如下代码： 1234from PIL import Imagefrom io import BytesIOi = Image.open(BytesIO(response.content)) JSON 响应内容Requests 中也有一个内置的 JSON 解码器，助我们处理 JSON 数据： 12345import requestsurl = 'https://github.com/timeline.json'response = requests.get(url)print(response.json()) 如果 JSON 解码失败， response .json() 就会抛出一个异常。例如，响应内容是 401 (Unauthorized)，尝试访问 response .json() 将会抛出 ValueError: No JSON object could be decoded 异常。 响应状态码我们需要根据响应码来判断请求的结果，具体是这样获取状态码： 1response.status_code Requests 内部提供了一个状态表，如果有需要对状态码进行判断，可以看下requests.codes的源码。 高级用法重定向与请求历史有些页面会做一些重定向的处理。Requests 又发挥人性化的特性。它在默认情况下，会帮我们自动处理所有重定向，包括 301 和 302 两种状态码。我们可以使用response .history来追踪重定向。 Response.history是一个 Response 对象的列表，为了完成请求而创建了这些对象。这个对象列表按照从最老到最近的请求进行排序。 如果我们要禁用重定向处理，可以使用allow_redirects参数： 1response = requests.get(url, allow_redirects=False) 会话Requests 支持 session 来跟踪用户的连接。例如我们要来跨请求保持一些 cookie，我们可以这么做： 1234567s = requests.Session()s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')r = s.get(\"http://httpbin.org/cookies\")print(r.text)# '{\"cookies\": {\"sessioncookie\": \"123456789\"}}' 身份认证有些 web 站点都需要身份认证成功之后才能访问。urllib 具备这样的功能，Requests 也不例外。Requests 支持基本身份认证（HTTP Basic Auth）、netrc 认证、摘要式身份认证、OAuth 1 认证等。 基本身份认证 许多要求身份认证的web服务都接受 HTTP Basic Auth。这是最简单的一种身份认证，并且 Requests 对这种认证方式的支持是直接开箱即可用。HTTP Basic Auth 用法如下： 123456from requests.auth import HTTPBasicAuthurl = 'http://httpbin.org/basic-auth/user/passwd'requests.get(url, auth=HTTPBasicAuth('user', 'pass'))# 简写的使用方式requests.get(url, auth=('user', 'pass')) 摘要式身份认证 摘要式是 HTTP 1.1 必需的第二种身份验证机制。这种身份验证由用户名和密码组成。随后将用 MD5（一种单向哈希算法）对摘要式身份验证进行哈希运算，并将其发送到服务器。具体用法如下： 1234from requests.auth import HTTPDigestAuthurl = 'http://httpbin.org/digest-auth/auth/user/passwd/MD5'requests.get(url, auth=HTTPDigestAuth('user', 'pass')) OAuth 认证 OAuth（开放授权）认证在我们的生活中随处可见。Requests 同样也支持这中认证方式，其中包括 OAuth 1.0 和 OAuth 2.0。如果你需要用到该认证，你需要安装一个支持库requests-oauthlib。我以 OAuth 1.0 认证作为例子进行讲解： 123456789import requestsfrom requests_oauthlib import OAuth1url = 'https://api.twitter.com/1.1/account/verify_credentials.json'auth = OAuth1('YOUR_APP_KEY', 'YOUR_APP_SECRET', 'USER_OAUTH_TOKEN', 'USER_OAUTH_TOKEN_SECRET')requests.get(url, auth=auth)","link":"/711.html"}],"tags":[{"name":"HTTP","slug":"HTTP","link":"/tags/HTTP/"},{"name":"思念","slug":"思念","link":"/tags/%E6%80%9D%E5%BF%B5/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"鸡汤","slug":"鸡汤","link":"/tags/%E9%B8%A1%E6%B1%A4/"},{"name":"网络爬虫","slug":"网络爬虫","link":"/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"name":"正则表达式","slug":"正则表达式","link":"/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"beautifulSoup","slug":"beautifulSoup","link":"/tags/beautifulSoup/"},{"name":"爬虫实战","slug":"爬虫实战","link":"/tags/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"多进程","slug":"多进程","link":"/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"name":"Requests","slug":"Requests","link":"/tags/Requests/"}],"categories":[{"name":"计算机网络","slug":"计算机网络","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"Python必知必会","slug":"Python必知必会","link":"/categories/Python%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/"},{"name":"思维与认知","slug":"思维与认知","link":"/categories/%E6%80%9D%E7%BB%B4%E4%B8%8E%E8%AE%A4%E7%9F%A5/"},{"name":"网络爬虫","slug":"网络爬虫","link":"/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"}]}