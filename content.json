{"pages":[],"posts":[{"title":"深入理解HTTP","text":"HTTP是什么 HTTP全称是HyperText Transfer Protocal，即：超文本传输协议。它主要规定了客户端和服务器之间的通信格式。HTTP还是一个基于请求/响应模式的、无状态的协议；即我们通常所说的Request/Response。 HTTP与TCP的关系TCP协议是位于TCP/IP参考模型中的网络互连层，而HTTP协议属于应用层。因此，HTTP协议是基于TCP协议。 HTTP请求(HTTP Request)HTTP请求由三部分组成，分别是： 请求行 HTTP头 请求体 下面是请求示例： 123456789GET /?tn=90058352_hao_pg HTTP/1.1Host: www.hao123.comConnection: keep-aliveCache-Control: max-age=0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Upgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/5.7.16400.16 Safari/537.36Accept-Encoding: gzip, deflateAccept-Language: zh-CN,zh;q=0.8 请求行同样也是由请求方法（POST/GET）方式、请求的主机、协议版本号三部分组成。下面为请求行的示例：GET /?tn=90058352_hao_pg HTTP/1.1 HTTP头HTTP头又细分为请求头(request header)、普通头(general header)、实体头(entity header)而HTTP头主要关注点是其字段 Accept作用: 浏览器可以接受的媒体类型例如： Accept: text/html 代表浏览器可以接受服务器回发的类型为 text/html 也就是我们常说的html文档通配符 * 代表任意类型例如： Accept: */* 代表浏览器可以处理所有类型,(一般浏览器发给服务器都是发这个) Accept-Language作用： 浏览器申明自己接收的语言。语言跟字符集的区别：中文是语言，中文有多种字符集，比如big5，gb2312，gbk等等；例如： Accept-Language: zh-CN,zh Accept-Encoding作用： 浏览器申明自己接收的编码方法，通常指定压缩方法（gzip，deflate）例如：Accept-Encoding: gzip, Accept-Encoding: deflate User-Agent作用： 告诉HTTP服务器， 客户端使用的操作系统的名称和版本以及浏览器的名称和版本.例如： User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/5.7.16400.16 Safari/537.36 Content-Type作用： 告诉服务器，请求的内容的类型常见的字段： 假设使用POST方式请求 text/xml [请求体为文本] application/json [请求体为JSON数据] application/xml [请求体为xml数据] image/jpeg [请求体为jpeg图片] multipart/form-data [请求体为表单] Cookie作用： 最重要的header，将cookie的值发送给HTTP服务器 Connection例如： Connection: keep-alive 当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接例如： Connection: close 代表一个Request完成后，客户端和服务器之间用于传输HTTP数据的TCP连接会关闭， 当客户端再次发送Request，需要重新建立TCP连接。 Content-Length作用：发送给HTTP服务器数据的长度。例如： Content-Length: 18 Referer:作用： 提供了Request的上下文信息的服务器，告诉服务器我是从哪个链接过来的。 请求体这个只有post方式请求才有，get方式请求没有。 HTTP响应(HTTP Response)HTTP Response的结构跟Request的结构基本一样。同样分为三部分： 响应行 响应头 响应体 下面是响应示例： 1234567891011HTTP/1.1 200 OKCache-Control: max-age=0Content-Encoding: gzipContent-Length: 156474Content-Type: text/html;charset=UTF-8Cxy_all: 90058352_hao_pg+d4fa7f28cefb9b120f868558e440bafaDate: Sun, 20 Nov 2016 05:09:51 GMTExpires: Sun, 20 Nov 2016 05:09:51 GMTLfy: nj02.11Server: BWS/1.0Set-Cookie: __bsi=11619936655404239050_00_60_N_R_126_0303_c02f_Y; max-age=3600; domain=www.hao123.com; path=/ 响应行响应行由协议版本、响应状态构成下面为响应行的示例：HTTP/1.1 200 OK 响应头响应头关注点是字段，常见的字段如下： Cache-Control作用: 非常重要的规则。 这个用来指定Response-Request遵循的缓存机制。例如：Cache-Control:Public 可以被任何缓存所缓存Cache-Control:Private 内容只缓存到私有缓存中Cache-Control:no-cache 所有内容都不会被缓存 Content-Type作用：服务器告诉浏览器，自己响应的对象的类型和字符集例如:Content-Type: text/html; charset=utf-8Content-Type: image/jpeg Expires作用: 浏览器会在指定过期时间内使用本地缓存例如: Expires:Sun, 20 Nov 2016 05:09:51 GMT Connection跟HTTP头中的Connection是同样的原理 Content-Encoding跟HTTP中头的Content-Encoding是同样的原理 Content-Length作用：指明实体正文的长度，以字节方式存储的十进制数字来表示。例如: Content-Length: 156474 Date作用: 生成消息的具体时间和日期例如: Date: Sun, 20 Nov 2016 05:09:51 GMT 响应体响应体包含的内容是网页的内容信息，主要是html代码等","link":"/111.html"},{"title":"常用Python标准库","text":"众所周知，Python有庞大的库资源，有官方标准库以及第三方的扩展库。每个库都一把利器，能帮助我们快速处理某方面的问题。作为一名python的初学者，当把基本的语法、列表和元组、字典、迭代器、异常处理、I/O操作、抽象等知识点学完之后。我建议把官方常用的标准库也随便学下来。讲真的，你知道这些库之后，你会有种相见恨晚的感觉。 接下来带大家走进python标准库的世界。PS： 使用Python的版本为Python3 字符串 re: 正则表达式。用来判断字符串是否是你指定的特定字符串。在爬虫项目中，经常能捕获到它的身影。 StringIO: 提供以文件为保存形式来读和写字符串。还有个性能更加好的cStringIO版本 struct: 以二进制字节序列来解释字符串。可以通过格式化参数，指定类型、长度、字节序（大小端）、内存对齐等。 数据类型 bisect: 数组二分算法。提供支持按顺序对列表进行排序，而不必每次在列表中插入后再去排序。 heapq: 堆队列算法。最小堆：完全平衡二叉树， 所有节点都小于字节点。 datetime: 提供操作日期和时间的类。其中有两种日期和时间类型： naive和aware collections: 高性能容器数据类型。实现了Python的通用内置容器、字典、列表、集合，和元组专门的数据类型提供替代品 pprint: 提供”整洁低打印”任意Python数据结构的能力。 数学运算 random: 各种分布的伪随机数的生成器 math: 数学函数。提供了由C标准的数学函数访问。该库的函数不适用于复数。 cmath: 为复数提供的数学函数。 operator: 提供了重载操作符 文件和目录 os.path: 常用路径名操作。提供了操作路径名的常用的函数。 filecmp: 文件和目录的比较。提供了比较文件和目录的函数。 shutil: 高级的文件操作。提供了许多文件和文件集上的操作操作。尤其是提供支持文件复制和删除的函数。 数据存储 serialization: Python专用的序列化算法，通常不建议用来存储自定义数据。 pickle: Python对象序列化。提供了一个基本但功能强大的Python对象序列化和反序列化算法。 cPickle: 比pickle快1000倍的对象序列化库， 和pickle可互相替换。 shevle: 将对象pickle序列化，然后保存到anydbm格式文件。anydbm是KV结构的数据库，可以保存多个序列化对象。 sqlite3: SQLite数据库DB-API 2.0接口。 数据压缩 zipfile: 提供了ZIP文件个创建、读取、写入、最佳和列出zip文件的函数。 tarfile: 提供了tar文件的压缩和解压的函数。 文件格式 csv: 提供对CSV文件的读取和写入的函数。 加密 hashlib: 安全哈希和消息摘要。实现了一个通用的接口来实现多个不同的安全哈希和消息摘要算法。包括 FIPS 安全哈希算法 SHA1、SHA224、SHA256、SHA384和 SHA512（定义在 FIPS 180-2），以及 RSA 的 MD5 算法（在互联网 RFC 1321中定义)。 hmac: 用于消息认证的加密哈希算法。实现了RFC 2104 中描述的HMAC 算法。 md5: 实现了MD5加密算法。 sha: 实现了sha1加密算法。 操作系统 time: 时间获取和转换。提供了各种与时间相关的函数。 argparse: 命令行选项、参数和子命令的解析器。使用该库使得编码用户友好的命令行接口非常容易。取代了之前的optparse io: 提供接口处理IO流。 logging: Python的日志工具。提供了日志记录的API。 logging.config: Python日志配置。用于配置日志模块的API。 os: 提供丰富的雨MAC，NT，Posix等操作系统进行交互的能力。这个模块允许程序独立的于操作系统环境。文件系统，用户数据库和权限进行交互。 _thread: 多线程控制。提供了一个底层、原始的操作 —— 多个控制线程共享全局数据空间。 threading: 高级线程接口。是基于_thread模块的，但是比_thread更加容易使用、更高层次的线程API。 sys: 提供访问和维护python解释器的能力。这包括了提示信息，版本，整数的最大值，可用模块，路径钩子，标准错误，标准输入输出的定位和解释器调用的命令行参数。 进程通信 subprocess: 管理子进程。允许用户产生新的进程，然后连接他们的输入/输出/错误/管道，并获取返回值。 socket: 底层网络接口。 signal: 设置异步时间处理handlers。信号是软中断，提供了一种异步事件通知机制。 网络数据处理 json: JSON格式的编码器和解码器。 base64: 提供依据RFC 3548的规定（Base16, Base32, Base64 ）进行数据编码和解码。 htmllib: 提供了一个HTML语法解析器。 mimetypes: 提供了判断给定的URL的MIME类型。 操作因特网网络协议 urllib: 提供了用于获取万维网数据的高层接口。这个是Python2.7版本的，Python3已经将其拆分成多个模块urllib.request，urllib.parse和urllib.error。 urlparse: 提供了用于处理URL的函数，可以在URL和平台特定的文件名间相互转换。 http.client: HTTP协议客户端。 telnetlib: 提供了实现Telnet协议的Telnet类。 poplib: POP3协议客户端。 ftplib: FTP协议客户端。 smtplib: SMTP协议客户端。 webbrowser: 提供控制浏览器行为的函数。","link":"/123.html"},{"title":"每逢佳节倍思亲","text":"回忆童年每逢冬至来临外面寒风凛凛屋内热气腾腾全家人围在一起吃汤圆团团圆圆 如今身为游子在外漂流更思念家更思念父母亲只能发QQ说说、微博、朋友圈给远方家人送上祝福？不！没有比给家人打一电话来得更加直接来的更加亲切拿起你手中的手机给家里打一通电话为爸妈送上节日祝福！祝：冬至快乐~","link":"/124.html"},{"title":"你为何要那么拼命？","text":"很多人往往有这样的状态，当完成一个目标之后，就守着这收获的成果沾沾自喜。你觉得考上了大学，就可以整天逃课沉迷于游戏？你觉得找到工作了，就可以准时下班走人，天天潇潇洒洒？答案是否定的。如果你是现在身处这状态，说明你对自己未来人生没有什么规划，是对自己极其不负责任的表示。你试问你自己，是否有在为自己拼命？ 01.何炅, 这个名字已经家喻户晓了。大家都知道他是大名鼎鼎的湖南电视台主持人。平时我们都在享受何老师给我们带来快乐，可知背后辛酸的汗水呢？何老师在读大学三年级时，每天需要应对高难度的阿拉伯语的学习，还担任着学生会的工作，兼职文艺部和宣传部的“要职”。除此之外，他还在央视担任支持，平日要撰写台本以及录影，有时还要出差去外地录制。每天他都很晚才回到学校，同学们可能已经下了晚自习，甚至都已经入睡了。而他只能先在学生会里将自己学生干部的事情都做完后，再回到宿舍开始预习第二天上课要准备的内容。 02.彭宇年轻的时还是街头一个小混混。当他树立人生中第一个梦想————进入电视台工作，生活从此跟之前是天壤之别。他时常对着电视机练习如何应对突发，还报名参加骗子的演员培训班。到了后来，他听说北京机会多，毅然决定北漂，每天在北京电影学院门口等着接活，做群众演员。 为何现在要拼命？只为自己，只为自己生活得更好，只为青春无悔。 你是否感到很震撼？看下你目前的生活状态，如果整天这么懒散。那么你该制定短期目标，并为之拼命一把。","link":"/15.html"},{"title":"学爬虫之道","text":"近来在阅读 《轻量级 Django》,虽然还没有读完，但我已经收益颇多。我不得不称赞 Django 框架的开发人员，他们把 Web 开发降低门槛。Django 让我从对 Web 开发是一无所知到现在可以编写小型 web 应用，这很舒服。 Django 已经算是入门，所以自己把学习目标转到爬虫。自己接下来会利用三个月的时间来专攻 Python 爬虫。这几天，我使用“主题阅读方法”阅读 Python 爬虫入门的文档。制定 Python 爬虫的学习路线。 第一阶段：夯实入门要就是在打基础，所以要从最基础的库学起。下面是几个库是入门最经典的库 urllib它属于 Python 标准库。该库的作用是请求网页并下载数据。在学习该库之前，最好把 HTTP 协议了解下。这会大大提高后面的学习效率。 先学会如何使用 urllib 请求到数据，再学习一些高级用法。例如： 设置 Headers: 某些网站反感爬虫的到访，于是对爬虫一律拒绝请求。设置 Headers 可以把请求伪装成浏览器访问网站。 Proxy 的设置: 某些站点做了反倒链的设置，会将高频繁访问的 IP 地址封掉。所以我们需要用到代理池。 错误解析：根据 URLError 与 HTTPError 返回的错误码进行解析。 Cookie 的使用：可以模拟网站登录，需要结合 cookielib 一起使用。 rere 是正则表达式库。同时也是 Python 标准库之一。它的作用是匹配我们需要爬取的内容。所以我们需要掌握正则表达式常用符号以及常用方法的用法。 BeautifulSoupBeautifulSoup 是解析网页的一款神器。它可以从 HTML 或者 XML 文件中提取数据。配合 urllib 可以编写出各种小巧精干的爬虫脚本。 第二阶段：进阶当把基础打牢固之后，我们需要更进一步学习。使用更加完善的库来提高爬取效率 使用多线程使用多线程抓取数据，提高爬取数据效率。 学习 RequestsRequests 作为 urlilb 的替代品。它是更加人性化、更加成熟的第三方库。使用 Requests 来处理各种类型的请求，重复抓取问题、cookies 跟随问题、多线程多进程、多节点抓取、抓取调度、资源压缩等一系列问题。 学习 XpathXpath 也算是一款神器。它是一款高效的、表达清晰简单的分析语言。掌握它以后介意弃用正则表达式了。一般是使用浏览器的开发者工具 加 lxml 库。 学习 Selenium使用 Selenium，模拟浏览器提交类似用户的操作，处理js动态产生的网页。因为一些网站的数据是动态加载的。类似这样的网站，当你使用鼠标往下滚动时，会自动加载新的网站。 第三阶段：突破学习 ScrapyScrapy 是一个功能非常强大的分布式爬虫框架。我们学会它，就可以不用重复造轮子。 数据存储如果爬取的数据条数较多，我们可以考虑将其存储到数据库中。因此，我们需要学会 MySqlMongoDB、SqlLite的用法。更加深入的，可以学习数据库的查询优化。 第四阶段：为我所用当爬虫完成工作，我们已经拿到数据。我们可以利用这些数据做数据分析、数据可视化、做创业项目原始启动数据等。我们可以学习 NumPy、Pandas、 Matplotlib 这三个库。 NumPy ：它是高性能科学计算和数据分析的基础包。 Pandas : 基于 NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。它可以算得上作弊工具。 Matplotlib：Python中最著名的绘图系统Python中最著名的绘图系统。它可以制作出散点图，折线图，条形图，直方图，饼状图，箱形图散点图，折线图，条形图，直方图，饼状图，箱形图等。","link":"/62.html"},{"title":"详解 python3 urllib","text":"本文是爬虫系列文章的第一篇，主要讲解 Python 3 中的 urllib 库的用法。urllib 是 Python 标准库中用于网络请求的库。该库有四个模块，分别是urllib.request，urllib.error，urllib.parse，urllib.robotparser。其中urllib.request，urllib.error两个库在爬虫程序中应用比较频繁。那我们就开门见山，直接讲解这两个模块的用法。 发起请求模拟浏览器发起一个 HTTP 请求，我们需要用到 urllib.request 模块。urllib.request 的作用不仅仅是发起请求， 还能获取请求返回结果。发起请求，单靠 urlopen() 方法就可以叱咤风云。我们先看下 urlopen() 的 API 1urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) 第一个参数 String 类型的地址或者 data 是 bytes 类型的内容，可通过 bytes()函数转为化字节流。它也是可选参数。使用 data 参数，请求方式变成以 POST 方式提交表单。使用标准格式是application/x-www-form-urlencoded timeout 参数是用于设置请求超时时间。单位是秒。 cafile和capath代表 CA 证书和 CA 证书的路径。如果使用HTTPS则需要用到。 context参数必须是ssl.SSLContext类型，用来指定SSL设置 cadefault参数已经被弃用，可以不用管了。 该方法也可以单独传入urllib.request.Request对象 该函数返回结果是一个http.client.HTTPResponse对象。 简单抓取网页我们使用 urllib.request.urlopen() 去请求百度贴吧，并获取到它页面的源代码。 123456import urllib.requesturl = \"http://tieba.baidu.com\"response = urllib.request.urlopen(url)html = response.read() # 获取到页面的源代码print(html.decode('utf-8')) # 转化为 utf-8 编码 设置请求超时有些请求可能因为网络原因无法得到响应。因此，我们可以手动设置超时时间。当请求超时，我们可以采取进一步措施，例如选择直接丢弃该请求或者再请求一次。 12345import urllib.requesturl = \"http://tieba.baidu.com\"response = urllib.request.urlopen(url, timeout=1)print(response.read().decode('utf-8')) 使用 data 参数提交数据在请求某些网页时需要携带一些数据，我们就需要使用到 data 参数。 123456789101112import urilib.parseimport urllib.requesturl = \"http://127.0.0.1:8000/book\"params = { 'name':'浮生六记', 'author':'沈复'}data = bytes(urllib.parse.urlencode(params), encoding='utf8')response = urllib.request.urlopen(url, data=data)print(response.read().decode('utf-8')) params 需要被转码成字节流。而 params 是一个字典。我们需要使用 urllib.parse.urlencode() 将字典转化为字符串。再使用 bytes() 转为字节流。最后使用 urlopen() 发起请求，请求是模拟用 POST 方式提交表单数据。 使用 Request由上我们知道利用 urlopen() 方法可以发起简单的请求。但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入headers（请求头）、指定请求方式等信息，我们就可以利用更强大的Request类来构建一个请求。按照国际惯例，先看下 Request 的构造方法： 1urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None) url 参数是请求链接，这个是必传参数，其他的都是可选参数。 data 参数跟 urlopen() 中的 data 参数用法相同。 headers 参数是指定发起的 HTTP 请求的头部信息。headers 是一个字典。它除了在 Request 中添加，还可以通过调用 Reques t实例的 add_header() 方法来添加请求头。 origin_req_host 参数指的是请求方的 host 名称或者 IP 地址。 unverifiable 参数表示这个请求是否是无法验证的，默认值是False。意思就是说用户没有足够权限来选择接收这个请求的结果。例如我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，我们就要将 unverifiable 的值设置成 True。 method 参数指的是发起的 HTTP 请求的方式，有 GET、POST、DELETE、PUT等 简单使用 Request使用 Request 伪装成浏览器发起 HTTP 请求。如果不设置 headers 中的 User-Agent，默认的User-Agent是Python-urllib/3.5。可能一些网站会将该请求拦截，所以需要伪装成浏览器发起请求。我使用的 User-Agent 是 Chrome 浏览器。 123456789import urllib.requesturl = \"http://tieba.baidu.com/\"headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}request = urllib.request.Request(url=url, headers=headers)response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) Request 高级用法如果我们需要在请求中添加代理、处理请求的 Cookies，我们需要用到Handler和OpenerDirector。 1） HandlerHandler 的中文意思是处理者、处理器。 Handler 能处理请求（HTTP、HTTPS、FTP等）中的各种事情。它的具体实现是这个类 urllib.request.BaseHandler。它是所有的 Handler 的基类，其提供了最基本的Handler的方法，例如default_open()、protocol_request()等。继承 BaseHandler 有很多个，我就列举几个比较常见的类： ProxyHandler：为请求设置代理 HTTPCookieProcessor：处理 HTTP 请求中的 Cookies HTTPDefaultErrorHandler：处理 HTTP 响应错误。 HTTPRedirectHandler：处理 HTTP 重定向。 HTTPPasswordMgr：用于管理密码，它维护了用户名密码的表。 HTTPBasicAuthHandler：用于登录认证，一般和 HTTPPasswordMgr 结合使用。 2） OpenerDirector对于 OpenerDirector，我们可以称之为 Opener。我们之前用过 urlopen() 这个方法，实际上它就是 urllib 为我们提供的一个Opener。那 Opener 和 Handler 又有什么关系？opener 对象是由 build_opener(handler) 方法来创建出来 。我们需要创建自定义的 opener，就需要使用 install_opener(opener)方法。值得注意的是，install_opener 实例化会得到一个全局的 OpenerDirector 对象。 使用代理我们已经了解了 opener 和 handler，接下来我们就通过示例来深入学习。第一个例子是为 HTTP 请求设置代理有些网站做了浏览频率限制。如果我们请求该网站频率过高。该网站会被封 IP，禁止我们的访问。所以我们需要使用代理来突破这“枷锁”。 1234567891011121314151617import urllib.requesturl = \"http://tieba.baidu.com/\"headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}proxy_handler = urllib.request.ProxyHandler({ 'http': 'web-proxy.oa.com:8080', 'https': 'web-proxy.oa.com:8080'})opener = urllib.request.build_opener(proxy_handler)urllib.request.install_opener(opener)request = urllib.request.Request(url=url, headers=headers)response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) 认证登录有些网站需要携带账号和密码进行登录之后才能继续浏览网页。碰到这样的网站，我们需要用到认证登录。我们首先需要使用 HTTPPasswordMgrWithDefaultRealm() 实例化一个账号密码管理对象；然后使用 add_password() 函数添加账号和密码；接着使用 HTTPBasicAuthHandler() 得到 hander；再使用 build_opener() 获取 opener 对象；最后使用 opener 的 open() 函数发起请求。 第二个例子是携带账号和密码请求登录百度贴吧，代码如下： 123456789101112import urllib.requesturl = \"http://tieba.baidu.com/\"user = 'user'password = 'password'pwdmgr = urllib.request.HTTPPasswordMgrWithDefaultRealm()pwdmgr.add_password(None，url ，user ，password)auth_handler = urllib.request.HTTPBasicAuthHandler(pwdmgr)opener = urllib.request.build_opener(auth_handler)response = opener.open(url)print(response.read().decode('utf-8')) Cookies设置如果请求的页面每次需要身份验证，我们可以使用 Cookies 来自动登录，免去重复登录验证的操作。获取 Cookies 需要使用 http.cookiejar.CookieJar() 实例化一个 Cookies 对象。再用 urllib.request.HTTPCookieProcessor 构建出 handler 对象。最后使用 opener 的 open() 函数即可。 第三个例子是获取请求百度贴吧的 Cookies 并保存到文件中，代码如下： 123456789101112131415import http.cookiejarimport urllib.requesturl = \"http://tieba.baidu.com/\"fileName = 'cookie.txt'cookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open(url)f = open(fileName,'a')for item in cookie: f.write(item.name+\" = \"+item.value+'\\n')f.close() HTTPResponse从上面的例子可知， 使用 urllib.request.urlopen() 或者 opener.open(url) 返回结果是一个 http.client.HTTPResponse 对象。它具有 msg、version、status、reason、debuglevel、closed等属性以及read()、readinto()、getheader(name)、getheaders()、fileno()等函数。 错误解析发起请求难免会出现各种异常，我们需要对异常进行处理，这样会使得程序比较人性化。异常处理主要用到两个类，urllib.error.URLError和urllib.error.HTTPError。 URLErrorURLError 是 urllib.error 异常类的基类, 可以捕获由urllib.request 产生的异常。它具有一个属性reason，即返回错误的原因。 捕获 URL 异常的示例代码： 12345678import urllib.requestimport urllib.errorurl = \"http://www.google.com\"try: response = request.urlopen(url)except error.URLError as e: print(e.reason) HTTPError HTTPError 是 UEKRrror 的子类，专门处理 HTTP 和 HTTPS 请求的错误。它具有三个属性。 1)code：HTTP 请求返回的状态码。 1)renson：与父类用法一样，表示返回错误的原因。 1)headers`：HTTP 请求返回的响应头信息。 获取 HTTP 异常的示例代码, 输出了错误状态码、错误原因、服务器响应头 12345678910import urllib.requestimport urllib.errorurl = \"http://www.google.com\"try: response = request.urlopen(url)except error.HTTPError as e: print('code: ' + e.code + '\\n') print('reason: ' + e.reason + '\\n') print('headers: ' + e.headers + '\\n')","link":"/66.html"},{"title":"Python 正则表达式","text":"我们能够使用 urllib 向网页请求并获取其网页数据。但是抓取信息数据量比较大，我们可能需要其中一小部分数据。对付刚才的难题，就需要正则表达式出马了。正则表达式能帮助我们匹配过滤到我们需要的数据，但它学习起来非常枯燥无味。你可能会说，我还没有开始想学习正则表达式，你就来打击我？ 莫慌！层层递进地学习，一步一个脚印地学习，很快就会学会了。对于爬虫，我觉得学会最基本的符号就差不多了。 正则表达式下面是一张关于正则表达式字符的图，图片资料来自CSDN。先把图中字符了解清楚，基本上算是入门。 re 模块Python 自 1.5 版本起通过新增 re （Regular Expression 正则表达式）模块来提供对正则表达式的支持。使用 re 模块先将正则表达式填充到 Pattern 对象中，再把 Pattern 对象作为参数使用 match 方法去匹配的字符串文本。match 方法会返回一个 Match 对象，再通过 Match 对象会得到我们的信息并进行操作。下面介绍几个 re 常用的函数。 compile 函数compile 是把正则表达式的模式和标识转化成正则表达式对象，供 match() 和 search() 这两个函数使用。它的函数语法如下： 1re.compile(pattern[, flags]) 第一个参数是pattern，指的正则表达式。 第二个参数flags是匹配模式，是个可选参数。可以使用按位或’|’表示同时生效，也可以在正则表达式字符串中指定。匹配模式有以下几种： flag 描述 re.I(全拼：IGNORECASE) 忽略大小写（括号内是完整写法，下同） re.M(全拼：MULTILINE) 多行模式，改变’^’和’$’的行为（参见上图） re.S(全拼：DOTALL) 点任意匹配模式，改变’.’的行为 re.L(全拼：LOCALE) 使预定字符类 \\w \\W \\b \\B \\s \\S 取决于当前区域设定 re.U(全拼：UNICODE) 使预定字符类 \\w \\W \\b \\B \\s \\S \\d \\D 取决于unicode定义的字符属性 re.X(全拼：VERBOSE) 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。 该方法返回的结果是一个 Pattern 对象。 match 函数match()函数只在字符串的开始位置尝试匹配正则表达式，也就是说只有在 0 位置匹配成功的话才有返回。如果不是开始位置匹配成功的话，match() 就返回 none。它的函数语法如下： 1re.match(pattern, string[, flags]) 第一个参数：匹配的正则表达式 第二个参数：要被匹配的字符串 flags 是可选参数，跟 compile 用法相似 匹配成功 re.match 方法返回一个匹配的对象，否则返回None。要想获得匹配结果，既可以使用groups()函数获取一个包含所有字符串的元组（从 1 到 所含的小组号），也可以使用group(组号)函数获取某个组号的字符串。 match 函数用法的示例代码： 123456789import repattern = re.compile('Python')text = 'Python python pythonn'match = re.search(pattern, text)if match: print(match.group())else: print('没有匹配') search 函数 search() 函数是扫描整个字符串来查找匹配，它返回结果是第一个成功匹配的字符串。 1re.search(pattern, string[, flags]) 参数用法以及返回结果跟match函数用法相同。 search 函数用法的示例代码： 123456789import repattern = re.compile('Python')text = 'welcome to Python world!'match = re.search(pattern, text)if match: print(match.group())else: print('没有匹配') findall 函数findall函数在字符串中搜索子串，并以列表形式返回全部能匹配的所有子串。 1re.findall(pattern, string[, flags]) 参数用法以及返回结果跟match函数用法相同。 findall 函数用法的示例代码： 123456789import repattern = re.compile('\\d+')text = 'one1two2three3four4'list = re.findall(pattern, text)if list: print(list)else: print('没有匹配')","link":"/67.html"},{"title":"内容提取神器 beautifulSoup 的用法","text":"上篇文章只是简单讲述正则表达式如何读懂以及 re 常见的函数的用法。我们可能读懂别人的正则表达式，但是要自己写起正则表达式的话，可能会陷入如何写的困境。正则表达式写起来费劲又出错率高，那么有没有替代方案呢？俗话说得好，条条道路通罗马。目前还两种代替其的办法，一种是使用 Xpath 神器，另一种就是本文要讲的 BeautifulSoup。 BeautifulSoup 简介引用 BeautifulSoup 官网的说明： Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work. 大致意思如下: BeautifulSoup 是一个能从 HTML 或 XML 文件中提取数据的 Python 库。它能通过自己定义的解析器来提供导航、搜索，甚至改变解析树。它的出现，会大大节省开发者的时间。 安装 BeautifulSoup目前 BeautifulSoup 最新版本是 4.6.0，它是支持 Python3的。所以可以大胆去升级安装使用。 安装方法有两种： 使用pip比较推荐使用这种方式，既简单又方便管理。 1234pip install beautifulsoup4# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install beautifulsoup4 使用easy_install 1easy_install beautifulsoup4 使用系统包管理 12sudo apt-get install Python-bs4# 适用于 ubuntu 系统以及 Debian 系统 初始 BeautifulSoup首先导入 BeautifulSoup 库，然后创建一个 BeautifulSoup 对象，再利用对象做文章。具体参考示例代码： 1234from bs4 import BeautifulSoupsoup = BeautifulSoup(response)print(soup.prettify()) 上面代码中，response 可以urlllib或者request请求返回的内容，也可以是本地 HTML 文本。如果要打开本地，代码需要改为 12soup = BeautifulSoup(open(\"index.html\"))# 打开当前目录下 index.html 文件 soup.prettify()函数的作用是打印整个 html 文件的 dom 树，例如上面执行结果如下： 123456789101112&lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse's story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt; &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt; &lt;/body&gt;&lt;/html&gt; 解析 BeautifulSoup 对象想从 html 中获取到自己所想要的内容，我归纳出三种办法： 利用 Tag 对象从上文得知，BeautifulSoup 将复杂 HTML 文档转换成一个复杂的树形结构,每个节点都是Python对象。跟安卓中的Gson库有异曲同工之妙。节点对象可以分为 4 种：Tag, NavigableString, BeautifulSoup, Comment。 Tag 对象可以看成 HTML 中的标签。这样说，你大概明白具体是怎么回事。我们再通过例子来更加深入了解 Tag 对象。以下代码是以 prettify() 打印的结果为前提。 例子1 获取head标签内容 123print(soup.head)# 输出结果如下：&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt; 例子2 获取title标签内容 123print(soup.title)# 输出结果如下：&lt;title&gt;The Dormouse's story&lt;/title&gt; 例子3 获取p标签内容 123print(soup.p)# 输出结果如下：&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; 如果 Tag 对象要获取的标签有多个的话，它只会返回所以内容中第一个符合要求的标签。 对象一般含有属性，Tag 对象也不例外。它具有两个非常重要的属性， name 和 attrs。 namename 属性是 Tag 对象的标签名。不过也有特殊的，soup 对象的 name 是 [document] 12345print(soup.name)print(soup.head.name)# 输出结果如下：[document]head attrsattrs 属性是 Tag 对象所包含的属性值，它是一个字典类型。 123print(soup.p.attrs）# 输出结果如下：{'class': ['title'], 'name': 'dromouse'} 其他三个属性也顺带介绍下: NavigableString 说白了就是：Tag 对象里面的内容 123print(soup.title.string) # 输出结果如下：The Dormouse's story BeautifulSoup BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象。它是一个特殊的 Tag。 1234567print(type(soup.name))print(soup.name)print(soup.attrs)# 输出结果如下：&lt;type 'unicode'&gt;[document]{} 空字典 Comment Comment 对象是一个特殊类型的 NavigableString 对象。如果 HTML 页面中含有注释及特殊字符串的内容。而那些内容不是我们想要的，所以我们在使用前最好做下类型判断。例如： 12if type(soup.a.string) == bs4.element.Comment: ... # 执行其他操作，例如打印内容 利用过滤器过滤器其实是一个find_all()函数， 它会将所有符合条件的内容以列表形式返回。它的构造方法如下： 1find_all(name, attrs, recursive, text, **kwargs ) name 参数可以有多种写法： （1）节点名 123print(soup.find_all('p'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] （2）正则表达式 123print(soup.find_all(re.compile('^p')))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] （3）列表如果参数为列表，过滤标准为列表中的所有元素。看下具体代码，你就会一目了然了。 123print(soup.find_all(['p', 'a']))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;, &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 另外 attrs 参数可以也作为过滤条件来获取内容，而 limit 参数是限制返回的条数。 利用 CSS 选择器以 CSS 语法为匹配标准找到 Tag。同样也是使用到一个函数，该函数为select()，返回类型也是 list。它的具体用法如下, 同样以 prettify() 打印的结果为前提： （1）通过 tag 标签查找 123print(soup.select(head))# 输出结果如下：[&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;] （2）通过 id 查找 123print(soup.select('#link1'))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] （3）通过 class 查找 123print(soup.select('.sister'))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] （4）通过属性查找 123print(soup.select('p[name=dromouse]'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;] 123print(soup.select('p[class=title]'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;] （5）组合查找 1234print(soup.select(\"body p\"))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;,&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] 123print(soup.select(\"p &gt; a\"))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 123print(soup.select(\"p &gt; .sister\"))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 处理上下关系从上文可知，我们已经能获取到节点对象，但有时候需要获取其父节点或者子节点的内容，我们要怎么做了？这就需要对parse tree进行遍历 （1）获取子节点利用.children属性，该属性会返回当前节点所以的子节点。但是它返回的类型不是列表，而是迭代器 （2）获取所有子孙节点使用.descendants属性，它会返回所有子孙节点的迭代器 （3）获取父节点通过.parent属性可以获得所有子孙节点的迭代器 （4）获取所有父节点.parents属性，也是返回所有子孙节点的迭代器 （5）获取兄弟节点兄弟节点可以理解为和本节点处在统一级的节点，.next_sibling属性获取了该节点的下一个兄弟节点，.previous_sibling则与之相反，如果节点不存在，则返回 None 注意：实际 HTML 中的 tag 的.next_sibling和 .previous_sibling属性通常是字符串或空白，因为空白或者换行也可以被视作一个节点，所以得到的结果可能是空白或者换行 （5）获取所有兄弟节点通过.next_siblings和.previous_siblings属性可以对当前节点的兄弟节点迭代输出","link":"/78.html"},{"title":"爬虫实战一：爬取当当网所有 Python 书籍","text":"我们已经学习 urllib、re、BeautifulSoup 这三个库的用法。但只是停留在理论层面上，还需实践来检验学习成果。因此，本文主要讲解如何利用我们刚才的几个库去实战。 确定爬取目标任何网站皆可爬取，就看你要不要爬取而已。本次选取的爬取目标是当当网，爬取内容是 以 Python 为关键字搜索出来的页面中所有书籍的信息。具体如下图所示： 本次爬取结果有三项： 图书的封面图片 图书的书名 图书的链接页面最后把这三项内容保存到 csv 文件中。 爬取过程总所周知，每个站点的页面 DOM 树是不一样的。所以我们需要先对爬取页面进行分析，再确定自己要获取的内容，再定义程序爬取内容的规则。 确定 URL 地址我们可以通过利用浏览器来确定URL 地址，为 urllib 发起请求提供入口地址。接下来，我们就一步步来确定请求地址。搜索结果页面为 1 时，URL 地址如下： 搜索结果页面为 3 时，URL 地址如下： 搜索结果页面为 21 时，即最后一页，URL 地址如下： 从上面的图片中，我们发现 URL 地址的差异就在于 page_index 的值，所以 URL 地址最终为 http://search.dangdang.com/?key=python&amp;act=input&amp;show=big&amp;page_index=。而 page_index 的值，我们可以通过循环依次在地址后面添加。因此， urllib 请求代码可以这样写： 123456789101112# 爬取地址, 当当所有 Python 的书籍, 一共是 21 页url = \"http://search.dangdang.com/?key=python&amp;act=input&amp;show=big&amp;page_index=\"index = 1while index &lt;= 21: # 发起请求 request = urllib.request.Request(url=url+str(index), headers=headers) response = urllib.request.urlopen(request) index = index + 1 # 解析爬取内容 parseContent(response) time.sleep(1) # 休眠1秒 确定爬取节点有了 URL 地址，就能使用 urllib 获取到页面的 html 内容。到了这步，我们就需要找到爬取的节点的规则，以便于 BeautifulSoup 地解析。为了搞定这个问题，就要祭出大招 —— Chrome 浏览器的开发者功能（按下 F12 键就能启动）。我们按下 F12 键盘，依次对每本书进行元素检查（在页面使用鼠标右键，点击“检查”即可），具体结果如下： 从上图可以得知解析规则：每本书的节点是一个 a 标签，a 标签具有 title，href，子标签 img 的 src 三个属性，这三者分别对应书名、书的链接页面、书的封图。看到这里也需你不会小激动，感叹这不就是我们要感兴趣的内容吗？得到解析规则，编写BeautifulSoup 解析代码就有了思路，具体代码如下： 1234567891011121314151617# 提取爬取内容中的 a 标签, 例如：# &lt;a# class=\"pic\" dd_name=\"单品图片\"# ddclick=\"act=normalResult_picture&amp;amp;pos=23648843_53_2_q\"# href=\"http://product.dangdang.com/23648843.html\"# name=\"itemlist-picture\"# target=\"_blank\" title=\"# 趣学Python――教孩子学编程 \"&gt;## &lt;img# alt=\" 趣学Python――教孩子学编程 \"# data-original=\"http://img3x3.ddimg.cn/20/34/23648843-1_b_0.jpg\"# src=\"images/model/guan/url_none.png\"/&gt;# &lt;/a&gt;soup = BeautifulSoup(response)books = soup.find_all('a', class_='pic')print(books) 运行结果如下： 这证明刚才制定规则是正确爬取我们所需的内容。 保存爬取信息我写爬虫程序有个习惯，就是每次都会爬取内容持久化到文件中。这样方便以后查看使用。如果爬取数据量比较大，我们可以用其做数据分析。我这里为了方便，就将数据保存到 csv 文件中。用 Python 将数据写到文件中，我们经常中文乱码问题所烦恼。如果单纯使用 csv 库，可能摆脱不了这烦恼。所以我们将 csv 和 codecs 结合一起使用。在写数据到 csv 文件的时候，我们可以通过指定文件编码。这样中文乱码问题就迎刃而解。具体代码如下: 12345678910111213141516171819fileName = 'PythonBook.csv'# 指定编码为 utf-8, 避免写 csv 文件出现中文乱码with codecs.open(fileName, 'w', 'utf-8') as csvfile: filednames = ['书名', '页面地址', '图片地址'] writer = csv.DictWriter(csvfile, fieldnames=filednames) writer.writeheader() for book in books: # print(book) # print(book.attrs) # 获取子节点&lt;img&gt; # (book.children)[0] if len(list(book.children)[0].attrs) == 3: img = list(book.children)[0].attrs['data-original'] else: img = list(book.children)[0].attrs['src'] writer.writerow({'书名': book.attrs['title'], '页面地址': book.attrs['href'], '图片地址': img}) 看到这里，你可能会问为什么不把编码指定为 gb2312 呢，这样用 ecxel 打开就不会乱码了？原因是当书名全部为英文单词时，使用 gb2312 编码，writer.writerow()会出现编码错误的问题。 如果你要用 excel 打开 PythonBook.csv文件, 你则需多执行下面几步： 1) 打开 Excel 2) 执行“数据”-&gt;“自文本” 3) 选择 CSV 文件，出现文本导入向导 4) 选择“分隔符号”，下一步 5) 勾选“逗号”，去掉“ Tab 键”，下一步，完成 6）在“导入数据”对话框里，直接点确定 爬取结果最后，我们将上面代码整合起来即可。这里就不把代码贴出来了，具体阅读原文即可查看源代码。我就把爬取结果截下图： 写在最后这次实战算是结束了，但是我们不能简单地满足，看下程序是否有优化的地方。我把该程序不足的地方写出来。 该程序是单线程，没有使用多线程，执行效率不够高。 没有应用面向对象编程思想，程序的可扩展性不高。 没有使用随机 User-Agent 和 代理，容易被封 IP。 如果你想获取网站的源码, 点击☞☞☞Github 仓库地址","link":"/79.html"},{"title":"Python 多进程与多线程","text":"前言：为什么有人说 Python 的多线程是鸡肋，不是真正意义上的多线程？ 看到这里，也许你会疑惑。这很正常，所以让我们带着问题来阅读本文章吧。 问题：1、Python 多线程为什么耗时更长？2、为什么在 Python 里面推荐使用多进程而不是多线程？ 基础知识现在的 PC 都是多核的，使用多线程能充分利用 CPU 来提供程序的执行效率。 线程线程是一个基本的 CPU 执行单元。它必须依托于进程存活。一个线程是一个execution context（执行上下文），即一个 CPU 执行时所需要的一串指令。 进程进程是指一个程序在给定数据集合上的一次执行过程，是系统进行资源分配和运行调用的独立单位。可以简单地理解为操作系统中正在执行的程序。也就说，每个应用程序都有一个自己的进程。 每一个进程启动时都会最先产生一个线程，即主线程。然后主线程会再创建其他的子线程。 两者的区别 线程必须在某个进行中执行。 一个进程可包含多个线程，其中有且只有一个主线程。 多线程共享同个地址空间、打开的文件以及其他资源。 多进程共享物理内存、磁盘、打印机以及其他资源。 线程的类型线程的因作用可以划分为不同的类型。大致可分为： 主线程 子线程 守护线程（后台线程） 前台线程 Python 多线程GIL其他语言，CPU 是多核时是支持多个线程同时执行。但在 Python 中，无论是单核还是多核，同时只能由一个线程在执行。其根源是 GIL 的存在。 GIL 的全称是 Global Interpreter Lock(全局解释器锁)，来源是 Python 设计之初的考虑，为了数据安全所做的决定。某个线程想要执行，必须先拿到 GIL，我们可以把 GIL 看作是“通行证”，并且在一个 Python 进程中，GIL 只有一个。拿不到通行证的线程，就不允许进入 CPU 执行。 而目前 Python 的解释器有多种，例如： CPython：CPython 是用C语言实现的 Python 解释器。 作为官方实现，它是最广泛使用的 Python 解释器。 PyPy：PyPy 是用RPython实现的解释器。RPython 是 Python 的子集， 具有静态类型。这个解释器的特点是即时编译，支持多重后端（C, CLI, JVM）。PyPy 旨在提高性能，同时保持最大兼容性（参考 CPython 的实现）。 Jython：Jython 是一个将 Python 代码编译成 Java 字节码的实现，运行在JVM (Java Virtual Machine) 上。另外，它可以像是用 Python 模块一样，导入 并使用任何Java类。 IronPython：IronPython 是一个针对 .NET 框架的 Python 实现。它 可以用 Python 和 .NET framewor k的库，也能将 Python 代码暴露给 .NET 框架中的其他语言。 GIL 只在 CPython 中才有，而在 PyPy 和 Jython 中是没有 GIL 的。 每次释放 GIL锁，线程进行锁竞争、切换线程，会消耗资源。这就导致打印线程执行时长，会发现耗时更长的原因。 并且由于 GIL 锁存在，Python 里一个进程永远只能同时执行一个线程(拿到 GIL 的线程才能执行)，这就是为什么在多核CPU上，Python 的多线程效率并不高的根本原因。 创建多线程Python提供两个模块进行多线程的操作，分别是thread和threading，前者是比较低级的模块，用于更底层的操作，一般应用级别的开发不常用。 方法1：直接使用threading.Thread() 1234567891011import threading# 这个函数名可随便定义def run(n): print(\"current task：\", n)if __name__ == \"__main__\": t1 = threading.Thread(target=run, args=(\"thread 1\",)) t2 = threading.Thread(target=run, args=(\"thread 2\",)) t1.start() t2.start() 方法2：继承threading.Thread来自定义线程类，重写run方法 12345678910111213141516import threadingclass MyThread(threading.Thread): def __init__(self, n): super(MyThread, self).__init__() # 重构run函数必须要写 self.n = n def run(self): print(\"current task：\", n)if __name__ == \"__main__\": t1 = MyThread(\"thread 1\") t2 = MyThread(\"thread 2\") t1.start() t2.start() 线程合并Join函数执行顺序是逐个执行每个线程，执行完毕后继续往下执行。主线程结束后，子线程还在运行，join函数使得主线程等到子线程结束时才退出。 1234567891011121314import threadingdef count(n): while n &gt; 0: n -= 1if __name__ == \"__main__\": t1 = threading.Thread(target=count, args=(\"100000\",)) t2 = threading.Thread(target=count, args=(\"100000\",)) t1.start() t2.start() # 将 t1 和 t2 加入到主线程中 t1.join() t2.join() 线程同步与互斥锁线程之间数据共享的。当多个线程对某一个共享数据进行操作时，就需要考虑到线程安全问题。threading模块中定义了Lock 类，提供了互斥锁的功能来保证多线程情况下数据的正确性。 用法的基本步骤： 123456#创建锁mutex = threading.Lock()#锁定mutex.acquire([timeout])#释放mutex.release() 其中，锁定方法acquire可以有一个超时时间的可选参数timeout。如果设定了timeout，则在超时后通过返回值可以判断是否得到了锁，从而可以进行一些其他的处理。 具体用法见示例代码： 123456789101112131415161718192021import threadingimport timenum = 0mutex = threading.Lock()class MyThread(threading.Thread): def run(self): global num time.sleep(1) if mutex.acquire(1): num = num + 1 msg = self.name + ': num value is ' + str(num) print(msg) mutex.release()if __name__ == '__main__': for i in range(5): t = MyThread() t.start() 可重入锁（递归锁）为了满足在同一线程中多次请求同一资源的需求，Python 提供了可重入锁（RLock）。RLock内部维护着一个Lock和一个counter变量，counter 记录了 acquire 的次数，从而使得资源可以被多次 require。直到一个线程所有的 acquire 都被 release，其他的线程才能获得资源。 具体用法如下： 1234567891011#创建 RLockmutex = threading.RLock()class MyThread(threading.Thread): def run(self): if mutex.acquire(1): print(\"thread \" + self.name + \" get mutex\") time.sleep(1) mutex.acquire() mutex.release() mutex.release() 守护线程如果希望主线程执行完毕之后，不管子线程是否执行完毕都随着主线程一起结束。我们可以使用setDaemon(bool)函数，它跟join函数是相反的。它的作用是设置子线程是否随主线程一起结束，必须在start() 之前调用，默认为False。 定时器如果需要规定函数在多少秒后执行某个操作，需要用到Timer类。具体用法如下： 12345678from threading import Timer def show(): print(\"Pyhton\")# 指定一秒钟之后执行 show 函数t = Timer(1, hello)t.start() Python 多进程创建多进程Python 要进行多进程操作，需要用到muiltprocessing库，其中的Process类跟threading模块的Thread类很相似。所以直接看代码熟悉多进程。 方法1：直接使用Process, 代码如下： 123456789from multiprocessing import Process def show(name): print(\"Process name is \" + name)if __name__ == \"__main__\": proc = Process(target=show, args=('subprocess',)) proc.start() proc.join() 方法2：继承Process来自定义进程类，重写run方法, 代码如下： 123456789101112131415161718from multiprocessing import Processimport timeclass MyProcess(Process): def __init__(self, name): super(MyProcess, self).__init__() self.name = name def run(self): print('process name :' + str(self.name)) time.sleep(1)if __name__ == '__main__': for i in range(3): p = MyProcess(i) p.start() for i in range(3): p.join() 多进程通信进程之间不共享数据的。如果进程之间需要进行通信，则要用到Queue模块或者Pipi模块来实现。 Queue Queue 是多进程安全的队列，可以实现多进程之间的数据传递。它主要有两个函数,put和get。 put() 用以插入数据到队列中，put 还有两个可选参数：blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，该方法会阻塞 timeout 指定的时间，直到该队列有剩余的空间。如果超时，会抛出 Queue.Full 异常。如果 blocked 为 False，但该 Queue 已满，会立即抛出 Queue.Full 异常。 get()可以从队列读取并且删除一个元素。同样，get 有两个可选参数：blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，那么在等待时间内没有取到任何元素，会抛出 Queue.Empty 异常。如果blocked 为 False，有两种情况存在，如果 Queue 有一个值可用，则立即返回该值，否则，如果队列为空，则立即抛出 Queue.Empty 异常。 具体用法如下： 1234567891011from multiprocessing import Process, Queue def put(queue): queue.put('Queue 用法') if __name__ == '__main__': queue = Queue() pro = Process(target=put, args=(queue,)) pro.start() print(queue.get()) pro.join() Pipe Pipe的本质是进程之间的用管道数据传递，而不是数据共享，这和socket有点像。pipe() 返回两个连接对象分别表示管道的两端，每端都有send() 和recv()函数。 如果两个进程试图在同一时间的同一端进行读取和写入那么，这可能会损坏管道中的数据。 具体用法如下： 123456789101112from multiprocessing import Process, Pipe def show(conn): conn.send('Pipe 用法') conn.close() if __name__ == '__main__': parent_conn, child_conn = Pipe() pro = Process(target=show, args=(child_conn,)) pro.start() print(parent_conn.recv()) pro.join() 进程池创建多个进程，我们不用傻傻地一个个去创建。我们可以使用Pool模块来搞定。 Pool 常用的方法如下： 方法 含义 apply() 同步执行（串行） apply_async() 异步执行（并行） terminate() 立刻关闭进程池 join() 主进程等待所有子进程执行完毕。必须在close或terminate()之后使用 close() 等待所有进程结束后，才关闭进程池 具体用法见示例代码： 12345678910111213from multiprocessing import Pooldef show(num): print('num : ' + str(num))if __name__==\"__main__\": pool = Pool(processes = 3) for i in xrange(6): # 维持执行的进程总数为processes，当一个进程执行完毕后会添加新的进程进去 pool.apply_async(show, args=(i, )) print('====== apply_async ======') pool.close() #调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束 pool.join() 选择多线程还是多进程？在这个问题上，首先要看下你的程序是属于哪种类型的。一般分为两种 CPU 密集型 和 I/O 密集型。 CPU 密集型：程序比较偏重于计算，需要经常使用 CPU 来运算。例如科学计算的程序，机器学习的程序等。 I/O 密集型：顾名思义就是程序需要频繁进行输入输出操作。爬虫程序就是典型的 I/O 密集型程序。 如果程序是属于 CPU 密集型，建议使用多进程。而多线程就更适合应用于 I/O 密集型程序。","link":"/710.html"},{"title":"详解 Requests 库的用法","text":"如果你把上篇多线程和多进程的文章搞定了，那么要恭喜你了 。你编写爬虫的能力上了一个崭新的台阶。不过，我们还不能沾沾自喜，因为任重而道远。那么接下来就关注下本文的主要内容。本文主要介绍 urllib 库的代替品 —— Requests。 Requests 简介引用 Requests 官网的说明： Requests is the only Non-GMO HTTP library for Python, safe for human consumption. Requests 官方的介绍语是多么霸气。之所以能够这么霸气，是因为 Requests 相比 urllib 在使用方面上会让开发者感到更加人性化、更加简洁、更加舒服。并且国外一些知名的公司也在使用该库，例如 Google、Microsoft、Amazon、Twitter 等。因此，我们就更加有必要来学习 Request 库了。在学习之前，我们来看下它究竟具有哪些特性？ 具体如下： Keep-Alive &amp; 连接池 国际化域名和 URL 带持久 Cookie 的会话 浏览器式的 SSL 认证 自动内容解码 基本/摘要式的身份认证 优雅的 key/value Cookie 自动解压 Unicode 响应体 HTTP(S) 代理支持 文件分块上传 流下载 连接超时 分块请求 支持 .netrc 安装 Requests古人云：“工欲善其事，必先利其器”。在学习 Requests 之前，我们应先将库安装好。安装它有两种办法。 方法1：通过 pip 安装 比较推荐使用这种方式，既简单又方便管理。 1234pip install requests# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install requests 方法2：通过源码安装 先通过 git 克隆源码库： 1git clone git://github.com/kennethreitz/requests.git 或者直接到 github 网页上下载源码压缩包 接着进入到 requests 目录中执行以下命令： 1python setup.py install 发起请求有了前面学习 urllib 库的经验，现在我们学习 Requests 应该会更加容易上手。 简单抓取网页我们使用 Requests 向百度贴吧发起一个 HTTP 请求，并获取到它页面的源代码。 12345import requests# 使用 get 方式请求response = requests.get('https://tieba.baidu.com/')print(response.text) 那么使用 POST 请求网页，代码又该怎么写呢？相信答案已经浮现在你脑海中了。没错，就是将 get 换成 post 即可。 12345import requests# 使用 post 方式请求response = requests.post('https://tieba.baidu.com/')print(response.text) 传递 URL 参数我们在请求网页时，经常需要携带一些参数。Requests 提供了params关键字参数来满足我们的需求。params 是一个字符串字典，我们只要将字典构建并赋给 params 即可。我们也无须关心参数的编码问题，因为 Requests 很人性化，会将我们需要传递的参数正确编码。它的具体用法如下： 123456789import requestsurl = 'http://httpbin.org/get'params = {'name': 'Numb', 'author': 'Linkin Park'}# params 支持列表作为值# params = {'name': 'Good Time', 'author': ['Owl City', 'Carly Rae Jepsen']}response = requests.get(url, params=params)print(response.url)print(response.text) 如果字典为空是不会被拼接到 URL中的。另外，params 的拼接顺序是随机的，而不是写在前面就优先拼接。 123#运行结果如下：http://httpbin.org/get?name=Numb&amp;author=Linkin+Parkhttp://httpbin.org/get?name=Good+Time&amp;author=Owl+City&amp;author=Carly+Rae+Jepsen 你也许会疑问，为什么会有多了个”+”号呢？这个是 Requests 为了替代空格，它在请求时会自动转化为空格的。 构造请求头为了将 Requests 发起的 HTTP 请求伪装成浏览器，我们通常是使用headers关键字参数。headers 参数同样也是一个字典类型。具体用法见以下代码： 12345678910111213141516171819202122232425262728import requestsurl = 'https://tieba.baidu.com/'headers = { 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36' # 还可以设置其他字段。}response = requests.get(url, headers=headers)print(response.url)print(response.text)``` ### 使用 data 参数提交数据data 参数通常结合 POST 请求方式一起使用。如果我们需要用 POST 方式提交表单数据或者JSON数据，我们只需要传递一个字典给 data 参数。- 提交表单数据我们使用测试网页`http://httpbin.org/post`来提交表单数据作为例子进行展示。```pythonimport requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, data=data) print(response.text) 运行结果如下：我们会看到http://httpbin.org/post页面打印我们的请求内容中，有form字段。 12345678910111213141516171819202122{ \"args\": {}, \"data\": \"\", \"files\": {}, \"form\": { \"pass\": \"admin\", \"user\": \"admin\" }, \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Cache-Control\": \"max-age=259200\", \"Connection\": \"close\", \"Content-Length\": \"21\", \"Content-Type\": \"application/x-www-form-urlencoded\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.11.1\" }, \"json\": null, \"origin\": \"14.*.*.*\", \"url\": \"http://httpbin.org/post\"} 提交 JSON 数据 在HTTP 请求中，JSON 数据是被当作字符串文本。所以，我们使用 data 参数的传递 JSON 数据时，需要将其转为为字符串。我们继续使用上文的代码做演示。 12345678910import jsonimport requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, data=json.dumps(data))print(response.text) 你可以拿下面的运行结果和提交表单数据的运行结果做下对比，你会了解更加清楚两者的差异。 123456789101112131415161718192021{ \"args\": {}, \"data\": \"{\\\"pass\\\": \\\"admin\\\", \\\"user\\\": \\\"admin\\\"}\", \"files\": {}, \"form\": {}, \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Cache-Control\": \"max-age=259200\", \"Connection\": \"close\", \"Content-Length\": \"34\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.11.1\" }, \"json\": { \"pass\": \"admin\", \"user\": \"admin\" }, \"origin\": \"14.*.*.*\", \"url\": \"http://httpbin.org/post\"} 那是否有更加简便的方法来传递 JSON 数据？Requests 在 2.4.2 版本新增该功能。我们可以使用 json 参数直接传递，然后它会被自动编码。 123456789import requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, json=data)print(response.text) 使用代理有些网站做了浏览频率限制。如果我们请求该网站频率过高，该网站会被封掉我们的 IP，禁止我们的访问。所以我们需要使用代理来突破这“枷锁”。这里需要用到proxies参数，proxies 也是一个字典类型。具体用法如下： 123456789101112import requestsurl = 'https://tieba.baidu.com/'proxies = { 'http':\"web-proxy.oa.com:8080\", 'https':\"web-proxy.oa.com:8080\" # 若你的代理需要使用HTTP Basic Auth，可以使用 http://user:password@host/ 语法： # \"http\": \"http://user:pass@27.154.181.34:43353/\"}response = requests.get(url, proxies=proxies)print(response.url)print(response.text) 除了支持 HTTP 代理，Requests 在 2.10 版本新增支持 SOCKS 协议的代理。也就是说，Requests 能够使用 ShadowSocks 代理。看到这里，你的内心是不是有点小激动？使用 SOCKS 代理，需要额外安装一个第三方库，我们就使用 pip 来安装。 1pip install requests[socks] 安装成功之后，就可以正常使用了，用法跟 HTTP 代理相关。具体见代码： 1234proxies = { 'http': 'socks5://user:pass@host:port', 'https': 'socks5://user:pass@host:port'} 设置请求超时我们使用代理发起请求，经常会碰到因代理失效导致请求失败的情况。因此，我们对请求超时做下设置。当发现请求超时，更换代理再重连。 1response = requests.get(url, timeout=3) 如果你要同时设置 connect 和 read 的超时时间，可以传入一个元组进行设置。 1response = requests.get(url, timeout=(3, 30)) 使用 Cookie想在响应结果中获取 cookie 的一些值，可以直接访问。 1response.cookies['key'] # key 为 Cookie 字典中键 想发送 cookies 到服务器，可以使用cookies参数。同样该参数也是字典类型 1234567url = 'http://httpbin.org/cookies'# cookies = dict(domain='httpbin.org')cookies = { 'domain':'httpbin.org',}response = requests.get(url, cookies=cookies)print(response.text) 响应结果我们跟Python 打交道，摆脱不了编码的问题。使用 Requests 请求，我们无需担心编码问题。感觉 Requests 真的是太人性化了。请求发出后，Requests 会基于 HTTP 头部对响应的编码作出有根据的推测。当你访问 response .text 之时，Requests 会使用其推测的文本编码。 12response = requests.get(url)print(response.text) 如果你想改变 response 的编码格式，可以这么做： 1response.encoding = 'UTF-8' 二进制响应内容对于非文本请求， 我们能以字节的方式访问请求响应体。Requests 会自动为我们解码 gzip 和 deflate 传输编码的响应数据。例如，以请求返回的二进制数据创建一张图片，你可以使用如下代码： 1234from PIL import Imagefrom io import BytesIOi = Image.open(BytesIO(response.content)) JSON 响应内容Requests 中也有一个内置的 JSON 解码器，助我们处理 JSON 数据： 12345import requestsurl = 'https://github.com/timeline.json'response = requests.get(url)print(response.json()) 如果 JSON 解码失败， response .json() 就会抛出一个异常。例如，响应内容是 401 (Unauthorized)，尝试访问 response .json() 将会抛出 ValueError: No JSON object could be decoded 异常。 响应状态码我们需要根据响应码来判断请求的结果，具体是这样获取状态码： 1response.status_code Requests 内部提供了一个状态表，如果有需要对状态码进行判断，可以看下requests.codes的源码。 高级用法重定向与请求历史有些页面会做一些重定向的处理。Requests 又发挥人性化的特性。它在默认情况下，会帮我们自动处理所有重定向，包括 301 和 302 两种状态码。我们可以使用response .history来追踪重定向。 Response.history是一个 Response 对象的列表，为了完成请求而创建了这些对象。这个对象列表按照从最老到最近的请求进行排序。 如果我们要禁用重定向处理，可以使用allow_redirects参数： 1response = requests.get(url, allow_redirects=False) 会话Requests 支持 session 来跟踪用户的连接。例如我们要来跨请求保持一些 cookie，我们可以这么做： 1234567s = requests.Session()s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')r = s.get(\"http://httpbin.org/cookies\")print(r.text)# '{\"cookies\": {\"sessioncookie\": \"123456789\"}}' 身份认证有些 web 站点都需要身份认证成功之后才能访问。urllib 具备这样的功能，Requests 也不例外。Requests 支持基本身份认证（HTTP Basic Auth）、netrc 认证、摘要式身份认证、OAuth 1 认证等。 基本身份认证 许多要求身份认证的web服务都接受 HTTP Basic Auth。这是最简单的一种身份认证，并且 Requests 对这种认证方式的支持是直接开箱即可用。HTTP Basic Auth 用法如下： 123456from requests.auth import HTTPBasicAuthurl = 'http://httpbin.org/basic-auth/user/passwd'requests.get(url, auth=HTTPBasicAuth('user', 'pass'))# 简写的使用方式requests.get(url, auth=('user', 'pass')) 摘要式身份认证 摘要式是 HTTP 1.1 必需的第二种身份验证机制。这种身份验证由用户名和密码组成。随后将用 MD5（一种单向哈希算法）对摘要式身份验证进行哈希运算，并将其发送到服务器。具体用法如下： 1234from requests.auth import HTTPDigestAuthurl = 'http://httpbin.org/digest-auth/auth/user/passwd/MD5'requests.get(url, auth=HTTPDigestAuth('user', 'pass')) OAuth 认证 OAuth（开放授权）认证在我们的生活中随处可见。Requests 同样也支持这中认证方式，其中包括 OAuth 1.0 和 OAuth 2.0。如果你需要用到该认证，你需要安装一个支持库requests-oauthlib。我以 OAuth 1.0 认证作为例子进行讲解： 123456789import requestsfrom requests_oauthlib import OAuth1url = 'https://api.twitter.com/1.1/account/verify_credentials.json'auth = OAuth1('YOUR_APP_KEY', 'YOUR_APP_SECRET', 'USER_OAUTH_TOKEN', 'USER_OAUTH_TOKEN_SECRET')requests.get(url, auth=auth)","link":"/711.html"},{"title":"应该如何阅读？","text":"我最近阅读民主与建设出版社出版的《如何阅读一本书》，自己收获颇多。这本书算是经典之作。以通俗的语言告诉我们如何选择书籍？究竟要以什么方法来阅读一本书？我将自己学到的分享出来。希望能帮助大家提高阅读速度，把书籍读得更加明白，记得更牢固。 为什么要阅读？在进入本文主题之前，我们先思考一个问题 —— 我们为什么要阅读？你可以不必急着回答这个问题，带着问题来往下读。 古人有很多名句鼓励学子多读书，例如宋真宗赵恒的《劝学诗》，其中有我们耳熟能详的语句： 书中自有千种粟书中自有黄金屋书中车马多如簇书中自有颜如玉 由此可见，阅读是手段。我们可以通过读书来获得赖以谋能的技能。那么问题来了？我们要阅读什么书？随便阅读一本书就能获取知识吗？答案是否定的。所以我们要读好书，还要掌握些阅读的技巧。 阅读什么书？市面上书籍种类琳琅满目。我们该如何选择书籍？先来看看书籍的分类 第一类：如同主食 能够解决职业、生活、生理、心理等方面的现实问题的书籍都可以称为“主食”。”主食”是我们的刚需。所以我们就应该花大量时间去阅读。举个栗子，假如你是一名 Android 粉丝，想通过学习 Android 开发来谋生。那么你应该阅读 Android 开发的书籍，例如《第一行代码》、《Android 源码设计模式解析与实战》等。 第二类：如同美食 这类书籍是不求针对人生的现实问题，却可以满足思想要求。对于这些书籍，我们应该重“质”不重“量”。我们不知道怎么选择这类书籍时，可以根据一些名家推荐或者订阅一些名家的微信公众号。例如，张哥的 stomzhang 公众号。张哥的每篇推文，我基本上都有仔细阅读。我自己订阅张哥的公众号一年多了，提高不仅仅是专业技能，更是视野。 第三类：如同蔬菜、水果 可以理解为工具书。这类书籍不仅可以帮助我们查找不了解的字词、概念、数据等信息，也可以帮助我们掌握通用的方法与技巧。 第四类：如同甜点、零食 这类书籍是用于娱乐、消遣、满足休闲需求。一些网络小说和娱乐性图书，包括一部分畅销书都属于此列。对于这类书籍，我们只可偶尔阅读，但不能过。 阅读方法阅读可以分为四个层次，不同的阅读层次适用不同的阅读方法。具体分类如下：1）基础阅读（Elementray Reading）2）检视阅读（Inspectional Reading）3）分析阅读（Analytical Reading）4）主题阅读（Syntopical Reading） 第一层：基础阅读基础阅读，即处于四肢阶段的阅读。所以又被称为初级阅读。我们在小学阶段已经学会了，因为这一层次的阅读要求是认识文字并了解文字的意思。 第二层：检视阅读检视阅读，即系统化略读。类似我们初高中做语文的阅读理解题目。 检视阅读是非常有价值的阅读方式。通过检视阅读，我们可以了解一本书“主要讲什么内容”、“书的结构如何”、“各章重点讲什么”，进而判断这本书是否值得分析阅读或主题阅读。 对一本书进行检视阅读，可以按照以下步骤：A、看书名、副题、作者简介、序言，大致清楚作者的写作风格、作者在什么背景下著作本书的。B、研究目录，了解作者的写作路线。C、粗略地阅读一下各章的内容，遇到不懂的部分就跳过去。 看到这里，你可能有这样的疑问：我没有将一本书读完，只阅读其中几章，这算是阅读吗？算。诸葛孔明的“略其大意”，陶渊明的“不求甚解”都算是这种阅读方法。 第三层：分析阅读分析阅读，即完整阅读。也就是精读。通过分析阅读，我们可以对全书有更精准的把握，复述全书各部分的大意及重要细节，然后使之成为自己的知识。 这种阅读方法更适合“主食”和“美食”类图书。使用分析阅读就是带着四个问题去阅读： 1）这本书究竟讲了什么？ 回到这个问题，事实上就是做到以下三步：A、对书的体裁和主题进行确认。B、自己组织语言表述整本书的内容C、按照全书的结构顺序或逻辑顺序列举全书的大纲，并将各个部分的大纲也列出来。思维导图就可以派上用场了。 2）作者通过这本书解决了什么问题？ 回答这个问题，也需要一步一步找出作者的主要想法、声明与论点，并形成自己的判断。首先，找出可以表达作者观点的关键字，与作者达成共识。然后，在最重要的语句中，提炼关键字，抓住作者的主旨。最后，根据书中的内容确定作者已经解决了哪些问题，还有哪些是没有解决的。 3）这本书说得有道理吗？ 对本书进行评论就是这个问题的答案。但在没有对问题 2 进行完整的解答之前，不要轻易去尝试。评论一本书不要带有个人感情色彩。 我个人觉得评论一本书类似写议论文，要有理有据，求同存异。 4）这本书与我有什么关系？ 这个问题与阅读效用有密切相关，简单地回答，就是“有用”或者“部分有用”。 如果一本书告诉我们一些咨询，我们一定要问一问这些咨询有什么意义；如果一本书不仅提供咨询，还对我们有所启发，就更应该找出书中更深的含意或其他相关的建议，以获得更多启示。 第四层：主题阅读 主题阅读是主动的、专一的、大量的阅读。 主题阅读，顾名思义就是定个主题，然后使用检视阅读来筛选与主题有关的书籍，再对每本书中与主题极为相关的具体章节进行精读来建立自己的主旨（论点）。 这种阅读方法带有很强的阅读性，不能短时间能掌握的，需要长期的阅读积累以及阅读训练。","link":"/812.html"},{"title":"干将莫邪” —— Xpath 与 lxml 库","text":"前面的文章，我们已经学会正则表达式以及 BeautifulSoup库的用法。我们领教了正则表达式的便捷，感受 beautifulSoup 的高效。本文介绍也是内容提取的工具 —— Xpath，它一般和 lxml 库搭配使用。所以，我称这两者为“干将莫邪”。 Xpath 和 lxml Xpath XPath即为XML路径语言，它是一种用来确定XML（标准通用标记语言的子集）文档中某部分位置的语言。XPath 基于 XML 的树状结构，提供在数据结构树中找寻节点的能力。 Xpath 原本是用于选取 XML 文档节点信息。XPath 是于 1999 年 11 月 16 日 成为 W3C 标准。因其既简单方便又容易，所以它逐渐被人说熟知。 lxml lxml 是功能丰富又简单易用的，专门处理 XML 和 HTML 的 Python 官网标准库。 Xpath 的语法正则表达式的枯燥无味又学习成本高，Xpath 可以说是不及其万分之一。所以只要花上 10 分钟，掌握 Xpath 不在话下。Xpath 的语言以及如何从 HTML dom 树中提取信息，我将其归纳为“主干 - 树支 - 绿叶”。 “主干” —— 选取节点抓取信息，我们需知道要从哪里开始抓取。因此，需要找个起始节点。Xpath 选择起始节点有以下可选： 表达式 描述 nodename 选取标签节点的所有子节点。 / 从根节点选取，html DOM 树的节点就是 html。 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。 . 选择当前节点，一般用于二级提取。 .. 选取当前节点的父节点，在二级提取时用到。 @ 选取属性。 我们通过以下实例来了解其用法： 路径表达式 描述 xpath(‘//div’) 选取 div 元素的所有子节点。 xpath(‘/div’) 选取 div 元素作为根节点。如果同级有多个 div ，那么所有 div 都会被选为根节点。 xpath(‘/div/span’) 选取属于 div 元素下所有 span 元素节点。如果 span 有多个，也会被选中。 xpath(‘//div’) 选取所有 div 元素节点，不管它们在文档的位置。 xpath(‘//div/span’) 选取 div 元素下的所有 span 元素节点，不管位于 div 之下的什么位置 xpath(“//@[class=’content’]”) 选取包含属性 class 的值为 content 的节点，不管是 div 元素还是其他元素 xpath(“//@[id=’center’]”) 选取属性 id 的值为 center 的节点，不管是 div 元素还是其他元素 如果你对于提取节点没有头绪的时候，可以使用通配符来暂时替代。等查看输出内容之后再进一步确认。 路径表达式 描述 xpath(‘/div/*’) 选取 div 元素节点下的所有节点 xpath(‘/div[@*]’) 选取所有带属性的 div 元素节点 “分支” —— 关系节点与谓语这一步的过程其实是通过起点一步步来寻找最终包含我们所需内容的节点。我们有时需要使用到相邻节点信息。因此，我们需要了解关系节点或者谓语。 关系节点 一般而言，DOM 树中一个普通节点具有父节点、兄弟节点、子节点。当然也有例外的情况。这些有些节点比较特殊，可能没有父节点，如根节点；也有可能是没有子节点，如深度最大的节点。Xpath 也是有支持获取关系节点的语法。 关系 路径表达式 描述 parent（父） xpath(‘./parent::*’) 选取当前节点的父节点 ancestor（先辈） xpath(‘./ancestor::*’) 选取当前节点的所有先辈节点，包括父、祖父等 ancestor-or-self（先辈及本身） xpath(‘./ancestor-or-self::*’) 选取当前节点的所有先辈节点以及节点本身 child（子） xpath(‘./child::*’) 选取当前节点的所有子节点 descendant（后代） xpath(‘./descendant::*’) 选取当前节点的所有后代节点，包括子节点、孙节点等 following xpath(‘./following ::*’) 选取当前节点结束标签后的所有节点 following-sibing xpath(‘./following-sibing::*’) 选取当前节点之后的兄弟节点 preceding xpath(‘./preceding::*’) 选取当前节点开始标签前的所有节点 preceding-sibling xpath(‘./parent::*’) 选取当前节点之前的兄弟节点 self（本身） xpath(‘./self::*’) 选取当前节点本身 谓语 谓语用来查找某个特定的节点或者包含某个指定的值的节点。同时，它是被嵌在方括号中的。 路径表达式 描述 xpath(‘./body/div[1]’) 选取 body 元素节点下的第一个 div 子节。 xpath(‘./body/div[last()]’) 选取 body 元素节点下的最后一个 div 子节。 xpath(‘./body/div[last()-1]’) 选取 body 元素节点下的倒数第二个 div 子节。 xpath(‘./body/div[position()-3]’) 选取 body 元素节点下的前二个 div 子节。 xpath(‘./body/div[@class]’) 选取 body 元素节点下的所有带有 class 属性的 div 子节。 xpath(“./body/div[@class=’content’]”) 选取 body 元素节点下的 class 属性值为 centent 的 div 子节。 “绿叶” —— 节点内容以及属性到了这一步，我们已经找到所需内容的节点了。接下来就是获取该节点中的内容了。Xpath 语法提供了提供节点的文本内容以及属性内容的功能。 路径表达式 描述 text() 获取节点的本文内容 @属性 获取节点的属性内容 具体用法见以下实例： 路径表达式 描述 xpath(‘./a/text()’) 获取当前节点中 a 元素节点中的本文内容 xpath(‘./a/@href’) 获取当前节点中 a 元素节点中的 href 属性的内容 lxml 的用法安装 lxmlpip 是安装库文件的最简便的方法，具体命令如下： 1234pip install lxml# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install lxml 使用 lxmllxml 使用起来是比较简单的。我们首先要使用 lxml 的 etree 将 html 页面进行初始化，然后丢给 Xpath 匹配即可。具体用法如下： 12345from lxml import etreehtml = requests.get(url) # 使用 requests 请求网页selector = etree.HTML(html.text)content = selector.xpath('//a/text()') 没错，就这短短几行代码即可完成信息提取。值得注意的是：xpath 查找匹配返回的类型有可能是一个值，也有可能是一个存放多个值的列表。这个取决于你的路径表达式是如何编写的。","link":"/813.html"},{"title":"爬虫实战二：爬取电影天堂的最新电影","text":"前面两篇文章介绍 requests 和 xpath 的用法。我们推崇学以致用，所以本文讲解利用这两个工具进行实战。 爬取目标本次爬取的站点选择电影天堂，网址是： www.ydtt8.net。爬取内容是整个站点的所有电影信息，包括电影名称，导演、主演、下载地址等。具体抓取信息如下图所示： 设计爬虫程序确定爬取入口电影天堂里面的电影数目成千上万，电影类型也是让人眼花缭乱。我们为了保证爬取的电影信息不重复， 所以要确定一个爬取方向。目前这情况真让人无从下手。但是，我们点击主页中的【最新电影】选项，跳进一个新的页面。蓦然有种柳暗花明又一村的感觉。 由图可知道，电影天堂有 5 个电影栏目，分别为最新电影、日韩电影、欧美电影、国内电影、综合电影。每个栏目又有一定数量的分页，每个分页有 25 条电影信息。那么程序的入口可以有 5 个 url 地址。这 5 个地址分别对应每个栏目的首页链接。 爬取思路知道爬取入口，后面的工作就容易多了。我通过测试发现这几个栏目除了页面的 url 地址不一样之外，其他例如提取信息的 xpath 路径是一样的。因此，我把 5 个栏目当做 1 个类，再该类进行遍历爬取。 我这里“最新电影”为例说明爬取思路。1）请求栏目的首页来获取到分页的总数，以及推测出每个分页的 url 地址；2）将获取到的分页 url 存放到名为 floorQueue 队列中；3）从 floorQueue 中依次取出分页 url，然后利用多线程发起请求；4）将获取到的电影页面 url 存入到名为 middleQueue 的队列；5）从 middleQueue 中依次取出电影页面 url，再利用多线程发起请求；6）将请求结果使用 Xpath 解析并提取所需的电影信息；7）将爬取到的电影信息存到名为 contentQueue 队列中；8）从 contentQueue 队列中依次取出电影信息，然后存到数据库中。 设计爬虫架构根据爬取思路，我设计出爬虫架构。如下图所示： 代码实现主要阐述几个重要的类的代码 main 类 主要工作两个：第一，实例化出一个dytt8Moive对象，然后开始爬取信息。第二，等爬取结束，将数据插入到数据库中。 处理爬虫的逻辑代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 截止到2017-08-08, 最新电影一共才有 164 个页面LASTEST_MOIVE_TOTAL_SUM = 6 #164# 请求网络线程总数, 线程不要调太好, 不然会返回很多 400THREAD_SUM = 5def startSpider(): # 实例化对象 # 获取【最新电影】有多少个页面 LASTEST_MOIVE_TOTAL_SUM = dytt_Lastest.getMaxsize() print('【最新电影】一共 ' + str(LASTEST_MOIVE_TOTAL_SUM) + ' 有个页面') dyttlastest = dytt_Lastest(LASTEST_MOIVE_TOTAL_SUM) floorlist = dyttlastest.getPageUrlList() floorQueue = TaskQueue.getFloorQueue() for item in floorlist: floorQueue.put(item, 3) # print(floorQueue.qsize()) for i in range(THREAD_SUM): workthread = FloorWorkThread(floorQueue, i) workthread.start() while True: if TaskQueue.isFloorQueueEmpty(): break else: pass for i in range(THREAD_SUM): workthread = TopWorkThread(TaskQueue.getMiddleQueue(), i) workthread.start() while True: if TaskQueue.isMiddleQueueEmpty(): break else: pass insertData() if __name__ == '__main__': startSpider() 创建数据库以及表，接着再把电影信息插入到数据库的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def insertData(): DBName = 'dytt.db' db = sqlite3.connect('./' + DBName, 10) conn = db.cursor() SelectSql = 'Select * from sqlite_master where type = \"table\" and name=\"lastest_moive\";' CreateTableSql = ''' Create Table lastest_moive ( 'm_id' INTEGER PRIMARY KEY, 'm_type' varchar(100), 'm_trans_name' varchar(200), 'm_name' varchar(100), 'm_decade' varchar(30), 'm_conutry' varchar(30), 'm_level' varchar(100), 'm_language' varchar(30), 'm_subtitles' varchar(100), 'm_publish' varchar(30), 'm_IMDB_socre' varchar(50), 'm_douban_score' varchar(50), 'm_format' varchar(20), 'm_resolution' varchar(20), 'm_size' varchar(10), 'm_duration' varchar(10), 'm_director' varchar(50), 'm_actors' varchar(1000), 'm_placard' varchar(200), 'm_screenshot' varchar(200), 'm_ftpurl' varchar(200), 'm_dytt8_url' varchar(200) ); ''' InsertSql = ''' Insert into lastest_moive(m_type, m_trans_name, m_name, m_decade, m_conutry, m_level, m_language, m_subtitles, m_publish, m_IMDB_socre, m_douban_score, m_format, m_resolution, m_size, m_duration, m_director, m_actors, m_placard, m_screenshot, m_ftpurl, m_dytt8_url) values(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?); ''' if not conn.execute(SelectSql).fetchone(): conn.execute(CreateTableSql) db.commit() print('==== 创建表成功 ====') else: print('==== 创建表失败, 表已经存在 ====') count = 1 while not TaskQueue.isContentQueueEmpty(): item = TaskQueue.getContentQueue().get() conn.execute(InsertSql, Utils.dirToList(item)) db.commit() print('插入第 ' + str(count) + ' 条数据成功') count = count + 1 db.commit() db.close() TaskQueue 类 维护 floorQueue、middleQueue、contentQueue 三个队列的管理类。之所以选择队列的数据结构，是因为爬虫程序需要用到多线程，队列能够保证线程安全。 dytt8Moive 类 dytt8Moive 类是本程序的主心骨。程序最初的爬取目标是 5 个电影栏目，但是目前只现实了爬取最新栏目。如果你想爬取全部栏目电影，只需对 dytt8Moive 稍微改造下即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252class dytt_Lastest(object): # 获取爬虫程序抓取入口 breakoutUrl = 'http://www.dytt8.net/html/gndy/dyzz/index.html' def __init__(self, sum): self.sum = sum # 获取【最新电影】有多少个页面 # 截止到2017-08-08, 最新电影一共才有 164 个页面 @classmethod def getMaxsize(cls): response = requests.get(cls.breakoutUrl, headers=RequestModel.getHeaders(), proxies=RequestModel.getProxies(), timeout=3) # 需将电影天堂的页面的编码改为 GBK, 不然会出现乱码的情况 response.encoding = 'GBK' selector = etree.HTML(response.text) # 提取信息 optionList = selector.xpath(\"//select[@name='sldd']/text()\") return len(optionList) - 1 # 因首页重复, 所以要减1 def getPageUrlList(self): ''' 主要功能：目录页url取出，比如：http://www.dytt8.net/html/gndy/dyzz/list_23_'+ str(i) + '.html ''' templist = [] request_url_prefix = 'http://www.dytt8.net/html/gndy/dyzz/' templist = [request_url_prefix + 'index.html'] for i in range(2, self.sum + 1): templist.append(request_url_prefix + 'list_23_' + str(i) + '.html') for t in templist: print('request url is ### ' + t + ' ###') return templist @classmethod def getMoivePageUrlList(cls, html): ''' 获取电影信息的网页链接 ''' selector = etree.HTML(html) templist = selector.xpath(\"//div[@class='co_content8']/ul/td/table/tr/td/b/a/@href\") # print(len(templist)) # print(templist) return templist @classmethod def getMoiveInforms(cls, url, html): ''' 解析电影信息页面的内容, 具体如下： 类型 : 疾速特攻/疾速追杀2][BD-mkv.720p.中英双字][2017年高分惊悚动作] ◎译名 : ◎译\\u3000\\u3000名\\u3000疾速特攻/杀神John Wick 2(港)/捍卫任务2(台)/疾速追杀2/极速追杀：第二章/约翰·威克2 ◎片名 : ◎片\\u3000\\u3000名\\u3000John Wick: Chapter Two ◎年代 : ◎年\\u3000\\u3000代\\u30002017 ◎国家 : ◎产\\u3000\\u3000地\\u3000美国 ◎类别 : ◎类\\u3000\\u3000别\\u3000动作/犯罪/惊悚 ◎语言 : ◎语\\u3000\\u3000言\\u3000英语 ◎字幕 : ◎字\\u3000\\u3000幕\\u3000中英双字幕 ◎上映日期 ：◎上映日期\\u30002017-02-10(美国) ◎IMDb评分 : ◎IMDb评分\\xa0 8.1/10 from 86,240 users ◎豆瓣评分 : ◎豆瓣评分\\u30007.7/10 from 2,915 users ◎文件格式 : ◎文件格式\\u3000x264 + aac ◎视频尺寸 : ◎视频尺寸\\u30001280 x 720 ◎文件大小 : ◎文件大小\\u30001CD ◎片长 : ◎片\\u3000\\u3000长\\u3000122分钟 ◎导演 : ◎导\\u3000\\u3000演\\u3000查德·史塔赫斯基 Chad Stahelski ◎主演 : ◎简介 : 暂不要该字段 ◎获奖情况 : 暂不要该字段 ◎海报 影片截图 下载地址 ''' # print(html) contentDir = { 'type': '', 'trans_name': '', 'name': '', 'decade': '', 'conutry': '', 'level': '', 'language': '', 'subtitles': '', 'publish': '', 'IMDB_socre': '', 'douban_score': '', 'format': '', 'resolution': '', 'size': '', 'duration': '', 'director': '', 'actors': '', 'placard': '', 'screenshot': '', 'ftpurl': '', 'dytt8_url': '' } selector = etree.HTML(html) content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/p/text()\") # 匹配出来有两张图片, 第一张是海报, 第二张是电影画面截图 imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/p/img/@src\") # print(content) # 为了兼容 2012 年前的页面 if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/span/text()\") # 有些页面特殊, 需要用以下表达式来重新获取信息 # 电影天堂页面好混乱啊~ if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/div/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/font/text()\") if len(content) &lt; 5: content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/p/font/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/span/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/span/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/font/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/text()\") # print(content) # 不同渲染页面要采取不同的抓取方式抓取图片 if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/img/@src\") if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/img/@src\") if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/img/@src\") if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/div/img/@src\") # 类型 if content[0][0:1] != '◎': contentDir['type'] = '[' + content[0] actor = '' for each in content: if each[0:5] == '◎译\\u3000\\u3000名': # 译名 ◎译\\u3000\\u3000名\\u3000 一共占居6位 contentDir['trans_name'] = each[6: len(each)] elif each[0:5] == '◎片\\u3000\\u3000名': # 片名 contentDir['name'] = each[6: len(each)] elif each[0:5] == '◎年\\u3000\\u3000代': # 年份 contentDir['decade'] = each[6: len(each)] elif each[0:5] == '◎产\\u3000\\u3000地': # 产地 contentDir['conutry'] = each[6: len(each)] elif each[0:5] == '◎类\\u3000\\u3000别': # 类别 contentDir['level'] = each[6: len(each)] elif each[0:5] == '◎语\\u3000\\u3000言': # 语言 contentDir['language'] = each[6: len(each)] elif each[0:5] == '◎字\\u3000\\u3000幕': # 字幕 contentDir['subtitles'] = each[6: len(each)] elif each[0:5] == '◎上映日期': # 上映日期 contentDir['publish'] = each[6: len(each)] elif each[0:7] == '◎IMDb评分': # IMDb评分 contentDir['IMDB_socre'] = each[9: len(each)] elif each[0:5] == '◎豆瓣评分': # 豆瓣评分 contentDir['douban_score'] = each[6: len(each)] elif each[0:5] == '◎文件格式': # 文件格式 contentDir['format'] = each[6: len(each)] elif each[0:5] == '◎视频尺寸': # 视频尺寸 contentDir['resolution'] = each[6: len(each)] elif each[0:5] == '◎文件大小': # 文件大小 contentDir['size'] = each[6: len(each)] elif each[0:5] == '◎片\\u3000\\u3000长': # 片长 contentDir['duration'] = each[6: len(each)] elif each[0:5] == '◎导\\u3000\\u3000演': # 导演 contentDir['director'] = each[6: len(each)] elif each[0:5] == '◎主\\u3000\\u3000演': # 主演 actor = each[6: len(each)] for item in content: if item[0: 4] == '\\u3000\\u3000\\u3000\\u3000': actor = actor + '\\n' + item[6: len(item)] # 主演 contentDir['actors'] = actor # 海报 if imgs[0] != None: contentDir['placard'] = imgs[0] # 影片截图 if imgs[1] != None: contentDir['screenshot'] = imgs[1] # 下载地址 ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/table/tbody/tr/td/a/text()\") # 为了兼容 2012 年前的页面 if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/table/tbody/tr/td/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/div/table/tbody/tr/td/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/table/tbody/tr/td/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/span/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/div/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/span/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/span/div/table/tbody/tr/td/font/a/text()\") contentDir['ftpurl'] = ftp[0] # 页面链接 contentDir['dytt8_url'] = url print(contentDir) return contentDir getMoiveInforms 方法是主要负责解析电影信息节点并将其封装成字典。在代码中，你看到 Xpath 的路径表达式不止一条。因为电影天堂的电影详情页面的排版参差不齐，所以单单一条内容提取表达式、海报和影片截图表达式、下载地址表达式远远无法满足。 选择字典类型作为存储电影信息的数据结构，也是自己爬坑之后才决定的。这算是该站点另一个坑人的地方。电影详情页中有些内容节点是没有，例如类型、豆瓣评分，所以无法使用列表按顺序保存。 爬取结果我这里展示自己爬取最新栏目中 4000 多条数据中前面部分数据。 如果你想获取网站的源码, 点击☞☞☞Github 仓库地址","link":"/814.html"},{"title":"用 Python 学习数据结构, 有它就不用愁","text":"数据结构，我们对它已经是耳熟能详。对于计算机相关专业的大学生来说，它是一门专业必修课。从事软件开发的人员则把它作为谋生必备技能。这充分体现数据结构的重要性。因此，我们对数据结构是不得不学。 虽然数据结构的实现不限制语言，但市面上很多教程书籍都是以 C 语言作为编程语言进行讲解。如果你喜欢且在学习 Python，可能会陷入苦于这样的烦恼中。那就是没有 Python 版本的数据结构实现代码。莫慌！我给大家推荐一个第三方库，它能让你这种烦恼立刻云消雾散。 它就是Pygorithm 地址：https://github.com/OmkarPathak/pygorithm Pygorithm 是由一个热心肠的印度小哥编写的开源项目。他编写创建该库的初衷是处于教学目的。我们不仅可以阅读源码的方式学习数据结构，而且可以把它当做现成工具来使用。 安装安装 python 库，我推荐使用 pip 方式，方便又省事。 1234pip install Pygorithm# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install Pygorithm 支持的类型Pygorithm 实现的数据结构类型有以下这几种，括号中表示包名。 栈 栈 (data_structures.stack.Stack) 中缀表达式转换为后缀表达式 (data_structures.stack.InfixToPostfix) 队列 队列 (data_structures.queue.Queue) 双端队列 (data_structures.queue.Deque) 链表 单向链表 (data_structures.linked_list.SinglyLinkedList) 双向链表 (data_structures.linked_list.DoublyLinkedList) 树 二叉树 (data_structures.tree.BinaryTree) 搜索二叉树 (data_structures.tree.BinarySearchTree) 图 图 (data_structures.graph.Graph) 拓扑排序 (data_structures.graph.TopologicalSort) 有向图 (data_structures.graph.CheckCycleDirectedGraph) 无向图 (data_structures.graph.CheckCycleUndirectedGraph) 堆 堆 (data_structures.heap.Heap) 字典树 字典树 （data_structures.trie.Trie） 常见算法你也许没有想到吧。Pygorithm 中也实现一些常见的路径搜索、查找、排序等算法。 常见的路径搜索算法： Dijkstra(迪杰斯特拉) Unidirectional AStar（单向 A*算法） BiDirectional AStar（双向 A*算法） 常见的查找算法： Linear Search (线性查找) Binary Search (二分法查找) Breadth First Search (广度优先搜索) Depth First Search (深度优先搜索) 常见的排序算法： bubble_sort（冒泡算法） bucket_sort（桶排序） counting_sort （计数排序） heap_sort（堆排序） insertion_sort（插入排序） merge_sort（归并排序） quick_sort （快速排序） selection_sort（选择排序） shell_sort（希尔排序）","link":"/815.html"},{"title":"学会运用爬虫框架 Scrapy (一)","text":"对于规模小、爬取数据量小、对爬取速度不敏感的爬虫程序， 使用 Requests 能轻松搞定。这些爬虫程序主要功能是爬取网页、玩转网页。如果我们需要爬取网站以及系列网站，要求爬虫具备爬取失败能复盘、爬取速度较高等特点。很显然 Requests 不能完全满足我们的需求。因此，需要一功能更加强大的第三方爬虫框架库 —— Scrapy 简介 ScrapyScrapy 是一个为了方便人们爬取网站数据，提取结构性数据而编写的分布式爬取框架。它可以应用在包括数据挖掘， 信息处理或存储历史数据等一系列的程序中。因其功能颇多，所以学会它需要一定的时间成本。 Scrapy 的特性Scrapy 是一个框架。因此，它集一些各功能强大的 python 库的优点于一身。下面列举其一些特性： HTML, XML源数据 选择及提取 的内置支持 提供了一系列在spider之间共享的可复用的过滤器(即 Item Loaders)，对智能处理爬取数据提供了内置支持。 通过 feed导出 提供了多格式(JSON、CSV、XML)，多存储后端(FTP、S3、本地文件系统)的内置支持 提供了media pipeline，可以 自动下载 爬取到的数据中的图片(或者其他资源)。 高扩展性。您可以通过使用 signals ，设计好的API(中间件, extensions, pipelines)来定制实现您的功能。 内置的中间件及扩展为下列功能提供了支持: cookies and session 处理 HTTP 压缩 HTTP 认证 HTTP 缓存 user-agent模拟 robots.txt 爬取深度限制 健壮的编码支持和自动识别，用于处理外文、非标准和错误编码问题 针对多爬虫下性能评估、失败检测，提供了可扩展的 状态收集工具 。 内置 Web service, 使您可以监视及控制您的机器。 安装 ScrapyScrapy 是单纯用 Python 语言编写的库。所以它有依赖一些第三方库，如lxml, twisted,pyOpenSSL等。我们也无需逐个安装依赖库，使用 pip 方式安装 Scrapy 即可。pip 会自动安装 Scrapy 所依赖的库。随便也说下 Scrapy 几个重要依赖库的作用。 lxml：XML 和 HTML 文本解析器，配合 Xpath 能提取网页中的内容信息。如果你对 lxml 和 Xpath 不熟悉，你可以阅读我之前介绍该库用法的文章。 Twisted：Twisted 是 Python 下面一个非常重要的基于事件驱动的IO引擎。 pyOpenSSL：pyopenssl 是 Python 的 OpenSSL 接口。 在终端执行以下命令来安装 Scrapy 1234pip install Scrapy # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install Scrapy 你在安装过程中也许会报出安装 Twisted 失败的错误： 1234567running build_extbuilding 'twisted.test.raiser' extensionerror: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": http://landinghub.visualstudio.com/visual-cpp-build-tools----------------------------------------Failed building wheel for TwistedRunning setup.py clean for TwistedFailed to build Twisted 原因是 Twisted 底层是由 C 语言编写的，所以需要安装C语言的编译环境。对于Python3.5来说，可以通过安装 Visual C++ Build Tools 来安装这个环境。打开上面报错文本中的链接，下载并安装 visualcppbuildtools_full 。等安装完成，再执行 安装 Scrapy 命令。 安装成功之后如下图： 初探 ScrapyScrapy 项目解析Scrapy 新建项目需通过命令行操作。在指定文件夹中，打开终端执行以下命令： 1scrapy startproject 项目的名字 我新建一个名为 scrapy_demo，执行结果如下。 使用 Pycharm 打开该项目，我们会发现项目的层级架构以及文件。 这些文件的作用是： scrapy.cfg：项目的配置文件，开发无需用到。 scrapy_demo：项目中会有两个同名的文件夹。最外层表示 project，里面那个目录代表 module（项目的核心）。 scrapy_demo/items.py：以字段形式定义后期需要处理的数据。 scrapy_demo/pipelines.py：提取出来的 Item 对象返回的数据并进行存储。 scrapy_demo/settings.py：项目的设置文件。可以对爬虫进行自定义设置，比如选择深度优先爬取还是广度优先爬取，设置对每个IP的爬虫数，设置每个域名的爬虫数，设置爬虫延时，设置代理等等。 scrapy_demo/spider： 这个目录存放爬虫程序代码。 __init__.py：python 包要求，对 scrapy 作用不大。 Scrapy 的架构我们刚接触到新事物，想一下子就熟悉它。这明显是天方夜谭。应按照一定的顺序层次、逐步深入学习。学习 Scrapy 也不外乎如此。在我看来，Scrapy 好比由许多组件拼装起来的大机器。因此，可以采取从整体到局部的顺序学习 Scrapy。下图是 Scrapy 的架构图，它能让我们对 Scrapy 有了大体地认识。后续的文章会逐个介绍其组件用法。 我按照从上而下，从左往右的顺序阐述各组件的作用。 Scheduler：调度器。负责接受 Engine 发送过来的 Requests 请求，并将其队列化； Item Pipeline：Item Pipeline负责处理被spider提取出来的item。其有典型应用，如清理 HTML 数据、验证爬取的数据（检查 item 包含某些字段）、查重（并丢弃）、爬取数据持久化（存入数据库、写入文件等）； Scrapy Engine：引擎是 Scrapy 的中枢。它负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件； Downloader Middlewares：下载中间件是 Engine 和 Downloader 的枢纽。负责处理 Downloader 传递给 Engine 的 responses；它还支持自定义扩展。 Downloader：负责下载 Engine 发送的所有 Requests 请求，并将其获取到的 responses 回传给 Scrapy Engine； Spider middlewares：Spider 中间件是 Engine 和 Spider 的连接桥梁；它支持自定义扩展来处理 Spider 的输入(responses) 以及输出 item 和 requests 给 Engine ； Spiders：负责解析 Responses 并提取 Item 字段需要的数据，再将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)； Scrapy 工作机制我们对 Scrapy 有了大体上的认识。接下来我们了解下 Scrapy 内部的工作流程。同样先放出一张图，然后我再细细讲解。 当引擎(Engine) 收到 Spider 发送过来的 url 主入口地址（其实是一个 Request 对象, 因为 Scrapy 内部是用到 Requests 请求库），Engine 会进行初始化操作。 Engine 请求调度器（Scheduler），让 Scheduler 调度出下一个 url 给 Engine。 Scheduler 返回下一个 url 给 Engine。 Engine 将 url通过下载中间件(请求(request)方向)转发给下载器(Downloader)。 一旦页面下载完毕，Downloader 生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给 Engine 引擎将从下载器中接收到 Response 发送给Spider处理。 Spider 处理 Response 并返回爬取到的 Item 及新的 Request 给引擎。 Engine 将 Spider 返回的爬取到的 Item 转发给Item Pipeline，顺便也将将 Request 给调度器。 重复（第2步)直到调度器中没有更多地request，引擎关闭该网站。","link":"/916.html"},{"title":"学会运用爬虫框架 Scrapy (二)","text":"上篇文章介绍了爬虫框架 Scrapy 如何安装，以及其特性、架构、数据流程。相信大家已经对 Scrapy 有人了初步的认识。本文是 Scrapy 系列文章的第二篇，主要通过一个实例讲解 scrapy 的用法。 选取目标网络爬虫，顾名思义是对某个网站或者系列网站，按照一定规则进行爬取信息。爬取程序的首要工作当然是选定爬取目标。本次爬取目标选择是V电影，网址是http://www.vmovier.com/。爬取内容是[最新推荐]栏目的前15条短视频数据信息。具体信息包括封面、标题、详细说明以及视频播放地址。 定义 Item为什么将爬取信息定义清楚呢？因为接下来 Item 需要用到。在 Item.py 文件中，我们以类的形式以及 Field 对象来声明。其中 Field 对象其实是一个字典类型，用于保存爬取到的数据。而定义出来的字段，可以简单理解为数据库表中的字段，但是它没有数据类型。Item 则复制了标准的 dict API，存放以及读取跟字典没有差别。 V电影的 Item，我们可以这样定义： 12345678910111213import scrapyclass ScrapyDemoItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 封面 cover = scrapy.Field() # 标题 title = scrapy.Field() # 简述 dec = scrapy.Field() # 播放地址 playUrl = scrapy.Field() 编写 SpiderSpider 目录是我们爬虫程序爬取网站以及提取信息的模块。我们首先在目录下新建一个名为 VmoiveSpider 的文件。同时，该类继承scrapy.Spider。 这里我们用到的scrapy.spider.Spider 是 Scrapy 中最简单的内置 spider。继承 spider 的类需要定义父类中的属性以及实现重要的方法。 name 这个属性是非常重要的，所以必须定义它。定义 name 目的是为爬虫程序命名。因此，还要保持 name 属性是唯一的。它是 String 类型，我们在 VmoiveSpider 可以定义： 1name = 'vmoive' start_urls start_urls 是 Url 列表，也是必须被定义。可以把它理解为存放爬虫程序的主入口 url 地址的容器。 allowed_domains 可选字段。包含了spider允许爬取的域名(domain)列表(list)。 当 OffsiteMiddleware 启用时， 域名不在列表中的URL不会被跟进。根据 V 电影的 url 地址，我们可以这样定义： 1allowed_domains = ['vmovier.com'] parse(response) parser 方法是Scrapy处理下载的response的默认方法。它同样必须被实现。parse 主要负责处理 response 并返回处理的数据以及跟进的URL。该方法及其他的Request回调函数必须返回一个包含 Request 及(或) Item 的可迭代的对象。 在 scrapy_demo/sipders/VmoiveSpider 的完整代码如下： 12345678910111213141516171819202122232425262728#!/usr/bin/env python# coding=utf-8import scrapyfrom scrapy_demo.items import ScrapyDemoItemclass VmoiveSpider(scrapy.Spider): # name是spider最重要的属性, 它必须被定义。同时它也必须保持唯一 name = 'vmoive' # 可选定义。包含了spider允许爬取的域名(domain)列表(list) allowed_domains = ['vmovier.com'] start_urls = [ 'http://www.vmovier.com/' ] def parse(self, response): self.log('item page url is ==== ' + response.url) moivelist = response.xpath(\"//li[@class='clearfix']\") for m in moivelist: item = ScrapyDemoItem() item['cover'] = m.xpath('./a/img/@src')[0].extract() item['title'] = m.xpath('./a/@title')[0].extract() item['dec'] = m.xpath(\"./div/div[@class='index-intro']/a/text()\")[0].extract() print(item) yield item 运行程序在项目目录下打开终端，并执行以下命令。我们没有pipelines.py中将爬取结果进行存储，所以我们使用 scrapy 提供的导出数据命令，将 15 条电影信息导出到名为 items.json 文件中。其中 vmoive 为刚才在 VmoiveSpider 中定义的 name 属性的值。 1scrapy crawl vmoive -o items.json 运行的部分结果如下： 12345{'cover': 'http://cs.vmoiver.com/Uploads/cover/2017-09-08/59b25a504e4e0_cut.jpeg@600w_400h_1e_1c.jpg', 'dec': '15年过去了，但我依然有话说', 'title': '灾难反思纪录短片《“911”之殇》' } 深究在阅读上述代码过程中，大家可能会有两个疑问。第一，为什么要在 xpath 方法后面添加[0]？ 第二，为什么要在 [0] 后面添加 extract()方法 ? 请听我慢慢道来。 1) 添加个[0], 因为 xpath() 返回的结果是列表类型。我以获取标题内容为例子讲解不添加[0]会出现什么问题。那么代码则变为 1m.xpath('./a/@title').extract() 运行结果会返回一个列表，而不是文本信息。 1['灾难反思纪录短片《“911”之殇》'] 2）这里涉及到内建选择器 Selecter 的知识。extract()方法的作用是串行化并将匹配到的节点返回一个unicode字符串列表。看了定义，是不是更加懵逼了。那就看下运行结果来压压惊。不加上 extract() 的运行结果如下： 1&lt;Selector xpath='./a/@title' data='灾难反思纪录短片《“911”之殇》'&gt; 进阶上述代码只是在 V电影主页中提取信息，而进入电影详情页面中匹配搜索信息。因此，我们是获取不到电影的播放地址的。如何搞定这难题？我们可以在 parse 方法中做文章。parse() 前文提到它必须返回一个 Reuqest 对象或者 Item。再者， Request 中就包含 url。换句话说，我们只有获取到电影详情页的 url 地址，并在传递给返回的 Request 对象中。 因此，代码可以这么改进： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/usr/bin/env python# coding=utf-8import scrapyfrom scrapy_demo.items import ScrapyDemoItemclass VmoiveSpider(scrapy.Spider): # name是spider最重要的属性, 它必须被定义。同时它也必须保持唯一 name = 'vmoive' # 可选定义。包含了spider允许爬取的域名(domain)列表(list) allowed_domains = ['vmovier.com'] start_urls = [ 'http://www.vmovier.com/' ] def parse(self, response): self.log('item page url is ==== ' + response.url) moivelist = response.xpath(\"//li[@class='clearfix']\") for m in moivelist: item = ScrapyDemoItem() item['cover'] = m.xpath('./a/img/@src')[0].extract() item['title'] = m.xpath('./a/@title')[0].extract() item['dec'] = m.xpath(\"./div/div[@class='index-intro']/a/text()\")[0].extract() # print(item) # 提取电影详细页面 url 地址 urlitem = m.xpath('./a/@href')[0].extract() url = response.urljoin(urlitem) # 如果你想将上面的 item 字段传递给 parse_moive, 使用 meta 参数 yield scrapy.Request(url, callback=self.parse_moive, meta={ 'cover':item['cover'], 'title': item['title'], 'dec': item['dec'], }) def parse_moive(self, response): item = ScrapyDemoItem() item['cover'] = response.meta['cover'] item['title'] = response.meta['title'] item['dec'] = response.meta['dec'] item['playUrl'] = response.xpath(\"//div[@class='p00b204e980']/p/iframe/@src\")[0].extract() yield item 再次运行程序，查看运行结果。 数据持久化在实际生产中，我们很少把数据导出到 json 文件中。因为后期维护、数据查询、数据修改都是一件麻烦的事情。我们通常是将数据保存到数据库中。 我们先定义并创建数据库表 12345678DROP TABLE IF EXISTS vmoiveCREATE TABLE vmoive( id INT(6) NOT NULL AUTO_INCREMENT PRIMARY KEY , cover VARCHAR(255), title VARCHAR(255), mdec VARCHAR(255), playUrl VARCHAR(255)) DEFAULT CHARSET=utf8; 在 settings 文件中增加数据库的配置 123456789101112# Configure item pipelines 这里默认是注释的# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = { 'scrapy_demo.pipelines.ScrapyDemoPipeline': 300, # 保存到 mysql 数据库中}# Mysql 配置信息# 根据你的环境修改MYSQL_HOST = '127.0.0.1'MYSQL_DBNAME = 'vmoive' # 数据库名MYSQL_USER = 'root' # 数据库用户MYSQL_PASSWORD = '123456' # 数据库密码 在 scrapy 中，我们要在 pipeline 文件中编写处理数据存储的代码。 12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*-import pymysqlfrom scrapy_demo import settingsclass ScrapyDemoPipeline(object): def __init__(self,): self.conn = pymysql.connect( host = settings.MYSQL_HOST, db = settings.MYSQL_DBNAME, user = settings.MYSQL_USER, passwd = settings.MYSQL_PASSWORD, charset = 'utf8', # 编码要加上，否则可能出现中文乱码问题 use_unicode = False ) self.cursor = self.conn.cursor() # pipeline 默认调用 def process_item(self, item, spider): # 调用插入数据的方法 self.insertData(item) return item # 插入数据方法 def insertData(self, item): sql = \"insert into vmoive(cover, title, mdec, playUrl) VALUES(%s, %s, %s, %s);\" params = (item['cover'], item['title'], item['dec'], item['playUrl']) self.cursor.execute(sql, params) self.conn.commit()","link":"/917.html"},{"title":"学会运用爬虫框架 Scrapy (三)","text":"上篇文章介绍 Scrapy 框架爬取网站的基本用法。但是爬虫程序比较粗糙，很多细节还需打磨。本文主要是讲解 Scrapy 一些小技巧，能让爬虫程序更加完善。 设置 User-agentScrapy 官方建议使用 User-Agent 池, 轮流选择其中一个常用浏览器的 User-Agent来作为 User-Agent。scrapy 发起的 http 请求中 headers 部分中 User-Agent 字段的默认值是Scrapy/VERSION (+http://scrapy.org)，我们需要修改该字段伪装成浏览器访问网站。 1) 同样在 setting.py 中新建存储 User-Agent 列表, 1234567891011121314151617181920212223242526272829303132333435UserAgent_List = [ \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2224.3 Safari/537.36\", \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\", \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 4.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\", \"Mozilla/5.0 (X11; OpenBSD i386) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1944.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.3319.102 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2309.372 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2117.157 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1866.237 Safari/537.36\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.137 Safari/4E423F\", \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1\", \"Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10; rv:33.0) Gecko/20100101 Firefox/33.0\", \"Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/31.0\", \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20130401 Firefox/31.0\", \"Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20100101 Firefox/31.0\", \"Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16\", \"Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14\", \"Mozilla/5.0 (Windows NT 6.0; rv:2.0) Gecko/20100101 Firefox/4.0 Opera 12.14\", \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0) Opera 12.14\", \"Opera/9.80 (Windows NT 5.1; U; zh-sg) Presto/2.9.181 Version/12.00\"] 2) 在 middlewares.py 文件中新建一个名为RandomUserAgentMiddleware的代理中间层类 123456789101112import randomfrom scrapy_demo.settings import UserAgent_Listclass RandomUserAgentMiddleware(object): '''动态随机设置 User-agent''' def process_request(self, request, spider): ua = random.choice(UserAgent_List) if ua: request.headers.setdefault('User-Agent', ua) print(request.headers) 3) 在 settings.py 中配置 RandomUserAgentMiddleware , 激活中间件 123456789DOWNLOADER_MIDDLEWARES = { # 第二行的填写规则 # yourproject.middlewares(文件名).middleware类 # 设置 User-Agent 'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None, 'scrapy_demo.middlewares.RandomUserAgentMiddleware': 400, # scrapy_demo 是你项目的名称} 禁用cookies有些站点会使用 cookies 来发现爬虫的轨迹。因此，我们最好禁用 cookies 在 settings.py 文件中新增以下配置。 123# 默认是被注释的, 也就是运行使用 cookies# Disable cookies (enabled by default)COOKIES_ENABLED = False 设置下载延迟当 scrapy 的下载器在下载同一个网站下一个页面前需要等待的时间。我们设置下载延迟, 可以有效避免下载器获取到下载地址就立刻执行下载任务的情况发生。从而可以限制爬取速度, 减轻服务器压力。 在 settings.py 文件中新增以下配置。 1234567# 默认是被注释的# Configure a delay for requests for the same website (default: 0)# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay# See also autothrottle settings and docsDOWNLOAD_DELAY = 3 # 单位是秒, 上述设置是延迟 3s。 # 同时还支持设置小数, 例 0.3, 延迟 300 ms 设置代理有些网站设置反爬虫机制，这使得我们的爬虫程序可能爬到一定数量网页就爬取不下去了。我们需要装饰下爬虫，让它访问网站行为更像类人行为。使用 IP 代理池能突破大部分网站的限制。 1) 我们可以通过国内一些知名代理网站(例如：迅代理、西刺代理)获取代理服务器地址。 我将自己收集一些代理地址以列表形式保存到 settings.py 文件中 1234567891011# 代理地址具有一定的使用期限, 不保证以下地址都可用。PROXY_LIST = [ \"https://175.9.77.240:80\", \"http://61.135.217.7:80\", \"http://113.77.101.113:3128\" \"http://121.12.42.180:61234\", \"http://58.246.59.59:8080\", \"http://27.40.144.98:808\", \"https://119.5.177.167:4386\", \"https://210.26.54.43:808\",] 2) 在 middlewares.py 文件中新建一个名为ProxyMiddleware的代理中间层类 12345678910import randomfrom scrapy_demo.settings import PROXY_LISTclass ProxyMiddleware(object): # overwrite process request def process_request(self, request, spider): # Set the location of the proxy # request.meta['proxy'] = \"https://175.9.77.240:80\" request.meta['proxy'] = random.choice(PROXY_LIST) 3) 在 settings.py 文件中增加代理配置： 123456789DOWNLOADER_MIDDLEWARES = { # 第二行的填写规则 # yourproject.middlewares(文件名).middleware类 # 设置代理 'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 110, 'scrapy_demo.middlewares.ProxyMiddleware': 100, # scrapy_demo 是你项目的名称} 除此之外，如果你比较狠的话，可以采用 VPN + Tor 方式来突破反爬虫机制。 减小下载超时如果您对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)， 减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。 在 settings.py 文件中增加配置： 1DOWNLOAD_TIMEOUT = 15 页面跟随规则在爬取网站时，可能一些页面是我们不想爬取的。如果使用 最基本的 Spider，它还是会将这些页面爬取下来。因此，我们需要使用更加强大的爬取类CrawlSpider。 我们的爬取类继承 CrawlSpider，必须新增定义一个 rules 属性。rules 是一个包含至少一个 Rule（爬取规则）对象的 list。 每个 Rule 对爬取网站的动作定义了特定表现。CrawlSpider 也是继承 Spider 类，所以具有Spider的所有函数。 Rule 对象的构造方法如下： 1Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None) 我们在使用 Rule 一般只会用到前面几个参数，它们作用如下： link_extractor： 它是一个 Link Extractor 对象。 其定义了如何从爬取到的页面提取链接。link_extractor既可以自己定义，也可以使用已有LinkExtractor类，主要参数为： allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。 deny：与这个正则表达式(或正则表达式列表)不匹配的 Url 一定不提取。 allow_domains：会被提取的链接的domains。 deny_domains：一定不会被提取链接的domains。 restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。还有一个类似的restrict_css callback：从 link_extractor 中每获取到链接时将会调用该函数。它指定一个回调方法。会返回一个包含 Item 对象的列表。 follow：它 是一个布尔(boolean)值，指定了根据该规则从 response 提取的链接是否需要跟进。 如果 callback 为None， follow 默认设置为 True ，否则默认为 False 。 process_links：从link_extractor中获取到链接列表时将会调用该函数。它同样需要指定一个方法，该方法主要用来过滤 Url。 我以爬取豆瓣电影 Top 250 页面为例子进行讲解如何利用 rules 进行翻页爬取。 在页面的底部，有这样的分页。我们想通过抓取翻页 url 进行下一个页面爬取。 通过分析页面可知，链接的规则是 1https://movie.douban.com/top250?start=当前分页第一个电影序号&amp;filter=分页数 我使用 xpath 来匹配，当然你也可以使用正则表达式或者 CSS 选择器。rules 可以这样定义： 1234567rules = ( Rule(LinkExtractor(allow=(), restrict_xpaths=('//div[@class=\"paginator\"]',)), follow=True, callback='parse_item', process_links='process_links', ),) 完整的 spider 代码如下： 12345678910111213141516171819202122# -*- coding: utf-8 -*-from scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorclass DoubanTop250(CrawlSpider): name = 'movie_douban' allowed_domains = ['douban.com'] start_urls = [\"https://movie.douban.com/top250\"] rules = ( Rule(LinkExtractor(allow=(), restrict_xpaths=('//div[@class=\"paginator\"]',)), follow=True, callback='parse_item', process_links='process_links', ), ) def parse_item(self, response): # 解析 response, 将其转化为 item yield item 另外，LinkExtractor 参数中的 allow() 和 deny() ，我们也是经常使用到。规定爬取哪些页面是否要进行爬取。 7 动态创建Item类对于有些应用，item的结构由用户输入或者其他变化的情况所控制。我们可以动态创建class。 1234567from scrapy.item import DictItem, Fielddef create_item_class(class_name, field_list): fields = { field_name: Field() for field_name in field_list } return type(class_name, (DictItem,), {'fields': fields})","link":"/918.html"},{"title":"学会运用爬虫框架 Scrapy (四)  —— 高效下载图片","text":"爬虫程序爬取的目标通常不仅仅是文字资源，经常也会爬取图片资源。这就涉及如何高效下载图片的问题。这里高效下载指的是既能把图片完整下载到本地又不会对网站服务器造成压力。也许你会这么做，在 pipeline 中自己实现下载图片逻辑。但 Scrapy 提供了图片管道ImagesPipeline，方便我们操作下载图片。 为什么要选用 ImagesPipeline ？ImagesPipeline 具有以下特点： 将所有下载的图片转换成通用的格式（JPG）和模式（RGB） 避免重新下载最近已经下载过的图片 缩略图生成 检测图像的宽/高，确保它们满足最小限制 具体实现定义字段在 item.py 文件中定义我们两个字段image_urls 和images_path 12345678import scrapyclass PicsDownloadItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() image_urls = scrapy.Field() # 图片的下载地址， 该字段是存储图片的列表 image_path = scrapy.Field() # 图片本地存储路径(相对路径) 编写 spider我以爬取 freebuf 首页部分图片为例子讲解。具体代码如下： 12345678910111213141516171819202122import scrapyfrom pics_download.items import PicsDownloadItemclass freebuf_pic_spider(scrapy.Spider): name = 'freebuf' allowed_domains = ['freebuf.com'] start_urls = [ 'http://www.freebuf.com/' ] def parse(self, response): self.log(response.headers) # 获取 freebuf 首页所有的图片, 以列表形式保存到 image_urls 字段中。 piclist = response.xpath(\"//div[@class='news-img']/a/img/@src\").extract() if piclist: item = PicsDownloadItem() item['image_urls'] = piclist yield item 实现 Pipeline我新建一个名为PicsDownloadPipeline的类。需要注意一点的是： Scrapy 默认生成的类是继承Object， 要将该类修改为继承ImagesPipeline。然后实现get_media_requests和item_completed这两个函数。 get_media_requests(item, info) ImagePipeline 根据 image_urls 中指定的 url 进行爬取，可以通过 get_media_requests 为每个 url 生成一个 Request。具体实现如下： 123def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) item_completed(self, results, item, info) 当一个单独项目中的所有图片请求完成时，该方法会被调用。处理结果会以二元组的方式返回给 item_completed() 函数。这个二元组定义如下：(success, image_info_or_failure)其中，第一个元素表示图片是否下载成功；第二个元素是一个字典，包含三个属性： 1) url - 图片下载的url。这是从 get_media_requests() 方法返回请求的url。2) path - 图片存储的路径（类似 IMAGES_STORE）3) checksum - 图片内容的 MD5 hash 具体实现如下： 1234567def item_completed(self, results, item, info): # 将下载的图片路径（传入到results中）存储到 image_paths 项目组中，如果其中没有图片，我们将丢弃项目: image_path = [x['path'] for ok, x in results if ok] if not image_path: raise DropItem(\"Item contains no images\") item['image_path'] = image_path return item 综合起来，PicsDownloadPipeline 的实现下载图片逻辑的代码如下： 1234567891011121314151617import scrapyfrom scrapy.exceptions import DropItemfrom scrapy.pipelines.images import ImagesPipelineclass PicsDownloadPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) def item_completed(self, results, item, info): # 将下载的图片路径（传入到results中）存储到 image_paths 项目组中，如果其中没有图片，我们将丢弃项目: image_path = [x['path'] for ok, x in results if ok] if not image_path: raise DropItem(\"Item contains no images\") item['image_path'] = image_path return item 配置设置在 setting.py 配置存放图片的路径以及自定义下载的图片管道。 123456789# 设置存放图片的路径IMAGES_STORE = 'D:\\\\freebuf'# 配置自定义下载的图片管道， 默认是被注释的ITEM_PIPELINES = { # 第二行的填写规则 # yourproject.middlewares(文件名).middleware类 'pics_download.pipelines.PicsDownloadPipeline': 300, # pics_download 是你项目的名称} 运行程序在 Scrapy 项目的根目录下，执行以下命令： 1scrapy crawl freebuf # freebuf 是我们在 spider 定义的 name 属性 如果你使用的 Python 版本是 3.x 的，可能会报出以下的错误。 123 File \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\pipelines\\images.py\", line 15, in &lt;module&gt; from PIL import ImageModuleNotFoundError: No module named 'PIL' 这是因为 Scrapy 框架用到这个Python Imaging Library (PIL)图片加载库，但是这个库只支持 2.x 版本，所以会运行出错。对于使用 Python 3.x 版本的我们，难道就束手无策？Scrapy 的开发者建议我们使用更好的图片加载库Pillow。为什么说更好呢？一方面是兼容了 PIL，另一方面在该库支持生成缩略图。 因此，我们安装 Pillow 就能解决运行报错的问题。具体安装 Pillow命令如下： 1234pip install pillow # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install pillow 安装之后，重新运行爬虫程序。Scrapy 会运行结果中显示我们定义的image_urls 和images_path字段。 运行结果我们会发现在 D 盘有个名为freebuf的文件夹。在该文件夹中有个full文件夹，里面存放我们刚才爬取到的图片。 如果有在 setting.py 文件中设置生成缩略图。 1234IMAGES_THUMBS = { 'small': (50, 50), # (宽， 高) 'big': (270, 270),} 那么到时候，与full同级的目录下会多出个thumbs文件夹。里面会有两个文件夹small和big，分别对应小分辨率的图片和大分辨率的图片。 ##优化 避免重复下载在 setting.py 中新增以下配置可以避免下载最近已经下载的图片。 12# 90天的图片失效期限IMAGES_EXPIRES = 90 设置该字段，对于已经完成爬取的网站，重新运行爬虫程序。爬虫程序不会重新下载新的图片资源。 自动限速（AutoTrottle）下载图片是比较消耗服务器的资源以及流量。如果图片资源比较大，爬虫程序一直在下载图片。这会对目标网站造成一定的影响。同时，爬虫有可能遭到封杀的情况。 因此，我们有必要对爬虫程序做爬取限速处理。Scrapy 已经为我们提供了AutoTrottle功能。 只要在 setting.py 中开启AutoTrottle功能并配置限速算法即可。我采用默认的配置，具体配置如下： 12345678910# 启用AutoThrottle扩展AUTOTHROTTLE_ENABLED = True# 初始下载延迟(单位:秒)AUTOTHROTTLE_START_DELAY = 5# 在高延迟情况下最大的下载延迟(单位秒)AUTOTHROTTLE_MAX_DELAY = 60# 设置 Scrapy应该与远程网站并行发送的平均请求数, 目前是以1个并发请求数AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0# 启用AutoThrottle调试模式#AUTOTHROTTLE_DEBUG = False 值得注意的是，启用AutoThrottle扩展时，仍然受到DOWNLOAD_DELAY（下载延迟）和CONCURRENT_REQUESTS_PER_DOMAIN（对单个网站进行并发请求的最大值）以及CONCURRENT_REQUESTS_PER_IP（对单个IP进行并发请求的最大值）的约束。","link":"/919.html"},{"title":"学会运用爬虫框架 Scrapy (五)  —— 部署爬虫","text":"本文是 Scrapy 爬虫系列的最后一篇文章。主要讲述如何将我们编写的爬虫程序部署到生产环境中。我们使用由 scrapy 官方提供的爬虫管理工具 scrapyd 来部署爬虫程序。 为什么使用 scrapyd?一是它由 scrapy 官方提供的，二是我们使用它可以非常方便地运用 JSON API来部署爬虫、控制爬虫以及查看运行日志。 使用 scrapyd原理选择一台主机当做服务器，安装并启动 scrapyd 服务。再这之后，scrapyd 会以守护进程的方式存在系统中，监听爬虫地运行与请求，然后启动进程来执行爬虫程序。 安装 scrapyd使用 pip 能比较方便地安装 scrapyd。 1234pip install scrapyd # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install scrapyd 启动 scrapyd在终端命令行下以下命令来启动服务： 1scrapyd 启动服务结果如下： scrapyd 也提供了 web 的接口。方便我们查看和管理爬虫程序。默认情况下 scrapyd 监听 6800 端口，运行 scrapyd 后。在本机上使用浏览器访问 http://localhost:6800/地址即可查看到当前可以运行的项目。 项目部署直接使用 scrapyd-client 提供的 scrapyd-deploy 工具 原理scrapyd 是运行在服务器端，而 scrapyd-client 是运行在客户端。客户端使用 scrapyd-client 通过调用 scrapyd 的 json 接口来部署爬虫项目。 安装 scrapyd-client在终端下运行以下安装命令： 1234pip install scrapyd-client # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install scrapyd-client 配置项目的服务器信息修改工程目录下的 scrapy.cfg 文件。 12345678910111213# Automatically created by: scrapy startproject## For more information about the [deploy] section see:# https://scrapyd.readthedocs.org/en/latest/deploy.html[settings]default = demo.settings # test 为 project 的名称, 默认创建的[deploy:server]# 默认情况下并没有 server，它只是为服务器指定一个名字url = http://localhost:6800/ # 这是部署到本地, 如果你使用其他机器远程部署程序, 需将本地地址换成服务器的 IP 地址。默认是注释的project = test # test 为 project 的名称, 默认创建的 如果你服务器有配置 HTTP basic authentication 验证，那么需要在 scrapy.cfg 文件增加用户名和密码。这是用于登录服务器用的。 123456789[settings]default = demo.settings # demo 为 project 的名称, 默认创建的[deploy:server]# 默认情况下并没有 server，它只是为服务器指定一个名字url = http://192.168.161.129:6800/ # 这是部署到远程服务器project = demo # demo 为 project 的名称, 默认创建的username = monkeypassword = 123456 # 如果不需要密码可以不写 部署爬虫程序在爬虫项目根目录下执行下面的命令: 1scrapyd-deploy &lt;target&gt; -p &lt;project&gt; 其中 target 为上一步配置的服务器名称，project 为项目名称，可以根据实际情况自己指定。 我指定 target 为 server，project 为 demo，所以我要执行的命令如下： 1scrapyd-deploy server -p demo 部署操作会打包你的当前项目，如果当前项目下有setup.py文件，就会使用它，没有的会就会自动创建一个。(如果后期项目需要打包的话，可以根据自己的需要修改里面的信息，也可以暂时不管它). 从返回的结果里面，我们可以看到部署的状态，项目名称，版本号和爬虫个数，以及当前的主机名称. 运行结果如下： 12345$ scrapyd-deploy server -p demoPacking version 1507376760Deploying to project \"demo\" in http://localhost:6800/addversion.jsonServer response (200):{\"status\": \"ok\", \"project\": \"demo\", \"version\": \"1507376760\", \"spiders\": 1, \"node_name\": \"james-virtual-machine\"} 使用以下命令检查部署爬虫结果： 1scrapyd-deploy -l 服务器名称 我指定服务器名称为 server，所以要执行命令如下： 123$ scrapyd-deploy -L severdefaultdemo 刷新 http://localhost:6800/ 页面, 也可以看到Available projects: demo的字样。 使用 API 管理爬虫scrapyd 的 web 界面比较简单，主要用于监控，所有的调度工作全部依靠接口实现。官方推荐使用 curl 来管理爬虫。所以要先安装 curl。 windows 用户可以到该网站https://curl.haxx.se/download.html下载 curl 安装包进行安装。 ubuntu/Mac 用户直接使用命令行安装即可。 开启爬虫 schedule在爬虫项目的根目录下，使用终端运行以下命令： 1curl http://localhost:6800/schedule.json -d project=demo -d spider=demo_spider 成功启动爬虫结果如下： 12curl http://localhost:6800/schedule.json -d project=tutorial -d spider=tencent{\"status\": \"ok\", \"jobid\": \"94bd8ce041fd11e6af1a000c2969bafd\", \"node_name\": \"james-virtual-machine\"} 取消爬虫1curl http://localhost:6800/cancel.json -d project=demo -d job=94bd8ce041fd11e6af1a000c2969bafd 列出项目1curl http://localhost:6800/listprojects.json 列出爬虫、版本、job 信息1curl http://localhost:6800/listspiders.json?project=demo 删除爬虫项目1curl http://localhost:6800/delproject.json -d project=demo","link":"/1020.html"},{"title":"爬虫与反爬虫的博弈","text":"今天猴哥给大家说说爬虫与反爬虫的博弈。 前言近来这两三个月，我陆续将自己学到的爬虫技术分享出来。以标准网络库 urllib 的用法起笔，接着介绍各种内容提供工具，再到后续的 scrapy 爬虫框架系列。我的爬虫分享之旅已经接近尾声了。本文就来聊聊如何防止爬虫被 ban 以及如何限制爬虫。 介绍我们编写的爬虫在爬取网站的时候，要遵守 robots 协议，爬取数据做到“盗亦有道”。在爬取数据的过程中，不要对网站的服务器造成压力。尽管我们做到这么人性化。对于网络维护者来说，他们还是很反感爬虫的。因为爬虫的肆意横行意味着自己的网站资料泄露，甚至是自己刻意隐藏在网站的隐私的内容也会泄露。所以，网站维护者会运用各种方法来拦截爬虫。 攻防战 场景一 防：检测请求头中的字段，比如：User-Agent、referer等字段。 攻：只要在 http 请求的 headers 中带上对于的字段即可。下图中的七个字段被大多数浏览器用来初始化所有网络请求。建议将以下所有字段都带上。 场景二 防：后台对访问的 IP 进行统计，如果单个 IP 访问超过设定的阈值，给予封锁。虽然这种方法效果还不错， 但是其实有两个缺陷。 一个是非常容易误伤普通用户， 另一个就是 IP 其实不值钱， 各种代理网站都有出售大量的 IP 代理地址。 所以建议加大频率周期,每小时或每天超过一定次数屏蔽 IP 一段时间（不提示时间）。 攻：针对这种情况，可通过使用代理服务器解决。同时，爬虫设置下载延迟，每隔几次请求，切换一下所用代理的IP地址。 场景三 防：后台对访问进行统计， 如果单个 userAgent 访问超过阈值， 予以封锁。这种方法拦截爬虫效果非常明显，但是杀伤力过大，误伤普通用户概率非常高。所以要慎重使用。攻：收集大量浏览器的 userAgent 即可。 场景四 防：网站对访问有频率限制，还设置验证码。增加验证码是一个既古老又相当有效果的方法。能够让很多爬虫望风而逃。而且现在的验证码的干扰线, 噪点都比较多，甚至还出现了人类肉眼都难以辨别的验证码（12306 购票网站）。攻：python+tesseract 验证码识别库模拟训练，或使用类似 tor 匿名中间件（广度遍历IP） 场景五 防：网站页面是动态页面，采用 Ajax 异步加载数据方式来呈现数据。这种方法其实能够对爬虫造成了绝大的麻烦。 攻：首先用 Firebug 或者 HttpFox 对网络请求进行分析。如果能够找到 ajax 请求，也能分析出具体的参数和响应的具体含义。则直接模拟相应的http请求，即可从响应中得到对应的数据。这种情况，跟普通的请求没有什么区别。 能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把 ajax 请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求，请看场景六。 场景六 防：基于 JavaScript 的反爬虫手段，主要是在响应数据页面之前，先返回一段带有JavaScript 代码的页面，用于验证访问者有无 JavaScript 的执行环境，以确定使用的是不是浏览器。例如淘宝、快代理这样的网站。 这种反爬虫方法。通常情况下，这段JS代码执行后，会发送一个带参数key的请求，后台通过判断key的值来决定是响应真实的页面，还是响应伪造或错误的页面。因为key参数是动态生成的，每次都不一样，难以分析出其生成方法，使得无法构造对应的http请求。 攻：采用 selenium+phantomJS 框架的方式进行爬取。调用浏览器内核，并利用phantomJS 执行 js 来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。","link":"/1021.html"},{"title":"Python 绘图,我只用 Matplotlib(一)","text":"当我们的爬虫程序已经完成使命，帮我们抓取大量的数据。你内心也许会空落落的。或许你会疑惑，自己抓取这些数据有啥用？如果要拿去分析，那要怎么分析呢？ 说到数据分析，Python 完全能够胜任这方面的工作。Python 究竟如何在数据分析领域做到游刃有余？因为它有“四板斧”，分别是Matplotlib、NumPy、SciPy/Pandas。Matplotlib 是画图工具，NumPy 是矩阵运算库，SciPy 是数学运算工具，Pandas 是数据处理的工具。 为什么选择 Matplotlib？Python 有很多强大的画图库，为什么我偏偏独爱 Maplotlib？我先买个关子，先来看看还有哪些库。 SeabornSeaborn 是一个基于 Matplotlib 的高级可视化效果库， 偏向于统计作图。因此，针对的点主要是数据挖掘和机器学习中的变量特征选取。相比 Matplotlib ，它语法相对简化些，绘制出来的图不需要花很多功夫去修饰。但是它绘图方式比较局限，不过灵活。 BokehBokeh 是基于 javascript 来实现交互可视化库，它可以在WEB浏览器中实现美观的视觉效果。但是它也有明显的缺点。其一是版本时常更新，最重要的是有时语法还不向下兼容。这对于我们来说是噩梦。其二是语法晦涩，与 matplotlib做比较，可以说是有过之而无不及。 ggplotggplot 是 yhat 大神基于 R 语言的 ggplot2 制作的 python 版本库。 如果你使用 R 语言的话，ggplot2 可以算是必不可少的工具。所以，很多人都推荐使用该库。不过可惜的是，yhat 大神已经停止维护该库了。 PlotlyPlotly 也是一个做可视化交互的库。它不仅支持 Python 还支持 R 语言。Plotly 的优点是能提供 WEB 在线交互，配色也真心好看。如果你是一名数据分析师，Plotly 强大的交互功能能助你一臂之力完成展示。 MapboxMapbox 使用处理地理数据引擎更强的可视化工具库。如果你需要绘制地理图，那么它值得你信赖。 总之， Python 绘图库众多，各有特点。但是 Maplotlib 是最基础的 Python 可视化库。如果你将学习 Python 数据可视化。那么 Maplotlib 是非学不可，然后再学习其他库做纵横向的拓展。 Matplotlib 能绘制什么图？Matiplotlib 非常强大，所以最基本的图表自然不在话下。例如说：直线图 曲线图 柱状图 直方图 饼图 散点图 只能绘制这些最基础的图？显示是不可能的，还能绘制些高级点的图例如：高级点的柱状图 等高线图 类表格图形 不仅仅只有这些，还能绘制 3D 图形。例如三维柱状图 3D 曲面图 因此，Matplotlib 绘制的图种类能够满足我们做数据分析了。 安装 Matplotlib看到这里，你是否惊叹不已，很很迫不及待地想学习 Matplotlib。而工欲善其事，必先利其器。我们先来学习如何安装 Matplotlib。其实也是很简单，我们借助 pip 工具来安装。 在终端执行以下命令来安装 Matplotlib 1234pip install Matplotlib # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install Matplotlib","link":"/1022.html"},{"title":"Python 绘图,我只用 Matplotlib(二)","text":"上篇文章，我们了解到 Matplotlib 是一个风格类似 Matlab 的基于 Python 的绘图库。它提供了一整套和matlab相似的命令API，十分适合交互式地进行制图。而且我们也可以方便地将它作为绘图控件，嵌入GUI应用程序中。本文主要走进 Matplotlib 的世界，初步学会绘制图形。 基础知识在学习绘制之前，先来了解下 Matplotlib 基础概念。 库我们绘制图形主要用到两个库，matplotlib.pyplot和numpy。在编码过程中，这两个库的使用频率较高，而这两个库的名字较长。这难免会给我们带来不便。所以我们一般给其设置别名， 大大减少重复性工作量。具体如下代码： 12import matplotlib.pyplot as plt # 导入模块 matplotlib.pyplot，并简写成 plt import numpy as np # 导入模块 numpy，并简写成 np numpy 是 Python 用于数学运算的库，它是在安装 matplotlib 时候顺带安装的。pyplot 是 matplotlib 一个子模块，主要为底层的面向对象的绘图库提供状态机界面。状态机隐式地自动创建数字和坐标轴以实现所需的绘图。 matplotlib 中的所有内容都按照层次结果进行组织。顶层就是由 pyplot 提供的 matplotlib “状态机环境”。基于这个状态机环境，我们就可以创建图形。 图形组成标签我在 matplotlib 官网上找图像组件说明图并在上面增加中文翻译。通过这张图，我们对 matplotlib 整体地认识。 接下来，我主要讲解 matplotlib 中几个重要的标签。 Figure Figure 翻译成中文是图像窗口。Figure 是包裹 Axes、tiles、legends 等组件的最外层窗口。它其实是一个 Windows 应用窗口 。Figure 中最主要的元素是 Axes（子图）。一个 Figure 中可以有多个子图，但至少要有一个能够显示内容的子图。 Axes Axes 翻译成中文是轴域/子图。Axes 是带有数据的图像区域。从上文可知，它是位于 Figure 里面。那它和 Figure 是什么关系？这里可能文字难以表述清楚，我以图说文。用两图带你彻底弄清它们的关系。 在看运行结果之前，我先呈上代码给各位看官品尝。 12345fig = plt.figure() # 创建一个没有 axes 的 figurefig.suptitle('No axes on this figure') # 添加标题以便我们辨别fig, ax_lst = plt.subplots(2, 2) # 创建一个以 axes 为单位的 2x2 网格的 figure plt.show() 根据运行结果图，我们不难看出。左图的 Figure1 中没有 axes，右图的 Figure2 中有 4 个 axes。因此，我们可以将 Axes 理解为面板，而面板是覆在窗口(Figure) 上。 Axis Axis 在中文的意思是轴。官网文档对 Axis 定义解释不清楚，让我们看得云里雾里的。如果你有留意前文的组成说明图，可以看到 X Axis 和 Y Axis 的字样。按照平常人的见识， 观察该图就能明白 Axis 是轴的意思。此外，Axis 和 Axes 以及 Figure 这三者关系，你看完下图，会恍然大悟。 绘制第一张图按照剧本发展，我接下来以绘制曲线并逐步美化它为例子，一步步讲解如何绘制图形。在这过程中，我也会逐一说明各个函数的作用。 初步绘制曲线12345678910import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-2, 6, 50)y1 = x + 3 # 曲线 y1y2 = 3 - x # 曲线 y2plt.figure() # 定义一个图像窗口plt.plot(x, y1) # 绘制曲线 y1plt.plot(x, y2) # 绘制曲线 y2plt.show() 调用np.linspace是创建一个 numpy 数组，并记作 x。x 包含了从 -2 到 6 之间等间隔的 50 个值。y1 和 y2 则分别是这 50 个值对应曲线的函数值组成的 numpy 数组。前面的操作还处于设置属性的阶段，还没有开始绘制图形。plt.figure() 函数才意味着开始执行绘图操作。最后别忘记调用show()函数将图形呈现出来。 简单修饰我们已经绘制出两条直线，但样式比较简陋。所以我给两条曲线设置鲜艳的颜色、线条类型。同时，还给纵轴和横轴的设置上下限，增加可观性。 123456789101112131415161718192021222324import matplotlib.pyplot as pltimport numpy as np# 创建一个点数为 8 x 6 的窗口, 并设置分辨率为 80像素/每英寸plt.figure(figsize=(8, 6), dpi=80)# 再创建一个规格为 1 x 1 的子图plt.subplot(1，1，1)x = np.linspace(-2, 6, 50)y1 = x + 3 # 曲线 y1y2 = 3 - x # 曲线 y2# 绘制颜色为蓝色、宽度为 1 像素的连续曲线 y1plt.plot(x, y1, color=\"blue\", linewidth=1.0, linestyle=\"-\")# 绘制颜色为紫色、宽度为 2 像素的不连续曲线 y2plt.plot(x, y2, color=\"#800080\", linewidth=2.0, linestyle=\"--\")# 设置横轴的上下限plt.xlim(-1, 6)# 设置纵轴的上下限plt.ylim(-2, 10)plt.show() 设置纵横轴标签在图像中，我们不能一味地认为横轴就是 X 轴，纵轴就是 Y 轴。图形因内容数据不同，纵横轴标签往往也会不同。这也体现了给纵横轴设置标签说明的重要性。 1234567...# 设置横轴标签plt.xlabel(\"X\")# 设置纵轴标签plt.ylabel(\"Y\")plt.show() 设置精准刻度matplotlib 画图设置的刻度是由曲线以及窗口的像素点等因素决定。这些刻度精确度无法满足需求，我们需要手动添加刻度。上图中，纵轴只显示 2 的倍数的刻度，横轴只显示 1 的倍数的刻度。我们为其添加精准刻度，纵轴变成单位间隔为 1 的刻度，横轴变成单位间隔为 0.5 的刻度。 1234567...# 设置横轴精准刻度plt.xticks([-1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5])# 设置纵轴精准刻度plt.yticks([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])plt.show() xticks() 和 yticks() 需要传入一个列表作为参数。 该方法默认是将列表的值来设置刻度标签，如果你想重新设置刻度标签，则需要传入两个列表参数给 xticks() 和 yticks() 。第一个列表的值代表刻度，第二个列表的值代表刻度所显示的标签。 12345678...# 设置横轴精准刻度plt.xticks([-1, 0, 1, 2, 3, 4, 5, 6], [\"-1m\", \"0m\", \"1m\", \"2m\", \"3m\", \"4m\", \"5m\", \"6m\"])# 设置纵轴精准刻度plt.yticks([-2, 0, 2, 4, 6, 8, 10], [\"-2m\", \"0m\", \"2m\", \"4m\", \"6m\", \"8m\", \"10m\"])plt.show() 添加图例如果需要在图的左上角添加一个图例。我们只需要在 plot() 函数里以「键 - 值」的形式增加一个参数。首先我们需要在绘制曲线的时候，增加一个 label 参数，然后再调用 plt.legend() 绘制出一个图例。plt.legend() 需要传入一个位置值。loc 的值可选如下： 值 说明 best 自动选择最佳位置，默认是左上 upper right 右上 upper left 左上 lower right 右下 lower left 左下 right 右边，默认是右上。如果因图形挡住右上，会自动往下选择空白地方绘制 center right 垂直居中且靠右 center left 垂直居中且靠左 lower center 垂直居中且靠底部 upper center 垂直居中且靠顶部 center 居中 1234567...# 绘制颜色为蓝色、宽度为 1 像素的连续曲线 y1plt.plot(x, y1, color=\"blue\", linewidth=1.0, linestyle=\"-\", label=\"y1\")# 绘制颜色为紫色、宽度为 2 像素的不连续曲线 y2plt.plot(x, y2, color=\"#800080\", linewidth=2.0, linestyle=\"--\", label=\"y2\")plt.legend(loc=\"upper left\")... 注释特殊点位有时某些数据点非常关键，需要突显出来。我们需要将改点绘制出来，即绘制散点图，再对其做注释。实现上述需求，我们要用到scatter()和annotate()函数。scatter() 是用于绘制散图，这里我们只是用其来绘制单个点。scatter() 用法，后续文章会详细对其用法做说明。annotate()则是添加标注 。 scatter() 函数必须传入两个参数 x 和 y。值得注意得是，它们的数据类型是列表。x 代表要标注点的横轴位置，y 代表要标注点的横轴位置。x 和 y 列表中下标相同的数据是对应的。例如 x 为 [3, 4]，y 为 [6, 8]，这表示会绘制点（3，6），（4， 8）。因此，x 和 y 长度要一样。 annotate函数同样也有两个必传参数，一个是标注内容，另一个是 xy。标注内容是一个字符串。xy 表示要在哪个位置（点）显示标注内容。xy 位置地选定。一般是在scatter() 绘制点附近，但不建议重合，这样会影响美观。 1234567891011121314151617181920...# 绘制颜色为蓝色、宽度为 1 像素的连续曲线 y1plt.plot(x, y1, color=\"blue\", linewidth=1.0, linestyle=\"-\", label=\"y1\")# 绘制散点(3, 6)plt.scatter([3], [6], s=30, color=\"blue\") # s 为点的 size# 对(3, 6)做标注plt.annotate(\"(3, 6)\", xy=(3.3, 5.5), # 在(3.3, 5.5)上做标注 fontsize=16, # 设置字体大小为 16 xycoords='data') # xycoords='data' 是说基于数据的值来选位置# 绘制颜色为紫色、宽度为 2 像素的不连续曲线 y2plt.plot(x, y2, color=\"#800080\", linewidth=2.0, linestyle=\"--\", label=\"y2\")# 绘制散点(3, 0)plt.scatter([3], [0], s=50, color=\"#800080\")# 对(3, 0)做标注plt.annotate(\"(3, 0)\", xy=(3.3, 0), # 在(3.3, 0)上做标注 fontsize=16, # 设置字体大小为 16 xycoords='data') # xycoords='data' 是说基于数据的值来选位置 点已经被标注出来了，如果你还想给点添加注释。这需要使用text()函数。text(x，y，s) 作用是在点(x，y) 上添加文本 s。matplotlib 目前好像对中午支持不是很友好， 中文均显示为乱码。 1234567···# 绘制散点(3, 0)plt.scatter([3], [0], s=50, color=\"#800080\")# 对(3, 0)做标注plt.annotate(\"(3, 0)\", xy=(3.3, 0))plt.text(4, -0.5, \"this point very important\", fontdict={'size': 12, 'color': 'green'}) # fontdict设置文本字体 到此为止，我们基本上完成了绘制直线所有工作。Matplotlib 能绘制种类繁多且绘图功能强大，所以我接下来的文章将单独对每种类型图做分享讲解。","link":"/1123.html"},{"title":"彻底理解Iterable、Iterator、generator","text":"本文介绍猴哥对于Python中的Iterable、Iterator、generator的理解。 Iterable我们一般称Iterable为可迭代对象。Python 中任意的对象，只要它定义了可以返回一个迭代器的__iter__方法，或者定义了可以支持下标索引的__getitem__方法，那么它就是一个可迭代对象。我们常用到的集合数据类型都是 Iterable。例如列表（list）、元组（tuple）、字典（dict）、集合（set）、字符串（str）等。 我定义了一个列表 numlist，打印出该列表的方法。 1234numlist = [1, 2, 3]print(numlist)print(numlist.__iter__) # 调用__iter__方法print(numlist.__getitem__) # 调用__getitem__方法 运行结果如下： 根据运行结果，我们可知列表就是个可迭代对象。Python 的collections库有个isinstance()函数。可以用来判断一个对象是否是 Iterable 对象。 12345from collections import Iterable isinstance({}, Iterable) isinstance((), Iterable) isinstance(999, Iterable) 运行结果为： 如果我们每次都要使用这个函数来判断一个对象是否为可迭代对象，这样操作有点麻烦。有没有快速判定的方法呢？答案是肯定的。可以直接使用 for 循环进行遍历的对象就是可迭代对象。 除此之外，generator(生成器) 和带 yield 的 generator function 也是可迭代的对象。 IteratorIterator是迭代器的意思。任意对象，只要定义了next()（Python 2 版本）或者__next__()（Python 3 版本） 方法，那么它就是一个迭代器。迭代器中还有另一个函数__iter__()，它和 next() 方法形成迭代器协议。 iter()返回主要是返回迭代器对象本身，即return self。如果你自己定义个迭代器，实现该函数就能使用for ... in ...语句遍历了。 next()获取容器中的下一个元素，当没有可访问元素后，就抛出StopIteration异常。 遍历迭代器有两个方式。一种是使用 next() 函数；另一种则是使用 for each 循环，本质上就是通过不断调用 next() 函数实现的。 1234567891011121314151617181920from collections import Iteratornumlist = [1, 2, 3]# 将数组转化为迭代器ite1 = iter(numlist)print(ite1)for i in ite1: print(i)print(\"=========\")ite2 = iter(numlist)while True: try: num = ite2.__next__() print(num) except StopIteration: break 值得注意的是一个 Iterator 只能遍历一次。 generatorgenerator 翻译成中文是生成器。生成器也是一种特殊迭代器。它其实是生成器函数返回生成器的迭代，“生成器的迭代器”这个术语通常被称作”生成器”。yield 是生成器实现__next__()方法的关键。它作为生成器执行的暂停恢复点，可以对 yield 表达式进行赋值，也可以将 yield 表达式的值返回。任何包含 yield 语句的函数被称为生成器。 yield是一个语法糖，内部实现支持了迭代器协议，同时yield内部是一个状态机，维护着挂起和继续的状态。 个人认为，生成器算是 Python 非常棒的特性。它的出现能帮助大大节省些内存空间。假如我们要生成从 1 到 10 这 10 个数字，采用列表的方式定义，会占用 10 个地址空间。采用生成器，只会占用一个地址空间。因为生成器并没有把所有的值存在内存中，而是在运行时生成值。所以生成器只能访问一次。 创建一个从包含 1 到 10 的生成器的例子。 1234gen = (i for i in range(10))print(gen)for i in gen: print(i) 运行结果如下： 带有 yield 关键字 的例子。重点关注运行结果，这能让你对 yield 有更深的认识。 12345678910111213def testYield(n): for i in range(n): print(\"当前值: \", i) yield doubeNumber(i) print(\"第 \", i, \" 次运行\") print(\"testYield 运行结束\")def doubeNumber(i): return i*2 if __name__ == '__main__': for i in testYield(3): print(i, \"===\", i) 运行结果如下：","link":"/1124.html"},{"title":"Python 绘图,我只用 Matplotlib(三)","text":"上篇文章，我已经讲解绘制图像大致步骤，接下来的系列文章将分别对各种图形做讲解。其实就是了解各个图种的绘图 API。文章就讲解第一种图形，柱状图。 基础绘制柱状图，我们主要用到bar()函数。只要将该函数理解透彻，我们就能绘制各种类型的柱状图。 我们先看下bar()的构造函数：bar(x，height， width，*，align='center'，**kwargs) x包含所有柱子的下标的列表 height包含所有柱子的高度值的列表 width每个柱子的宽度。可以指定一个固定值，那么所有的柱子都是一样的宽。或者设置一个列表，这样可以分别对每个柱子设定不同的宽度。 align柱子对齐方式，有两个可选值：center和edge。center表示每根柱子是根据下标来对齐, edge则表示每根柱子全部以下标为起点，然后显示到下标的右边。如果不指定该参数，默认值是center。 其他可选参数有： color每根柱子呈现的颜色。同样可指定一个颜色值，让所有柱子呈现同样颜色；或者指定带有不同颜色的列表，让不同柱子显示不同颜色。 edgecolor每根柱子边框的颜色。同样可指定一个颜色值，让所有柱子边框呈现同样颜色；或者指定带有不同颜色的列表，让不同柱子的边框显示不同颜色。 linewidth每根柱子的边框宽度。如果没有设置该参数，将使用默认宽度，默认是没有边框。 tick_label每根柱子上显示的标签，默认是没有内容。 xerr每根柱子顶部在横轴方向的线段。如果指定一个固定值，所有柱子的线段将一直长；如果指定一个带有不同长度值的列表，那么柱子顶部的线段将呈现不同长度。 yerr每根柱子顶端在纵轴方向的线段。如果指定一个固定值，所有柱子的线段将一直长；如果指定一个带有不同长度值的列表，那么柱子顶部的线段将呈现不同长度。 ecolor设置 xerr 和 yerr 的线段的颜色。同样可以指定一个固定值或者一个列表。 capsize这个参数很有趣, 对xerr或者yerr的补充说明。一般为其设置一个整数，例如 10。如果你已经设置了yerr 参数，那么设置 capsize 参数，会在每跟柱子顶部线段上面的首尾部分增加两条垂直原来线段的线段。对 xerr 参数也是同样道理。可能看说明会觉得绕，如果你看下图就一目了然了。 error_kw设置 xerr 和 yerr 参数显示线段的参数，它是个字典类型。如果你在该参数中又重新定义了 ecolor 和 capsize，那么显示效果以这个为准。 log这个参数，我暂时搞不懂有什么用。 orientation设置柱子是显示方式。设置值为 vertical ，那么显示为柱形图。如果设置为 horizontal 条形图。不过 matplotlib 官网不建议直接使用这个来绘制条形图，使用barh来绘制条形图。 下面我就调用 bar 函数绘制一个最简单的柱形图。 123456789101112131415161718192021222324252627282930313233343536373839import matplotlib.pyplot as pltimport numpy as np# 创建一个点数为 8 x 6 的窗口, 并设置分辨率为 80像素/每英寸plt.figure(figsize=(8, 6), dpi=80)# 再创建一个规格为 1 x 1 的子图plt.subplot(1, 1, 1)# 柱子总数N = 6# 包含每个柱子对应值的序列values = (25, 32, 34, 20, 41, 50)# 包含每个柱子下标的序列index = np.arange(N)# 柱子的宽度width = 0.35# 绘制柱状图, 每根柱子的颜色为紫罗兰色p2 = plt.bar(index, values, width, label=\"rainfall\", color=\"#87CEFA\")# 设置横轴标签plt.xlabel('Months')# 设置纵轴标签plt.ylabel('rainfall (mm)')# 添加标题plt.title('Monthly average rainfall')# 添加纵横轴的刻度plt.xticks(index, ('Jan', 'Fub', 'Mar', 'Apr', 'May', 'Jun'))plt.yticks(np.arange(0, 81, 10))# 添加图例plt.legend(loc=\"upper right\")plt.show() 运行结果为： 进阶bar 函数的参数很多，你可以使用这些参数绘制你所需要柱形图的样式。如果你还不会灵活使用这样参数，那就让我们来学习 matplotlib 官方提供的例子。 123456789101112131415161718192021222324252627282930313233343536373839404142# Credit: Josh Hemannimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib.ticker import MaxNLocatorfrom collections import namedtuplen_groups = 5means_men = (20, 35, 30, 35, 27)std_men = (2, 3, 4, 1, 2)means_women = (25, 32, 34, 20, 25)std_women = (3, 5, 2, 3, 3)fig, ax = plt.subplots()index = np.arange(n_groups)bar_width = 0.35opacity = 0.4error_config = {'ecolor': '0.3'}rects1 = ax.bar(index, means_men, bar_width, alpha=opacity, color='b', yerr=std_men, error_kw=error_config, label='Men')rects2 = ax.bar(index + bar_width, means_women, bar_width, alpha=opacity, color='r', yerr=std_women, error_kw=error_config, label='Women')ax.set_xlabel('Group')ax.set_ylabel('Scores')ax.set_title('Scores by group and gender')ax.set_xticks(index + bar_width / 2)ax.set_xticklabels(('A', 'B', 'C', 'D', 'E'))ax.legend()fig.tight_layout()plt.show() 运行结果如下： 开动你的大脑，想想还能绘制出什么样式的柱形图。","link":"/1125.html"},{"title":"Python定时任务(上)","text":"在项目中，我们可能遇到有定时任务的需求。其一：定时执行任务。例如每天早上 8 点定时推送早报。其二：每隔一个时间段就执行任务。比如：每隔一个小时提醒自己起来走动走动，避免长时间坐着。今天，我跟大家分享下 Python 定时任务的实现方法。 ##第一种办法是最简单又最暴力。那就是在一个死循环中，使用线程睡眠函数 sleep()。 12345678910111213from datetime import datetimeimport time'''每个 10 秒打印当前时间。'''def timedTask(): while True: print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) time.sleep(10)if __name__ == '__main__': timedTask() 这种方法能够执行固定间隔时间的任务。如果timedTask()函数之后还有些操作，我们还使用死循环 + 阻塞线程。这会使得timedTask()一直占有 CPU 资源，导致后续操作无法执行。我建议谨重使用。 ##既然第一种方法暴力，那么有没有比较优雅地方法？答案是肯定的。Python 标准库 threading 中有个 Timer 类。它会新启动一个线程来执行定时任务，所以它是非阻塞函式。 如果你有使用多线程的话，需要关心线程安全问题。那么你可以选使用threading.Timer模块。 123456789101112131415161718192021222324from datetime import datetimefrom threading import Timerimport time'''每个 10 秒打印当前时间。'''def timedTask(): ''' 第一个参数: 延迟多长时间执行任务(单位: 秒) 第二个参数: 要执行的任务, 即函数 第三个参数: 调用函数的参数(tuple) ''' Timer(10, task, ()).start()# 定时任务def task(): print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))if __name__ == '__main__': timedTask() while True: print(time.time()) time.sleep(5) 运行结果： 12341512486945.11963751512486950.1198732017-12-05 23:15:501512486955.133385 ##第三种方式是也是使用标准库中sched模块。sched 是事件调度器，它通过 scheduler 类来调度事件，从而达到定时执行任务的效果。 sched库使用起来也是非常简单。1）首先构造一个sched.scheduler类它接受两个参数：timefunc 和 delayfunc。timefunc 应该返回一个数字，代表当前时间，delayfunc 函数接受一个参数，用于暂停运行的时间单元。 一般使用默认参数就行，即传入这两个参数 time.time 和 time.sleep.当然，你也可以自己实现时间暂停的函数。 2）添加调度任务scheduler 提供了两个添加调度任务的函数: enter(delay, priority, action, argument=(), kwargs={}) 该函数可以延迟一定时间执行任务。delay 表示延迟多长时间执行任务，单位是秒。priority为优先级，越小优先级越大。两个任务指定相同的延迟时间，优先级大的任务会向被执行。action 即需要执行的函数，argument 和 kwargs 分别是函数的位置和关键字参数。 scheduler.enterabs(time, priority, action, argument=(), kwargs={}) 添加一项任务，但这个任务会在 time 这时刻执行。因此，time 是绝对时间.其他参数用法与 enter() 中的参数用法是一致。 3）把任务运行起来调用 scheduler.run()函数就完事了。 下面是 sche 使用的简单示例： 123456789101112131415161718192021from datetime import datetimeimport schedimport time'''每个 10 秒打印当前时间。'''def timedTask(): # 初始化 sched 模块的 scheduler 类 scheduler = sched.scheduler(time.time, time.sleep) # 增加调度任务 scheduler.enter(10, 1, task) # 运行任务 scheduler.run()# 定时任务def task(): print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))if __name__ == '__main__': timedTask() 值得注意的是： scheduler 中的每个调度任务只会工作一次，不会无限循环被调用。如果想重复执行同一任务， 需要重复添加调度任务即可。","link":"/1226.html"},{"title":"Python定时任务(下)","text":"上篇文章，我们了解到有三种办法能实现定时任务，但是都无法做到循环执行定时任务。因此，需要一个能够担当此重任的库。它就是APScheduler。 简介APScheduler的全称是Advanced Python Scheduler。它是一个轻量级的 Python 定时任务调度框架。APScheduler 支持三种调度任务：固定时间间隔，固定时间点（日期），Linux 下的 Crontab 命令。同时，它还支持异步执行、后台执行调度任务。 安装使用 pip 包管理工具安装 APScheduler 是最方便快捷的。 123pip install APScheduler# 如果出现因下载失败导致安装不上的情况，建议使用代理pip --proxy http://代理ip:端口 install APScheduler 使用步骤APScheduler 使用起来还算是比较简单。运行一个调度任务只需要以下三部曲。 1) 新建一个 schedulers (调度器) 。2) 添加一个调度任务(job stores)。3) 运行调度任务。 下面是执行每 2 秒报时的简单示例代码： 12345678910111213141516171819import datetimeimport timefrom apscheduler.schedulers.background import BackgroundSchedulerdef timedTask(): print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])if __name__ == '__main__': # 创建后台执行的 schedulers scheduler = BackgroundScheduler() # 添加调度任务 # 调度方法为 timedTask，触发器选择 interval(间隔性)，间隔时长为 2 秒 scheduler.add_job(timedTask, 'interval', seconds=2) # 启动调度任务 scheduler.start() while True: print(time.time()) time.sleep(5) 基础组件APScheduler 有四种组件，分别是：调度器(scheduler)，作业存储(job store)，触发器(trigger)，执行器(executor)。 schedulers（调度器）它是任务调度器，属于控制器角色。它配置作业存储器和执行器可以在调度器中完成，例如添加、修改和移除作业。 triggers（触发器）描述调度任务被触发的条件。不过触发器完全是无状态的。 job stores（作业存储器）任务持久化仓库，默认保存任务在内存中，也可将任务保存都各种数据库中，任务中的数据序列化后保存到持久化数据库，从数据库加载后又反序列化。 executors（执行器）负责处理作业的运行，它们通常通过在作业中提交指定的可调用对象到一个线程或者进城池来进行。当作业完成时，执行器将会通知调度器。 schedulers（调度器）我个人觉得 APScheduler 非常好用的原因。它提供 7 种调度器，能够满足我们各种场景的需要。例如：后台执行某个操作，异步执行操作等。调度器分别是： BlockingScheduler : 调度器在当前进程的主线程中运行，也就是会阻塞当前线程。 BackgroundScheduler : 调度器在后台线程中运行，不会阻塞当前线程。 AsyncIOScheduler : 结合 asyncio 模块（一个异步框架）一起使用。 GeventScheduler : 程序中使用 gevent（高性能的Python并发框架）作为IO模型，和 GeventExecutor 配合使用。 TornadoScheduler : 程序中使用 Tornado（一个web框架）的IO模型，用 ioloop.add_timeout 完成定时唤醒。 TwistedScheduler : 配合 TwistedExecutor，用 reactor.callLater 完成定时唤醒。 QtScheduler : 你的应用是一个 Qt 应用，需使用QTimer完成定时唤醒。 triggers（触发器）APScheduler 有三种内建的 trigger:1）date 触发器date 是最基本的一种调度，作业任务只会执行一次。它表示特定的时间点触发。它的参数如下： 参数 说明 run_date (datetime 或 str) 作业的运行日期或时间 timezone (datetime.tzinfo 或 str) 指定时区 date 触发器使用示例如下： 12345678910111213141516from datetime import datetimefrom datetime import datefrom apscheduler.schedulers.background import BackgroundSchedulerdef job_func(text): print(text)scheduler = BackgroundScheduler()# 在 2017-12-13 时刻运行一次 job_func 方法scheduler .add_job(job_func, 'date', run_date=date(2017, 12, 13), args=['text'])# 在 2017-12-13 14:00:00 时刻运行一次 job_func 方法scheduler .add_job(job_func, 'date', run_date=datetime(2017, 12, 13, 14, 0, 0), args=['text'])# 在 2017-12-13 14:00:01 时刻运行一次 job_func 方法scheduler .add_job(job_func, 'date', run_date='2017-12-13 14:00:01', args=['text'])scheduler.start() 2）interval 触发器固定时间间隔触发。interval 间隔调度，参数如下： 参数 说明 weeks (int) 间隔几周 days (int) 间隔几天 hours (int) 间隔几小时 minutes (int) 间隔几分钟 seconds (int) 间隔多少秒 start_date (datetime 或 str) 开始日期 end_date (datetime 或 str) 结束日期 timezone (datetime.tzinfo 或str) 时区 interval 触发器使用示例如下： 12345678910111213import datetimefrom apscheduler.schedulers.background import BackgroundSchedulerdef job_func(text): print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])scheduler = BackgroundScheduler()# 每隔两分钟执行一次 job_func 方法scheduler .add_job(job_func, 'interval', minutes=2)# 在 2017-12-13 14:00:01 ~ 2017-12-13 14:00:10 之间, 每隔两分钟执行一次 job_func 方法scheduler .add_job(job_func, 'interval', minutes=2, start_date='2017-12-13 14:00:01' , end_date='2017-12-13 14:00:10')scheduler.start() 3）cron 触发器 在特定时间周期性地触发，和Linux crontab格式兼容。它是功能最强大的触发器。 我们先了解 cron 参数： 参数 说明 year (int 或 str) 年，4位数字 month (int 或 str) 月 (范围1-12) day (int 或 str) 日 (范围1-31 week (int 或 str) 周 (范围1-53) day_of_week (int 或 str) 周内第几天或者星期几 (范围0-6 或者 mon,tue,wed,thu,fri,sat,sun) hour (int 或 str) 时 (范围0-23) minute (int 或 str) 分 (范围0-59) second (int 或 str) 秒 (范围0-59) start_date (datetime 或 str) 最早开始日期(包含) end_date (datetime 或 str) 最晚结束时间(包含) timezone (datetime.tzinfo 或str) 指定时区 这些参数是支持算数表达式，取值格式有如下： cron 触发器使用示例如下： 1234567891011import datetimefrom apscheduler.schedulers.background import BackgroundSchedulerdef job_func(text): print(\"当前时间：\", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])scheduler = BackgroundScheduler()# 在每年 1-3、7-9 月份中的每个星期一、二中的 00:00, 01:00, 02:00 和 03:00 执行 job_func 任务scheduler .add_job(job_func, 'cron', month='1-3,7-9',day='0, tue', hour='0-3')scheduler.start() 作业存储(job store)该组件是对调度任务的管理。1）添加 job有两种添加方法，其中一种上述代码用到的 add_job()， 另一种则是scheduled_job()修饰器来修饰函数。 这个两种办法的区别是：第一种方法返回一个 apscheduler.job.Job 的实例，可以用来改变或者移除 job。第二种方法只适用于应用运行期间不会改变的 job。 第二种添加任务方式的例子： 123456789import datetimefrom apscheduler.schedulers.background import BackgroundScheduler@scheduler.scheduled_job(job_func, 'interval', minutes=2)def job_func(text): print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])scheduler = BackgroundScheduler()scheduler.start() 2）移除 job移除 job 也有两种方法：remove_job() 和 job.remove()。remove_job() 是根据 job 的 id 来移除，所以要在 job 创建的时候指定一个 id。job.remove() 则是对 job 执行 remove 方法即可 12345scheduler.add_job(job_func, 'interval', minutes=2, id='job_one')scheduler.remove_job(job_one)job = add_job(job_func, 'interval', minutes=2, id='job_one')job.remvoe() 3）获取 job 列表通过 scheduler.get_jobs() 方法能够获取当前调度器中的所有 job 的列表 4) 修改 job如果你因计划改变要对 job 进行修改，可以使用Job.modify() 或者 modify_job()方法来修改 job 的属性。但是值得注意的是，job 的 id 是无法被修改的。 12345678scheduler.add_job(job_func, 'interval', minutes=2, id='job_one')scheduler.start()# 将触发时间间隔修改成 5分钟scheduler.modify_job('job_one', minutes=5)job = scheduler.add_job(job_func, 'interval', minutes=2)# 将触发时间间隔修改成 5分钟job.modify(minutes=5) 5）关闭 job默认情况下调度器会等待所有正在运行的作业完成后，关闭所有的调度器和作业存储。如果你不想等待，可以将 wait 选项设置为 False。 12scheduler.shutdown()scheduler.shutdown(wait=false) 执行器(executor)执行器顾名思义是执行调度任务的模块。最常用的 executor 有两种：ProcessPoolExecutor 和 ThreadPoolExecutor 下面是显式设置 job store(使用mongo存储)和 executor 的代码的示例。注：本代码来源于网络 1234567891011121314151617181920212223242526272829303132from pymongo import MongoClientfrom apscheduler.schedulers.blocking import BlockingSchedulerfrom apscheduler.jobstores.mongodb import MongoDBJobStorefrom apscheduler.jobstores.memory import MemoryJobStorefrom apscheduler.executors.pool import ThreadPoolExecutor, ProcessPoolExecutor def my_job(): print 'hello world'host = '127.0.0.1'port = 27017client = MongoClient(host, port) jobstores = { 'mongo': MongoDBJobStore(collection='job', database='test', client=client), 'default': MemoryJobStore()}executors = { 'default': ThreadPoolExecutor(10), 'processpool': ProcessPoolExecutor(3)}job_defaults = { 'coalesce': False, 'max_instances': 3}scheduler = BlockingScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults)scheduler.add_job(my_job, 'interval', seconds=5) try: scheduler.start()except SystemExit: client.close()","link":"/1227.html"},{"title":"实现识别弱图片验证码","text":"目前，很多网站为了防止爬虫肆意模拟浏览器登录，采用增加验证码的方式来拦截爬虫。验证码的形式有多种，最常见的就是图片验证码。其他验证码的形式有音频验证码，滑动验证码等。图片验证码越来越高级，识别难度也大幅提高，就算人为输入也经常会输错。本文主要讲解识别弱图片验证码。 图片验证码强度图片验证码主要采用加干扰线、字符粘连、字符扭曲方式来增强识别难度。 加干扰线加干扰线也分为两种，一种是线条跟字符同等颜色，另一种则线条的颜色是五颜六色。) 字符粘连各个字符之间的间隔比较小，互相依靠，能以分割。 字符扭曲字符显示的位置相对标准旋转一定角度。) 其中最弱的验证码为不具备以上的特征，干扰因素比较小。如下： 识别思路首先对图片做二值化来降噪处理，去掉图片中的噪点，干扰线等。然后将图片中的单个字符切分出来。最后识别每个字符。 图片的处理，我采用 Python 标准图像处理库 PIL。图片分割，我暂时采用谷歌开源库 Tesseract-OCR。字符识别则使用 pytesseract 库。 安装 Pillow 我使用的 Python 版本是 3.6， 而标准库 PIL 不支持 3.x。所以需要使用 Pillow 来替代。Pillow 是专门兼容 3.x 版本的 PIL 的分支。使用 pip 包管理工具安装 Pillow 是最方便快捷的。 123pip install Pillow# 如果出现因下载失败导致安装不上的情况，建议使用代理pip --proxy http://代理ip:端口 install Pillow Tesseract-OCR Tesseract：开源的OCR识别引擎，初期Tesseract引擎由HP实验室研发，后来贡献给了开源软件业，后经由Google进行改进，消除bug，优化，重新发布。这才让其重焕新生。 我们可以在 GitHub 上找到该库并下载。我是下载最新的 4.0 版本。github 的下载地址是：https://github.com/tesseract-ocr/tesseract/wiki/4.0-with-LSTM#400-alpha-for-windows pytesseract pytesseract 是 Tesseract-OCR 对进行包装，提供 Python 接口的库。同样可以使用 pip 方式来安装。 123pip install pytesseract# 如果出现因下载失败导致安装不上的情况，建议使用代理pip --proxy http://代理ip:端口 install pytesseract 代码实现获取并打开图片获取图片验证码，你可以通过使用网络请求库下载。我为了方便，将图片下载到本地并放在项目目录下。 123456789101112from PIL import Image'''获取图片'''def getImage(): fileName = '16.jpg' img = Image.open() # 打印当前图片的模式以及格式 print('未转化前的: ', img.mode, img.format) # 使用系统默认工具打开图片 # img.show() return img 预处理这一步主要是将图片进行降噪处理, 把图片从 “RGB” 模式转化为 “L” 模式，也就是把彩色图片变成黑白图片。再处理掉背景噪点，让字符和背景形成黑白的反差。 123456789101112131415161718192021'''1) 将图片进行降噪处理, 通过二值化去掉后面的背景色并加深文字对比度'''def convert_Image(img, standard=127.5): ''' 【灰度转换】 ''' image = img.convert('L') ''' 【二值化】 根据阈值 standard , 将所有像素都置为 0(黑色) 或 255(白色), 便于接下来的分割 ''' pixels = image.load() for x in range(image.width): for y in range(image.height): if pixels[x, y] &gt; standard: pixels[x, y] = 255 else: pixels[x, y] = 0 return image 打开彩色图片，PIL 会将图片解码为三通道的 “RGB” 图像。调用 convert(‘L’) 才会把图片转化为黑白图片。其中模式 “L” 为灰色图像, 它的每个像素用 8 个bit表示, 0 表示黑, 255 表示白, 其他数字表示不同的灰度。 在 PIL 中，从模式 “RGB” 转换为 “L” 模式是按照下面的公式转换的：L = R 的值 x 299/1000 + G 的值 x 587/1000+ B 的值 x 114/1000 图像的二值化，就是将图像上的像素点的灰度值两极分化(设置为 0 或 255，0表示黑，255表示白)，也就是将整个图像呈现出明显的只有黑和白的视觉效果。目的是加深字符与背景的颜色差，便于 Tesseract 的识别和分割。对于阈值的选取，我采用比较暴力的做法，直接使用 0 和 255 的平均值。 识别经过上述处理，图片验证码中的字符已经变成很清晰了。最后一步是直接用 pytesseract 库识别。 1234567891011121314import pytesseract'''使用 pytesseract 库来识别图片中的字符'''def change_Image_to_text(img): ''' 如果出现找不到训练库的位置, 需要我们手动自动 语法: tessdata_dir_config = '--tessdata-dir \"&lt;replace_with_your_tessdata_dir_path&gt;\"' ''' testdata_dir_config = '--tessdata-dir \"C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tessdata\"' textCode = pytesseract.image_to_string(img, lang='eng', config=testdata_dir_config) # 去掉非法字符，只保留字母数字 textCode = re.sub(\"\\W\", \"\", textCode) return textCode Tesseract-ORC 默认是没有指定安装路径。我们需要手动指定本地 Tesseract 的路径。不然会报出这样的错误： 1FileNotFoundError: [WinError 2] 系统找不到指定的文件 具体解决方案是：使用文本编辑器打开 pytesseract 库的 pytesseract.py 文件，一般路径如下：C:\\Program Files (x86)\\Python35-32\\Lib\\site-packages\\pytesseract\\pytesseract.py 将 tesseract_cmd 修改成你电脑本地的 Tesseract-OCR 的安装路径。 12# CHANGE THIS IF TESSERACT IS NOT IN YOUR PATH, OR IS NAMED DIFFERENTLYtesseract_cmd = 'C:/Program Files (x86)/Tesseract-OCR/tesseract.exe' 最后执行字符识别的实例代码 123456def main(): img = convert_Image(getImage(fileName)) print('识别的结果：', change_Image_to_text(img))if __name__ == '__main__': main() 运行结果如下： 12未转化前的: RGB JPEG识别的结果： 9834 总结Tesseract-ORC 对于这种弱验证码识别率还是可以，大部分字符能够正确识别出来。只不过有时候会将数字 8 识别为 0。如果图片验证码稍微变得复杂点，识别率大大降低，会经常识别不出来的情况。我自己也尝试收集 500 张图片来训练 Tesseract-ORC，识别率会有所提升，但识别率还是很低。 如果想要做到识别率较高，那么需要使用 CNN (卷积神经网络)或者 RNN (循环神经网络)训练出自己的识别库。正好机器学习很火爆很流行，学习一下也无妨。","link":"/1228.html"}],"tags":[{"name":"HTTP","slug":"HTTP","link":"/tags/HTTP/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"思念","slug":"思念","link":"/tags/%E6%80%9D%E5%BF%B5/"},{"name":"鸡汤","slug":"鸡汤","link":"/tags/%E9%B8%A1%E6%B1%A4/"},{"name":"网络爬虫","slug":"网络爬虫","link":"/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"name":"正则表达式","slug":"正则表达式","link":"/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"beautifulSoup","slug":"beautifulSoup","link":"/tags/beautifulSoup/"},{"name":"爬虫实战","slug":"爬虫实战","link":"/tags/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98/"},{"name":"当当","slug":"当当","link":"/tags/%E5%BD%93%E5%BD%93/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"多进程","slug":"多进程","link":"/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"name":"Requests","slug":"Requests","link":"/tags/Requests/"},{"name":"阅读","slug":"阅读","link":"/tags/%E9%98%85%E8%AF%BB/"},{"name":"方法论","slug":"方法论","link":"/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"},{"name":"认知","slug":"认知","link":"/tags/%E8%AE%A4%E7%9F%A5/"},{"name":"Xpath","slug":"Xpath","link":"/tags/Xpath/"},{"name":"lxml","slug":"lxml","link":"/tags/lxml/"},{"name":"电影","slug":"电影","link":"/tags/%E7%94%B5%E5%BD%B1/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Scrapy","slug":"Scrapy","link":"/tags/Scrapy/"},{"name":"反爬虫","slug":"反爬虫","link":"/tags/%E5%8F%8D%E7%88%AC%E8%99%AB/"},{"name":"数据分析","slug":"数据分析","link":"/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"generator","slug":"generator","link":"/tags/generator/"},{"name":"Iterator","slug":"Iterator","link":"/tags/Iterator/"},{"name":"Iterable","slug":"Iterable","link":"/tags/Iterable/"},{"name":"定时任务","slug":"定时任务","link":"/tags/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"name":"sched","slug":"sched","link":"/tags/sched/"},{"name":"APScheduler","slug":"APScheduler","link":"/tags/APScheduler/"},{"name":"OCR","slug":"OCR","link":"/tags/OCR/"},{"name":"验证码","slug":"验证码","link":"/tags/%E9%AA%8C%E8%AF%81%E7%A0%81/"}],"categories":[{"name":"计算机网络","slug":"计算机网络","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"Python必知必会","slug":"Python必知必会","link":"/categories/Python%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"思维与认知","slug":"思维与认知","link":"/categories/%E6%80%9D%E7%BB%B4%E4%B8%8E%E8%AE%A4%E7%9F%A5/"},{"name":"网络爬虫","slug":"网络爬虫","link":"/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"name":"数据分析","slug":"数据分析","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}]}