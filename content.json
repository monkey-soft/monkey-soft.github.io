{"pages":[{"title":"友情链接","text":"友链|极客猴 本站信息 猴哥的博客 https://geekmonkey.top 本站介绍：分享Python、网络爬虫、Web开发、数据分析的心得，思考与感悟。努力活成自己想要的样子~ 申请友链须知 原则上只和技术类博客交换，但不包括含有和色情、暴力、政治敏感的网站。 不和剽窃、侵权、无诚信的网站交换，优先和具有原创作品的网站交换。 申请链接前请先添加本博链接，通过留言或邮件告知。 申请请提供：站点名称、站点链接、站点描述、logo 或头像。 友情链接 说明：下面的友情链接顺序不分先后，每次刷新都会随机排序，申请友链请在下方留言。 Python 编程时光 https://python.iswbm.com/专注输出 Python 高质量文章 咸鱼自我满足的日常 http://www.xianyucoder.cn/专注 Python 爬虫, JS 逆向 水寒的博客 https://dp2px.com/物联网应用开发、Android应用开发、Web前端开发技术 Gcow 安全团队 http://www.gcowsec.com/一个低调的安全团队 声明：本站会定期对所有友链进行检查，如果发现 1 周内不能访问或者数月内没有内容更新可能会取消该友链。","link":"/friends/index.html"},{"title":"","text":"关于页面|极客猴 关于页面 本页面的作用是将我自己收藏一些比较常用或者实用的现实出来，算是一个的导航页面。 整个页面由 Viggo 开发，完全开源，如果你也喜欢，可以到 Github 仓库下载并引用 关于 Viggo Designer. Viggo. Full-time UI designer with an enduring interest in Coding. 一个优秀的用户界面设计师，前端开发工程师，擅长 WEB 开发、WEB 设计、UI/UX 设计，对编程，拍照和单车有着持久的兴趣，目前生活在广州； 关于极客猴 极客猴 不日进则日退 极客猴擅长 Python、网络爬虫、数据分析、Django 框架； 努力活成自己想要的样子~ Copyright &copy; 2019-2020 WebStack | Design by Viggo | Change by 极客猴","link":"/treasure/about.html"},{"title":"","text":"藏宝阁|极客猴 开发社区 语言文档 Pythoner 文档资料 学习资源 博客收藏 爬虫项目 HOT 常用工具 站长工具 文件处理 HOT 效率软件 HOT 免费图床 更多导航 关于页面 隐藏/显示侧边栏 博客首页 友情链接 关于博主 开发社区 Stack Overflow 全球最受程序员欢迎的开发社区。 CSDN 全球最大中文IT社区。 掘金 一个帮助开发者成长的社区。 博客园 代码改变世界 SegmentFault 每月帮助 1000 万开发者解决技术问题 V2EX V2EX = way to explore 简书 创作你的创作 开源中国 国内最大的开源技术社区 云+社区 来自腾讯的开发者技术分享社区 云栖社区 阿里云面向开发者的开放型技术平台 知乎 国内最受欢迎的知识性问答社区 语言文档 Java Java API 文档 C/C++ C/C++ API 文档 Python Python API 文档 SQL SQL API 文档 HTML HTML API 文档 CSS CSS API 文档 JavaScript JavaScript API 文档 Node.js Node.js API 文档 Mysql Mysql API 文档 MongoDB MongoDB API 文档 Django Django API 文档 Redis Redis API 文档 文档资料 Python 官方文档 Python 官方文档 Python 标准库 Python 标准库 Python Requests Python Requests 文档 正则表达式 Python 正则表达式官方文档 Python Selenium Python Selenium 中文翻译文档 PySpider PySpider 爬虫框架官方文档 Scrapy Scrapy 爬虫框架官方文档 Beautiful Soup Beautiful Soup 文档 Numpy Numpy 科学计算 官方中文文档 Matplotlib Matplotlib 2D绘图库 官方中文文档 Pandas Pandas 结构化数据分析 官方中文文档 学习资源 博客收藏 静觅 - 崔庆才 崔庆才的个人博客,专注Python,爬虫,深度学习,机器学习,数据分析 Python之禅 Python后台大佬 极客猴 专注于Python,网络爬虫,数据分析 咸鱼的日常 专注Python爬虫, JS逆向 唐松 专注Python网络爬虫, 数据科学, 数据挖掘, 数据分析 Jack Cui CSDN博客专家，Python 大佬 爬虫项目 Python 爬虫合集 Python 爬虫合集 python 模拟登陆一些大型网站 Python 模拟登陆一些大型网站 ScrapingOutsourcing 专注分享爬虫代码 尽量每周更新一个 interesting-python 有趣的Python爬虫和Python数据分析小项目 Python-crawler 从头开始 系统化的 学习如何写Python爬虫 PythonSpiderNotes Python入门网络爬虫之精华版 BiliOB BiliOB观测者是一个观测B站UP主及视频数据变化，并予以分析的Web应用程序。 站长工具 站长工具 - 站长之家 站长工具,SEO工具,权重查询,收录查询,PR查询,ICP备案查询,whois查询,友情链接查询,反向链接查询,网站测试,IP查询,Alexa查询 仿站工具箱 在线仿站工具箱 爱网站 爱站网站长工具提供网站收录查询和站长查询以及百度权重值查询等多个站长工具，免费查询各种工具，包括有关键词排名查询，百度收录查询等。 NnameBeta 国际域名搜索、域名注册、国别域名注册、域名比价 百度站长平台 百度搜索资源平台 - 让网站更具价值 Google 站长平台 Google 网站站长 - 支持、学习、互动交流和 Search Console – Google 百度统计 百度统计 — 最大的中文网站分析平台 Google Analytics（分析） 互联网上使用最广泛的网络分析服务，提供的网站流量统计服务 Google 广告 Google AdSense是网站通过网站战士广告主的广告来赚钱的地方 Boce.com 一个不错的网站测速和Ping检测网站 文件处理 Convertio 在线文件转换工具，支持超过309种不同的文档、图像、电子表格、电子书、文档、演示文稿、音频和视频格式 Office-Converter 免费在线转换视频，在线音频转换，在线图形转换，在线文档转换和在线压缩格式 Convertio Smallpdf - A Free Solution to all your PDF Problems,PDF压缩、转换、分割、合并等 Squoosh Google开源在线压缩、调整工具，支持WebP ILoveIMG 永远免费的在线图片处理工具，可在线编辑，压缩、裁剪、转换、水印等 PHOTOMOSH 故障艺术在线生成，可以输出jpg、gif和视频 SVGOMG SVG在线压缩平台 稿定抠图 免费在线抠图软件,图片快速换背景-抠白底图 音乐免费下载 全网音乐免费下载工具 视频鱼 在线下载各大网站视频的网站 效率软件 XMind 一个全功能的思维导图和头脑风暴软件 12306分流抢票 全程自动抢票,自动抢候补,自动识别验证码,多线程秒单、稳定捡漏,支持多天、多车次、多席别、多乘客等功能 PanDownload 破解百度网盘下载限速神器 免费图床 路过图床 免注册永久存储免费图床，最大限制：10M SM.MS SM 免费图床，每个文件最大支持 5MB 聚合图床 没有上传限制的免费图床 postimg 国外的图床，但是速度也很快。永久存储免注册 更多导航 高效搜罗 精准的职业导航 库房369 互联网资源库房 ShareHub 资源和工具的集合 77导航 77导航 Copyright &copy; 2019-2020 WebStack | Design by Viggo | Change by 极客猴 $(document).ready(function() { //img lazy loaded const observer = lozad(); observer.observe(); $(document).on('click', '.has-sub', function(){ var _this = $(this) if(!$(this).hasClass('expanded')) { setTimeout(function(){ _this.find('ul').attr(\"style\",\"\") }, 300); } else { $('.has-sub ul').each(function(id,ele){ var _that = $(this) if(_this.find('ul')[0] != ele) { setTimeout(function(){ _that.attr(\"style\",\"\") }, 300); } }) } }) $('.user-info-menu .hidden-sm').click(function(){ if($('.sidebar-menu').hasClass('collapsed')) { $('.has-sub.expanded > ul').attr(\"style\",\"\") } else { $('.has-sub.expanded > ul').show() } }) $(\"#main-menu li ul li\").click(function() { $(this).siblings('li').removeClass('active'); // 删除其他兄弟元素的样式 $(this).addClass('active'); // 添加当前元素的样式 }); $(\"a.smooth\").click(function(ev) { ev.preventDefault(); public_vars.$mainMenu.add(public_vars.$sidebarProfile).toggleClass('mobile-is-visible'); ps_destroy(); $(\"html, body\").animate({ scrollTop: $($(this).attr(\"href\")).offset().top - 30 }, { duration: 500, easing: \"swing\" }); }); return false; }); var href = \"\"; var pos = 0; $(\"a.smooth\").click(function(e) { $(\"#main-menu li\").each(function() { $(this).removeClass(\"active\"); }); $(this).parent(\"li\").addClass(\"active\"); e.preventDefault(); href = $(this).attr(\"href\"); pos = $(href).position().top - 30; });","link":"/treasure/index.html"},{"title":"关于我","text":"我大学本科 软件工程 专业，目前从事音视频行业有三、四年。因为兴趣爱好，利用业余时间研究 Python，网络爬虫，数据分析。 目的其一：工作中有些场景需要，方便自己快速输出报告。目的其二：方便自己快速获取数据，掌握流量动态。目的其三：想赚钱，虽然说到钱很多人会觉得很俗，但我就是这么真诚。 以前奋斗目标是能进入大厂，说实在，我就是大厂冲着高薪、高福利去的。后来做微信公众号，在机缘巧合情况下一些朋友，在他们的带领加入一些不同领域的圈子，认识更多的人。 这下打开了我的思维大门。 不得不说，人与人最大的区别是认知。 因为存在认知差、信息差，所以别人能站在更高的思维层面上思考问题、发现商级、挖掘赚钱的方式。 做技术出身的，第一想法是进入大厂，我也是一样。但是毕竟岗位有限、每个人的能力不一样，总有些人是幸运的，也总有些人是会被淘汰。 恰逢今年自己进入大厂的梦想破碎，成为淘汰大军中的一员。 于是乎，我在思考该何去何从？ 自己最后做出一个大胆的决定，利用自己会技术的优势，研究流量，在存量的世界里找到赚钱的方式。 这个博客之前是输出自己在技术上的总结，后续考虑多输出思考感悟。 我个人认为，技术是可以花时间来积累和学习的，但是认识没有被提高，思维视野永远停留原地，其实是一件很可怕的事情。 最后，如果你觉得我写的不错的，可以关注我的个人微信公众号『极客猴』。","link":"/about/index.html"}],"posts":[{"title":"常用 Python 标准库","text":"众所周知，Python有庞大的库资源，有官方标准库以及第三方的扩展库。每个库都一把利器，能帮助我们快速处理某方面的问题。作为一名python的初学者，当把基本的语法、列表和元组、字典、迭代器、异常处理、I/O操作、抽象等知识点学完之后。我建议把官方常用的标准库也随便学下来。讲真的，你知道这些库之后，你会有种相见恨晚的感觉。 接下来带大家走进python标准库的世界。PS： 使用Python的版本为Python3 字符串 re: 正则表达式。用来判断字符串是否是你指定的特定字符串。在爬虫项目中，经常能捕获到它的身影。 StringIO: 提供以文件为保存形式来读和写字符串。还有个性能更加好的cStringIO版本 struct: 以二进制字节序列来解释字符串。可以通过格式化参数，指定类型、长度、字节序（大小端）、内存对齐等。 数据类型 bisect: 数组二分算法。提供支持按顺序对列表进行排序，而不必每次在列表中插入后再去排序。 heapq: 堆队列算法。最小堆：完全平衡二叉树， 所有节点都小于字节点。 datetime: 提供操作日期和时间的类。其中有两种日期和时间类型： naive和aware collections: 高性能容器数据类型。实现了Python的通用内置容器、字典、列表、集合，和元组专门的数据类型提供替代品 pprint: 提供”整洁低打印”任意Python数据结构的能力。 数学运算 random: 各种分布的伪随机数的生成器 math: 数学函数。提供了由C标准的数学函数访问。该库的函数不适用于复数。 cmath: 为复数提供的数学函数。 operator: 提供了重载操作符 文件和目录 os.path: 常用路径名操作。提供了操作路径名的常用的函数。 filecmp: 文件和目录的比较。提供了比较文件和目录的函数。 shutil: 高级的文件操作。提供了许多文件和文件集上的操作操作。尤其是提供支持文件复制和删除的函数。 数据存储 serialization: Python专用的序列化算法，通常不建议用来存储自定义数据。 pickle: Python对象序列化。提供了一个基本但功能强大的Python对象序列化和反序列化算法。 cPickle: 比pickle快1000倍的对象序列化库， 和pickle可互相替换。 shevle: 将对象pickle序列化，然后保存到anydbm格式文件。anydbm是KV结构的数据库，可以保存多个序列化对象。 sqlite3: SQLite数据库DB-API 2.0接口。 数据压缩 zipfile: 提供了ZIP文件个创建、读取、写入、最佳和列出zip文件的函数。 tarfile: 提供了tar文件的压缩和解压的函数。 文件格式 csv: 提供对CSV文件的读取和写入的函数。 加密 hashlib: 安全哈希和消息摘要。实现了一个通用的接口来实现多个不同的安全哈希和消息摘要算法。包括 FIPS 安全哈希算法 SHA1、SHA224、SHA256、SHA384和 SHA512（定义在 FIPS 180-2），以及 RSA 的 MD5 算法（在互联网 RFC 1321中定义)。 hmac: 用于消息认证的加密哈希算法。实现了RFC 2104 中描述的HMAC 算法。 md5: 实现了MD5加密算法。 sha: 实现了sha1加密算法。 操作系统 time: 时间获取和转换。提供了各种与时间相关的函数。 argparse: 命令行选项、参数和子命令的解析器。使用该库使得编码用户友好的命令行接口非常容易。取代了之前的optparse io: 提供接口处理IO流。 logging: Python的日志工具。提供了日志记录的API。 logging.config: Python日志配置。用于配置日志模块的API。 os: 提供丰富的雨MAC，NT，Posix等操作系统进行交互的能力。这个模块允许程序独立的于操作系统环境。文件系统，用户数据库和权限进行交互。 _thread: 多线程控制。提供了一个底层、原始的操作 —— 多个控制线程共享全局数据空间。 threading: 高级线程接口。是基于_thread模块的，但是比_thread更加容易使用、更高层次的线程API。 sys: 提供访问和维护python解释器的能力。这包括了提示信息，版本，整数的最大值，可用模块，路径钩子，标准错误，标准输入输出的定位和解释器调用的命令行参数。 进程通信 subprocess: 管理子进程。允许用户产生新的进程，然后连接他们的输入/输出/错误/管道，并获取返回值。 socket: 底层网络接口。 signal: 设置异步时间处理handlers。信号是软中断，提供了一种异步事件通知机制。 网络数据处理 json: JSON格式的编码器和解码器。 base64: 提供依据RFC 3548的规定（Base16, Base32, Base64 ）进行数据编码和解码。 htmllib: 提供了一个HTML语法解析器。 mimetypes: 提供了判断给定的URL的MIME类型。 操作因特网网络协议 urllib: 提供了用于获取万维网数据的高层接口。这个是Python2.7版本的，Python3已经将其拆分成多个模块urllib.request，urllib.parse和urllib.error。 urlparse: 提供了用于处理URL的函数，可以在URL和平台特定的文件名间相互转换。 http.client: HTTP协议客户端。 telnetlib: 提供了实现Telnet协议的Telnet类。 poplib: POP3协议客户端。 ftplib: FTP协议客户端。 smtplib: SMTP协议客户端。 webbrowser: 提供控制浏览器行为的函数。","link":"/123.html"},{"title":"深入理解HTTP","text":"HTTP是什么 HTTP全称是HyperText Transfer Protocal，即：超文本传输协议。它主要规定了客户端和服务器之间的通信格式。HTTP还是一个基于请求/响应模式的、无状态的协议；即我们通常所说的Request/Response。 HTTP与TCP的关系TCP协议是位于TCP/IP参考模型中的网络互连层，而HTTP协议属于应用层。因此，HTTP协议是基于TCP协议。 HTTP请求(HTTP Request)HTTP请求由三部分组成，分别是： 请求行 HTTP头 请求体 下面是请求示例： 123456789GET /?tn=90058352_hao_pg HTTP/1.1Host: www.hao123.comConnection: keep-aliveCache-Control: max-age=0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Upgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/5.7.16400.16 Safari/537.36Accept-Encoding: gzip, deflateAccept-Language: zh-CN,zh;q=0.8 请求行同样也是由请求方法（POST/GET）方式、请求的主机、协议版本号三部分组成。下面为请求行的示例：GET /?tn=90058352_hao_pg HTTP/1.1 HTTP头HTTP头又细分为请求头(request header)、普通头(general header)、实体头(entity header)而HTTP头主要关注点是其字段 Accept作用: 浏览器可以接受的媒体类型例如： Accept: text/html 代表浏览器可以接受服务器回发的类型为 text/html 也就是我们常说的html文档通配符 * 代表任意类型例如： Accept: */* 代表浏览器可以处理所有类型,(一般浏览器发给服务器都是发这个) Accept-Language作用： 浏览器申明自己接收的语言。语言跟字符集的区别：中文是语言，中文有多种字符集，比如big5，gb2312，gbk等等；例如： Accept-Language: zh-CN,zh Accept-Encoding作用： 浏览器申明自己接收的编码方法，通常指定压缩方法（gzip，deflate）例如：Accept-Encoding: gzip, Accept-Encoding: deflate User-Agent作用： 告诉HTTP服务器， 客户端使用的操作系统的名称和版本以及浏览器的名称和版本.例如： User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/5.7.16400.16 Safari/537.36 Content-Type作用： 告诉服务器，请求的内容的类型常见的字段： 假设使用POST方式请求 text/xml [请求体为文本] application/json [请求体为JSON数据] application/xml [请求体为xml数据] image/jpeg [请求体为jpeg图片] multipart/form-data [请求体为表单] Cookie作用： 最重要的header，将cookie的值发送给HTTP服务器 Connection例如： Connection: keep-alive 当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接例如： Connection: close 代表一个Request完成后，客户端和服务器之间用于传输HTTP数据的TCP连接会关闭， 当客户端再次发送Request，需要重新建立TCP连接。 Content-Length作用：发送给HTTP服务器数据的长度。例如： Content-Length: 18 Referer:作用： 提供了Request的上下文信息的服务器，告诉服务器我是从哪个链接过来的。 请求体这个只有post方式请求才有，get方式请求没有。 HTTP响应(HTTP Response)HTTP Response的结构跟Request的结构基本一样。同样分为三部分： 响应行 响应头 响应体 下面是响应示例： 1234567891011HTTP/1.1 200 OKCache-Control: max-age=0Content-Encoding: gzipContent-Length: 156474Content-Type: text/html;charset=UTF-8Cxy_all: 90058352_hao_pg+d4fa7f28cefb9b120f868558e440bafaDate: Sun, 20 Nov 2016 05:09:51 GMTExpires: Sun, 20 Nov 2016 05:09:51 GMTLfy: nj02.11Server: BWS/1.0Set-Cookie: __bsi=11619936655404239050_00_60_N_R_126_0303_c02f_Y; max-age=3600; domain=www.hao123.com; path=/ 响应行响应行由协议版本、响应状态构成下面为响应行的示例：HTTP/1.1 200 OK 响应头响应头关注点是字段，常见的字段如下： Cache-Control作用: 非常重要的规则。 这个用来指定Response-Request遵循的缓存机制。例如：Cache-Control:Public 可以被任何缓存所缓存Cache-Control:Private 内容只缓存到私有缓存中Cache-Control:no-cache 所有内容都不会被缓存 Content-Type作用：服务器告诉浏览器，自己响应的对象的类型和字符集例如:Content-Type: text/html; charset=utf-8Content-Type: image/jpeg Expires作用: 浏览器会在指定过期时间内使用本地缓存例如: Expires:Sun, 20 Nov 2016 05:09:51 GMT Connection跟HTTP头中的Connection是同样的原理 Content-Encoding跟HTTP中头的Content-Encoding是同样的原理 Content-Length作用：指明实体正文的长度，以字节方式存储的十进制数字来表示。例如: Content-Length: 156474 Date作用: 生成消息的具体时间和日期例如: Date: Sun, 20 Nov 2016 05:09:51 GMT 响应体响应体包含的内容是网页的内容信息，主要是html代码等","link":"/111.html"},{"title":"每逢佳节倍思亲","text":"回忆童年每逢冬至来临外面寒风凛凛屋内热气腾腾全家人围在一起吃汤圆团团圆圆 如今身为游子在外漂流更思念家更思念父母亲只能发QQ说说、微博、朋友圈给远方家人送上祝福？不！没有比给家人打一电话来得更加直接来的更加亲切拿起你手中的手机给家里打一通电话为爸妈送上节日祝福！祝：冬至快乐~","link":"/124.html"},{"title":"学爬虫之道","text":"近来在阅读 《轻量级 Django》,虽然还没有读完，但我已经收益颇多。我不得不称赞 Django 框架的开发人员，他们把 Web 开发降低门槛。Django 让我从对 Web 开发是一无所知到现在可以编写小型 web 应用，这很舒服。 Django 已经算是入门，所以自己把学习目标转到爬虫。自己接下来会利用三个月的时间来专攻 Python 爬虫。这几天，我使用“主题阅读方法”阅读 Python 爬虫入门的文档。制定 Python 爬虫的学习路线。 第一阶段：夯实入门要就是在打基础，所以要从最基础的库学起。下面是几个库是入门最经典的库 urllib它属于 Python 标准库。该库的作用是请求网页并下载数据。在学习该库之前，最好把 HTTP 协议了解下。这会大大提高后面的学习效率。 先学会如何使用 urllib 请求到数据，再学习一些高级用法。例如： 设置 Headers: 某些网站反感爬虫的到访，于是对爬虫一律拒绝请求。设置 Headers 可以把请求伪装成浏览器访问网站。 Proxy 的设置: 某些站点做了反倒链的设置，会将高频繁访问的 IP 地址封掉。所以我们需要用到代理池。 错误解析：根据 URLError 与 HTTPError 返回的错误码进行解析。 Cookie 的使用：可以模拟网站登录，需要结合 cookielib 一起使用。 rere 是正则表达式库。同时也是 Python 标准库之一。它的作用是匹配我们需要爬取的内容。所以我们需要掌握正则表达式常用符号以及常用方法的用法。 BeautifulSoupBeautifulSoup 是解析网页的一款神器。它可以从 HTML 或者 XML 文件中提取数据。配合 urllib 可以编写出各种小巧精干的爬虫脚本。 第二阶段：进阶当把基础打牢固之后，我们需要更进一步学习。使用更加完善的库来提高爬取效率 使用多线程使用多线程抓取数据，提高爬取数据效率。 学习 RequestsRequests 作为 urlilb 的替代品。它是更加人性化、更加成熟的第三方库。使用 Requests 来处理各种类型的请求，重复抓取问题、cookies 跟随问题、多线程多进程、多节点抓取、抓取调度、资源压缩等一系列问题。 学习 XpathXpath 也算是一款神器。它是一款高效的、表达清晰简单的分析语言。掌握它以后介意弃用正则表达式了。一般是使用浏览器的开发者工具 加 lxml 库。 学习 Selenium使用 Selenium，模拟浏览器提交类似用户的操作，处理js动态产生的网页。因为一些网站的数据是动态加载的。类似这样的网站，当你使用鼠标往下滚动时，会自动加载新的网站。 第三阶段：突破学习 ScrapyScrapy 是一个功能非常强大的分布式爬虫框架。我们学会它，就可以不用重复造轮子。 数据存储如果爬取的数据条数较多，我们可以考虑将其存储到数据库中。因此，我们需要学会 MySqlMongoDB、SqlLite的用法。更加深入的，可以学习数据库的查询优化。 第四阶段：为我所用当爬虫完成工作，我们已经拿到数据。我们可以利用这些数据做数据分析、数据可视化、做创业项目原始启动数据等。我们可以学习 NumPy、Pandas、 Matplotlib 这三个库。 NumPy ：它是高性能科学计算和数据分析的基础包。 Pandas : 基于 NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。它可以算得上作弊工具。 Matplotlib：Python中最著名的绘图系统Python中最著名的绘图系统。它可以制作出散点图，折线图，条形图，直方图，饼状图，箱形图散点图，折线图，条形图，直方图，饼状图，箱形图等。","link":"/62.html"},{"title":"内容提取神器 beautifulSoup 的用法","text":"上篇文章只是简单讲述正则表达式如何读懂以及 re 常见的函数的用法。我们可能读懂别人的正则表达式，但是要自己写起正则表达式的话，可能会陷入如何写的困境。正则表达式写起来费劲又出错率高，那么有没有替代方案呢？俗话说得好，条条道路通罗马。目前还两种代替其的办法，一种是使用 Xpath 神器，另一种就是本文要讲的 BeautifulSoup。 BeautifulSoup 简介引用 BeautifulSoup 官网的说明： Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work. 大致意思如下: BeautifulSoup 是一个能从 HTML 或 XML 文件中提取数据的 Python 库。它能通过自己定义的解析器来提供导航、搜索，甚至改变解析树。它的出现，会大大节省开发者的时间。 安装 BeautifulSoup目前 BeautifulSoup 最新版本是 4.6.0，它是支持 Python3的。所以可以大胆去升级安装使用。 安装方法有两种： 使用pip比较推荐使用这种方式，既简单又方便管理。 1234pip install beautifulsoup4# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install beautifulsoup4 使用easy_install 1easy_install beautifulsoup4 使用系统包管理 12sudo apt-get install Python-bs4# 适用于 ubuntu 系统以及 Debian 系统 初始 BeautifulSoup首先导入 BeautifulSoup 库，然后创建一个 BeautifulSoup 对象，再利用对象做文章。具体参考示例代码： 1234from bs4 import BeautifulSoupsoup = BeautifulSoup(response)print(soup.prettify()) 上面代码中，response 可以urlllib或者request请求返回的内容，也可以是本地 HTML 文本。如果要打开本地，代码需要改为 12soup = BeautifulSoup(open(\"index.html\"))# 打开当前目录下 index.html 文件 soup.prettify()函数的作用是打印整个 html 文件的 dom 树，例如上面执行结果如下： 123456789101112&lt;html&gt; &lt;head&gt; &lt;title&gt; The Dormouse's story &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt; &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt; &lt;/body&gt;&lt;/html&gt; 解析 BeautifulSoup 对象想从 html 中获取到自己所想要的内容，我归纳出三种办法： 利用 Tag 对象从上文得知，BeautifulSoup 将复杂 HTML 文档转换成一个复杂的树形结构,每个节点都是Python对象。跟安卓中的Gson库有异曲同工之妙。节点对象可以分为 4 种：Tag, NavigableString, BeautifulSoup, Comment。 Tag 对象可以看成 HTML 中的标签。这样说，你大概明白具体是怎么回事。我们再通过例子来更加深入了解 Tag 对象。以下代码是以 prettify() 打印的结果为前提。 例子1 获取head标签内容 123print(soup.head)# 输出结果如下：&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt; 例子2 获取title标签内容 123print(soup.title)# 输出结果如下：&lt;title&gt;The Dormouse's story&lt;/title&gt; 例子3 获取p标签内容 123print(soup.p)# 输出结果如下：&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; 如果 Tag 对象要获取的标签有多个的话，它只会返回所以内容中第一个符合要求的标签。 对象一般含有属性，Tag 对象也不例外。它具有两个非常重要的属性， name 和 attrs。 namename 属性是 Tag 对象的标签名。不过也有特殊的，soup 对象的 name 是 [document] 12345print(soup.name)print(soup.head.name)# 输出结果如下：[document]head attrsattrs 属性是 Tag 对象所包含的属性值，它是一个字典类型。 123print(soup.p.attrs）# 输出结果如下：{'class': ['title'], 'name': 'dromouse'} 其他三个属性也顺带介绍下: NavigableString 说白了就是：Tag 对象里面的内容 123print(soup.title.string) # 输出结果如下：The Dormouse's story BeautifulSoup BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象。它是一个特殊的 Tag。 1234567print(type(soup.name))print(soup.name)print(soup.attrs)# 输出结果如下：&lt;type 'unicode'&gt;[document]{} 空字典 Comment Comment 对象是一个特殊类型的 NavigableString 对象。如果 HTML 页面中含有注释及特殊字符串的内容。而那些内容不是我们想要的，所以我们在使用前最好做下类型判断。例如： 12if type(soup.a.string) == bs4.element.Comment: ... # 执行其他操作，例如打印内容 利用过滤器过滤器其实是一个find_all()函数， 它会将所有符合条件的内容以列表形式返回。它的构造方法如下： 1find_all(name, attrs, recursive, text, **kwargs ) name 参数可以有多种写法： （1）节点名 123print(soup.find_all('p'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] （2）正则表达式 123print(soup.find_all(re.compile('^p')))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] （3）列表如果参数为列表，过滤标准为列表中的所有元素。看下具体代码，你就会一目了然了。 123print(soup.find_all(['p', 'a']))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;, &lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;, &lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 另外 attrs 参数可以也作为过滤条件来获取内容，而 limit 参数是限制返回的条数。 利用 CSS 选择器以 CSS 语法为匹配标准找到 Tag。同样也是使用到一个函数，该函数为select()，返回类型也是 list。它的具体用法如下, 同样以 prettify() 打印的结果为前提： （1）通过 tag 标签查找 123print(soup.select(head))# 输出结果如下：[&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;] （2）通过 id 查找 123print(soup.select('#link1'))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] （3）通过 class 查找 123print(soup.select('.sister'))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] （4）通过属性查找 123print(soup.select('p[name=dromouse]'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;] 123print(soup.select('p[class=title]'))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;] （5）组合查找 1234print(soup.select(\"body p\"))# 输出结果如下：[&lt;p class=\"title\" name=\"dromouse\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;,&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were&lt;/p&gt;] 123print(soup.select(\"p &gt; a\"))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 123print(soup.select(\"p &gt; .sister\"))# 输出结果如下：[&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;] 处理上下关系从上文可知，我们已经能获取到节点对象，但有时候需要获取其父节点或者子节点的内容，我们要怎么做了？这就需要对parse tree进行遍历 （1）获取子节点利用.children属性，该属性会返回当前节点所以的子节点。但是它返回的类型不是列表，而是迭代器 （2）获取所有子孙节点使用.descendants属性，它会返回所有子孙节点的迭代器 （3）获取父节点通过.parent属性可以获得所有子孙节点的迭代器 （4）获取所有父节点.parents属性，也是返回所有子孙节点的迭代器 （5）获取兄弟节点兄弟节点可以理解为和本节点处在统一级的节点，.next_sibling属性获取了该节点的下一个兄弟节点，.previous_sibling则与之相反，如果节点不存在，则返回 None 注意：实际 HTML 中的 tag 的.next_sibling和 .previous_sibling属性通常是字符串或空白，因为空白或者换行也可以被视作一个节点，所以得到的结果可能是空白或者换行 （5）获取所有兄弟节点通过.next_siblings和.previous_siblings属性可以对当前节点的兄弟节点迭代输出","link":"/78.html"},{"title":"详解 python3 urllib","text":"本文是爬虫系列文章的第一篇，主要讲解 Python 3 中的 urllib 库的用法。urllib 是 Python 标准库中用于网络请求的库。该库有四个模块，分别是urllib.request，urllib.error，urllib.parse，urllib.robotparser。其中urllib.request，urllib.error两个库在爬虫程序中应用比较频繁。那我们就开门见山，直接讲解这两个模块的用法。 发起请求模拟浏览器发起一个 HTTP 请求，我们需要用到 urllib.request 模块。urllib.request 的作用不仅仅是发起请求， 还能获取请求返回结果。发起请求，单靠 urlopen() 方法就可以叱咤风云。我们先看下 urlopen() 的 API 1urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) 第一个参数 String 类型的地址或者 data 是 bytes 类型的内容，可通过 bytes()函数转为化字节流。它也是可选参数。使用 data 参数，请求方式变成以 POST 方式提交表单。使用标准格式是application/x-www-form-urlencoded timeout 参数是用于设置请求超时时间。单位是秒。 cafile和capath代表 CA 证书和 CA 证书的路径。如果使用HTTPS则需要用到。 context参数必须是ssl.SSLContext类型，用来指定SSL设置 cadefault参数已经被弃用，可以不用管了。 该方法也可以单独传入urllib.request.Request对象 该函数返回结果是一个http.client.HTTPResponse对象。 简单抓取网页我们使用 urllib.request.urlopen() 去请求百度贴吧，并获取到它页面的源代码。 123456import urllib.requesturl = \"http://tieba.baidu.com\"response = urllib.request.urlopen(url)html = response.read() # 获取到页面的源代码print(html.decode('utf-8')) # 转化为 utf-8 编码 设置请求超时有些请求可能因为网络原因无法得到响应。因此，我们可以手动设置超时时间。当请求超时，我们可以采取进一步措施，例如选择直接丢弃该请求或者再请求一次。 12345import urllib.requesturl = \"http://tieba.baidu.com\"response = urllib.request.urlopen(url, timeout=1)print(response.read().decode('utf-8')) 使用 data 参数提交数据在请求某些网页时需要携带一些数据，我们就需要使用到 data 参数。 123456789101112import urilib.parseimport urllib.requesturl = \"http://127.0.0.1:8000/book\"params = { 'name':'浮生六记', 'author':'沈复'}data = bytes(urllib.parse.urlencode(params), encoding='utf8')response = urllib.request.urlopen(url, data=data)print(response.read().decode('utf-8')) params 需要被转码成字节流。而 params 是一个字典。我们需要使用 urllib.parse.urlencode() 将字典转化为字符串。再使用 bytes() 转为字节流。最后使用 urlopen() 发起请求，请求是模拟用 POST 方式提交表单数据。 使用 Request由上我们知道利用 urlopen() 方法可以发起简单的请求。但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入headers（请求头）、指定请求方式等信息，我们就可以利用更强大的Request类来构建一个请求。按照国际惯例，先看下 Request 的构造方法： 1urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None) url 参数是请求链接，这个是必传参数，其他的都是可选参数。 data 参数跟 urlopen() 中的 data 参数用法相同。 headers 参数是指定发起的 HTTP 请求的头部信息。headers 是一个字典。它除了在 Request 中添加，还可以通过调用 Reques t实例的 add_header() 方法来添加请求头。 origin_req_host 参数指的是请求方的 host 名称或者 IP 地址。 unverifiable 参数表示这个请求是否是无法验证的，默认值是False。意思就是说用户没有足够权限来选择接收这个请求的结果。例如我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，我们就要将 unverifiable 的值设置成 True。 method 参数指的是发起的 HTTP 请求的方式，有 GET、POST、DELETE、PUT等 简单使用 Request使用 Request 伪装成浏览器发起 HTTP 请求。如果不设置 headers 中的 User-Agent，默认的User-Agent是Python-urllib/3.5。可能一些网站会将该请求拦截，所以需要伪装成浏览器发起请求。我使用的 User-Agent 是 Chrome 浏览器。 123456789import urllib.requesturl = \"http://tieba.baidu.com/\"headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}request = urllib.request.Request(url=url, headers=headers)response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) Request 高级用法如果我们需要在请求中添加代理、处理请求的 Cookies，我们需要用到Handler和OpenerDirector。 1） HandlerHandler 的中文意思是处理者、处理器。 Handler 能处理请求（HTTP、HTTPS、FTP等）中的各种事情。它的具体实现是这个类 urllib.request.BaseHandler。它是所有的 Handler 的基类，其提供了最基本的Handler的方法，例如default_open()、protocol_request()等。继承 BaseHandler 有很多个，我就列举几个比较常见的类： ProxyHandler：为请求设置代理 HTTPCookieProcessor：处理 HTTP 请求中的 Cookies HTTPDefaultErrorHandler：处理 HTTP 响应错误。 HTTPRedirectHandler：处理 HTTP 重定向。 HTTPPasswordMgr：用于管理密码，它维护了用户名密码的表。 HTTPBasicAuthHandler：用于登录认证，一般和 HTTPPasswordMgr 结合使用。 2） OpenerDirector对于 OpenerDirector，我们可以称之为 Opener。我们之前用过 urlopen() 这个方法，实际上它就是 urllib 为我们提供的一个Opener。那 Opener 和 Handler 又有什么关系？opener 对象是由 build_opener(handler) 方法来创建出来 。我们需要创建自定义的 opener，就需要使用 install_opener(opener)方法。值得注意的是，install_opener 实例化会得到一个全局的 OpenerDirector 对象。 使用代理我们已经了解了 opener 和 handler，接下来我们就通过示例来深入学习。第一个例子是为 HTTP 请求设置代理有些网站做了浏览频率限制。如果我们请求该网站频率过高。该网站会被封 IP，禁止我们的访问。所以我们需要使用代理来突破这“枷锁”。 1234567891011121314151617import urllib.requesturl = \"http://tieba.baidu.com/\"headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}proxy_handler = urllib.request.ProxyHandler({ 'http': 'web-proxy.oa.com:8080', 'https': 'web-proxy.oa.com:8080'})opener = urllib.request.build_opener(proxy_handler)urllib.request.install_opener(opener)request = urllib.request.Request(url=url, headers=headers)response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) 认证登录有些网站需要携带账号和密码进行登录之后才能继续浏览网页。碰到这样的网站，我们需要用到认证登录。我们首先需要使用 HTTPPasswordMgrWithDefaultRealm() 实例化一个账号密码管理对象；然后使用 add_password() 函数添加账号和密码；接着使用 HTTPBasicAuthHandler() 得到 hander；再使用 build_opener() 获取 opener 对象；最后使用 opener 的 open() 函数发起请求。 第二个例子是携带账号和密码请求登录百度贴吧，代码如下： 123456789101112import urllib.requesturl = \"http://tieba.baidu.com/\"user = 'user'password = 'password'pwdmgr = urllib.request.HTTPPasswordMgrWithDefaultRealm()pwdmgr.add_password(None，url ，user ，password)auth_handler = urllib.request.HTTPBasicAuthHandler(pwdmgr)opener = urllib.request.build_opener(auth_handler)response = opener.open(url)print(response.read().decode('utf-8')) Cookies设置如果请求的页面每次需要身份验证，我们可以使用 Cookies 来自动登录，免去重复登录验证的操作。获取 Cookies 需要使用 http.cookiejar.CookieJar() 实例化一个 Cookies 对象。再用 urllib.request.HTTPCookieProcessor 构建出 handler 对象。最后使用 opener 的 open() 函数即可。 第三个例子是获取请求百度贴吧的 Cookies 并保存到文件中，代码如下： 123456789101112131415import http.cookiejarimport urllib.requesturl = \"http://tieba.baidu.com/\"fileName = 'cookie.txt'cookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open(url)f = open(fileName,'a')for item in cookie: f.write(item.name+\" = \"+item.value+'\\n')f.close() HTTPResponse从上面的例子可知， 使用 urllib.request.urlopen() 或者 opener.open(url) 返回结果是一个 http.client.HTTPResponse 对象。它具有 msg、version、status、reason、debuglevel、closed等属性以及read()、readinto()、getheader(name)、getheaders()、fileno()等函数。 错误解析发起请求难免会出现各种异常，我们需要对异常进行处理，这样会使得程序比较人性化。异常处理主要用到两个类，urllib.error.URLError和urllib.error.HTTPError。 URLErrorURLError 是 urllib.error 异常类的基类, 可以捕获由urllib.request 产生的异常。它具有一个属性reason，即返回错误的原因。 捕获 URL 异常的示例代码： 12345678import urllib.requestimport urllib.errorurl = \"http://www.google.com\"try: response = request.urlopen(url)except error.URLError as e: print(e.reason) HTTPError HTTPError 是 UEKRrror 的子类，专门处理 HTTP 和 HTTPS 请求的错误。它具有三个属性。 1)code：HTTP 请求返回的状态码。 1)renson：与父类用法一样，表示返回错误的原因。 1)headers`：HTTP 请求返回的响应头信息。 获取 HTTP 异常的示例代码, 输出了错误状态码、错误原因、服务器响应头 12345678910import urllib.requestimport urllib.errorurl = \"http://www.google.com\"try: response = request.urlopen(url)except error.HTTPError as e: print('code: ' + e.code + '\\n') print('reason: ' + e.reason + '\\n') print('headers: ' + e.headers + '\\n')","link":"/66.html"},{"title":"Python 正则表达式","text":"我们能够使用 urllib 向网页请求并获取其网页数据。但是抓取信息数据量比较大，我们可能需要其中一小部分数据。对付刚才的难题，就需要正则表达式出马了。正则表达式能帮助我们匹配过滤到我们需要的数据，但它学习起来非常枯燥无味。你可能会说，我还没有开始想学习正则表达式，你就来打击我？ 莫慌！层层递进地学习，一步一个脚印地学习，很快就会学会了。对于爬虫，我觉得学会最基本的符号就差不多了。 正则表达式下面是一张关于正则表达式字符的图，图片资料来自CSDN。先把图中字符了解清楚，基本上算是入门。 re 模块Python 自 1.5 版本起通过新增 re （Regular Expression 正则表达式）模块来提供对正则表达式的支持。使用 re 模块先将正则表达式填充到 Pattern 对象中，再把 Pattern 对象作为参数使用 match 方法去匹配的字符串文本。match 方法会返回一个 Match 对象，再通过 Match 对象会得到我们的信息并进行操作。下面介绍几个 re 常用的函数。 compile 函数compile 是把正则表达式的模式和标识转化成正则表达式对象，供 match() 和 search() 这两个函数使用。它的函数语法如下： 1re.compile(pattern[, flags]) 第一个参数是pattern，指的正则表达式。 第二个参数flags是匹配模式，是个可选参数。可以使用按位或’|’表示同时生效，也可以在正则表达式字符串中指定。匹配模式有以下几种： flag 描述 re.I(全拼：IGNORECASE) 忽略大小写（括号内是完整写法，下同） re.M(全拼：MULTILINE) 多行模式，改变’^’和’$’的行为（参见上图） re.S(全拼：DOTALL) 点任意匹配模式，改变’.’的行为 re.L(全拼：LOCALE) 使预定字符类 \\w \\W \\b \\B \\s \\S 取决于当前区域设定 re.U(全拼：UNICODE) 使预定字符类 \\w \\W \\b \\B \\s \\S \\d \\D 取决于unicode定义的字符属性 re.X(全拼：VERBOSE) 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。 该方法返回的结果是一个 Pattern 对象。 match 函数match()函数只在字符串的开始位置尝试匹配正则表达式，也就是说只有在 0 位置匹配成功的话才有返回。如果不是开始位置匹配成功的话，match() 就返回 none。它的函数语法如下： 1re.match(pattern, string[, flags]) 第一个参数：匹配的正则表达式 第二个参数：要被匹配的字符串 flags 是可选参数，跟 compile 用法相似 匹配成功 re.match 方法返回一个匹配的对象，否则返回None。要想获得匹配结果，既可以使用groups()函数获取一个包含所有字符串的元组（从 1 到 所含的小组号），也可以使用group(组号)函数获取某个组号的字符串。 match 函数用法的示例代码： 123456789import repattern = re.compile('Python')text = 'Python python pythonn'match = re.search(pattern, text)if match: print(match.group())else: print('没有匹配') search 函数 search() 函数是扫描整个字符串来查找匹配，它返回结果是第一个成功匹配的字符串。 1re.search(pattern, string[, flags]) 参数用法以及返回结果跟match函数用法相同。 search 函数用法的示例代码： 123456789import repattern = re.compile('Python')text = 'welcome to Python world!'match = re.search(pattern, text)if match: print(match.group())else: print('没有匹配') findall 函数findall函数在字符串中搜索子串，并以列表形式返回全部能匹配的所有子串。 1re.findall(pattern, string[, flags]) 参数用法以及返回结果跟match函数用法相同。 findall 函数用法的示例代码： 123456789import repattern = re.compile('\\d+')text = 'one1two2three3four4'list = re.findall(pattern, text)if list: print(list)else: print('没有匹配')","link":"/67.html"},{"title":"爬虫实战一：爬取当当网所有 Python 书籍","text":"我们已经学习 urllib、re、BeautifulSoup 这三个库的用法。但只是停留在理论层面上，还需实践来检验学习成果。因此，本文主要讲解如何利用我们刚才的几个库去实战。 确定爬取目标任何网站皆可爬取，就看你要不要爬取而已。本次选取的爬取目标是当当网，爬取内容是 以 Python 为关键字搜索出来的页面中所有书籍的信息。具体如下图所示： 本次爬取结果有三项： 图书的封面图片 图书的书名 图书的链接页面最后把这三项内容保存到 csv 文件中。 爬取过程总所周知，每个站点的页面 DOM 树是不一样的。所以我们需要先对爬取页面进行分析，再确定自己要获取的内容，再定义程序爬取内容的规则。 确定 URL 地址我们可以通过利用浏览器来确定URL 地址，为 urllib 发起请求提供入口地址。接下来，我们就一步步来确定请求地址。搜索结果页面为 1 时，URL 地址如下： 搜索结果页面为 3 时，URL 地址如下： 搜索结果页面为 21 时，即最后一页，URL 地址如下： 从上面的图片中，我们发现 URL 地址的差异就在于 page_index 的值，所以 URL 地址最终为 http://search.dangdang.com/?key=python&amp;act=input&amp;show=big&amp;page_index=。而 page_index 的值，我们可以通过循环依次在地址后面添加。因此， urllib 请求代码可以这样写： 123456789101112# 爬取地址, 当当所有 Python 的书籍, 一共是 21 页url = \"http://search.dangdang.com/?key=python&amp;act=input&amp;show=big&amp;page_index=\"index = 1while index &lt;= 21: # 发起请求 request = urllib.request.Request(url=url+str(index), headers=headers) response = urllib.request.urlopen(request) index = index + 1 # 解析爬取内容 parseContent(response) time.sleep(1) # 休眠1秒 确定爬取节点有了 URL 地址，就能使用 urllib 获取到页面的 html 内容。到了这步，我们就需要找到爬取的节点的规则，以便于 BeautifulSoup 地解析。为了搞定这个问题，就要祭出大招 —— Chrome 浏览器的开发者功能（按下 F12 键就能启动）。我们按下 F12 键盘，依次对每本书进行元素检查（在页面使用鼠标右键，点击“检查”即可），具体结果如下： 从上图可以得知解析规则：每本书的节点是一个 a 标签，a 标签具有 title，href，子标签 img 的 src 三个属性，这三者分别对应书名、书的链接页面、书的封图。看到这里也需你不会小激动，感叹这不就是我们要感兴趣的内容吗？得到解析规则，编写BeautifulSoup 解析代码就有了思路，具体代码如下： 1234567891011121314151617# 提取爬取内容中的 a 标签, 例如：# &lt;a# class=\"pic\" dd_name=\"单品图片\"# ddclick=\"act=normalResult_picture&amp;amp;pos=23648843_53_2_q\"# href=\"http://product.dangdang.com/23648843.html\"# name=\"itemlist-picture\"# target=\"_blank\" title=\"# 趣学Python――教孩子学编程 \"&gt;## &lt;img# alt=\" 趣学Python――教孩子学编程 \"# data-original=\"http://img3x3.ddimg.cn/20/34/23648843-1_b_0.jpg\"# src=\"images/model/guan/url_none.png\"/&gt;# &lt;/a&gt;soup = BeautifulSoup(response)books = soup.find_all('a', class_='pic')print(books) 运行结果如下： 这证明刚才制定规则是正确爬取我们所需的内容。 保存爬取信息我写爬虫程序有个习惯，就是每次都会爬取内容持久化到文件中。这样方便以后查看使用。如果爬取数据量比较大，我们可以用其做数据分析。我这里为了方便，就将数据保存到 csv 文件中。用 Python 将数据写到文件中，我们经常中文乱码问题所烦恼。如果单纯使用 csv 库，可能摆脱不了这烦恼。所以我们将 csv 和 codecs 结合一起使用。在写数据到 csv 文件的时候，我们可以通过指定文件编码。这样中文乱码问题就迎刃而解。具体代码如下: 12345678910111213141516171819fileName = 'PythonBook.csv'# 指定编码为 utf-8, 避免写 csv 文件出现中文乱码with codecs.open(fileName, 'w', 'utf-8') as csvfile: filednames = ['书名', '页面地址', '图片地址'] writer = csv.DictWriter(csvfile, fieldnames=filednames) writer.writeheader() for book in books: # print(book) # print(book.attrs) # 获取子节点&lt;img&gt; # (book.children)[0] if len(list(book.children)[0].attrs) == 3: img = list(book.children)[0].attrs['data-original'] else: img = list(book.children)[0].attrs['src'] writer.writerow({'书名': book.attrs['title'], '页面地址': book.attrs['href'], '图片地址': img}) 看到这里，你可能会问为什么不把编码指定为 gb2312 呢，这样用 ecxel 打开就不会乱码了？原因是当书名全部为英文单词时，使用 gb2312 编码，writer.writerow()会出现编码错误的问题。 如果你要用 excel 打开 PythonBook.csv文件, 你则需多执行下面几步： 1) 打开 Excel 2) 执行“数据”-&gt;“自文本” 3) 选择 CSV 文件，出现文本导入向导 4) 选择“分隔符号”，下一步 5) 勾选“逗号”，去掉“ Tab 键”，下一步，完成 6）在“导入数据”对话框里，直接点确定 爬取结果最后，我们将上面代码整合起来即可。这里就不把代码贴出来了，具体阅读原文即可查看源代码。我就把爬取结果截下图： 写在最后这次实战算是结束了，但是我们不能简单地满足，看下程序是否有优化的地方。我把该程序不足的地方写出来。 该程序是单线程，没有使用多线程，执行效率不够高。 没有应用面向对象编程思想，程序的可扩展性不高。 没有使用随机 User-Agent 和 代理，容易被封 IP。 源码如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/79.html"},{"title":"你为何要那么拼命？","text":"很多人往往有这样的状态，当完成一个目标之后，就守着这收获的成果沾沾自喜。你觉得考上了大学，就可以整天逃课沉迷于游戏？你觉得找到工作了，就可以准时下班走人，天天潇潇洒洒？答案是否定的。如果你是现在身处这状态，说明你对自己未来人生没有什么规划，是对自己极其不负责任的表示。你试问你自己，是否有在为自己拼命？ 01.何炅, 这个名字已经家喻户晓了。大家都知道他是大名鼎鼎的湖南电视台主持人。平时我们都在享受何老师给我们带来快乐，可知背后辛酸的汗水呢？何老师在读大学三年级时，每天需要应对高难度的阿拉伯语的学习，还担任着学生会的工作，兼职文艺部和宣传部的“要职”。除此之外，他还在央视担任支持，平日要撰写台本以及录影，有时还要出差去外地录制。每天他都很晚才回到学校，同学们可能已经下了晚自习，甚至都已经入睡了。而他只能先在学生会里将自己学生干部的事情都做完后，再回到宿舍开始预习第二天上课要准备的内容。 02.彭宇年轻的时还是街头一个小混混。当他树立人生中第一个梦想————进入电视台工作，生活从此跟之前是天壤之别。他时常对着电视机练习如何应对突发，还报名参加骗子的演员培训班。到了后来，他听说北京机会多，毅然决定北漂，每天在北京电影学院门口等着接活，做群众演员。 为何现在要拼命？只为自己，只为自己生活得更好，只为青春无悔。 你是否感到很震撼？看下你目前的生活状态，如果整天这么懒散。那么你该制定短期目标，并为之拼命一把。","link":"/15.html"},{"title":"详解 Requests 库的用法","text":"如果你把上篇多线程和多进程的文章搞定了，那么要恭喜你了 。你编写爬虫的能力上了一个崭新的台阶。不过，我们还不能沾沾自喜，因为任重而道远。那么接下来就关注下本文的主要内容。本文主要介绍 urllib 库的代替品 —— Requests。 Requests 简介引用 Requests 官网的说明： Requests is the only Non-GMO HTTP library for Python, safe for human consumption. Requests 官方的介绍语是多么霸气。之所以能够这么霸气，是因为 Requests 相比 urllib 在使用方面上会让开发者感到更加人性化、更加简洁、更加舒服。并且国外一些知名的公司也在使用该库，例如 Google、Microsoft、Amazon、Twitter 等。因此，我们就更加有必要来学习 Request 库了。在学习之前，我们来看下它究竟具有哪些特性？ 具体如下： Keep-Alive &amp; 连接池 国际化域名和 URL 带持久 Cookie 的会话 浏览器式的 SSL 认证 自动内容解码 基本/摘要式的身份认证 优雅的 key/value Cookie 自动解压 Unicode 响应体 HTTP(S) 代理支持 文件分块上传 流下载 连接超时 分块请求 支持 .netrc 安装 Requests古人云：“工欲善其事，必先利其器”。在学习 Requests 之前，我们应先将库安装好。安装它有两种办法。 方法1：通过 pip 安装 比较推荐使用这种方式，既简单又方便管理。 1234pip install requests# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install requests 方法2：通过源码安装 先通过 git 克隆源码库： 1git clone git://github.com/kennethreitz/requests.git 或者直接到 github 网页上下载源码压缩包 接着进入到 requests 目录中执行以下命令： 1python setup.py install 发起请求有了前面学习 urllib 库的经验，现在我们学习 Requests 应该会更加容易上手。 简单抓取网页我们使用 Requests 向百度贴吧发起一个 HTTP 请求，并获取到它页面的源代码。 12345import requests# 使用 get 方式请求response = requests.get('https://tieba.baidu.com/')print(response.text) 那么使用 POST 请求网页，代码又该怎么写呢？相信答案已经浮现在你脑海中了。没错，就是将 get 换成 post 即可。 12345import requests# 使用 post 方式请求response = requests.post('https://tieba.baidu.com/')print(response.text) 传递 URL 参数我们在请求网页时，经常需要携带一些参数。Requests 提供了params关键字参数来满足我们的需求。params 是一个字符串字典，我们只要将字典构建并赋给 params 即可。我们也无须关心参数的编码问题，因为 Requests 很人性化，会将我们需要传递的参数正确编码。它的具体用法如下： 123456789import requestsurl = 'http://httpbin.org/get'params = {'name': 'Numb', 'author': 'Linkin Park'}# params 支持列表作为值# params = {'name': 'Good Time', 'author': ['Owl City', 'Carly Rae Jepsen']}response = requests.get(url, params=params)print(response.url)print(response.text) 如果字典为空是不会被拼接到 URL中的。另外，params 的拼接顺序是随机的，而不是写在前面就优先拼接。 123#运行结果如下：http://httpbin.org/get?name=Numb&amp;author=Linkin+Parkhttp://httpbin.org/get?name=Good+Time&amp;author=Owl+City&amp;author=Carly+Rae+Jepsen 你也许会疑问，为什么会有多了个”+”号呢？这个是 Requests 为了替代空格，它在请求时会自动转化为空格的。 构造请求头为了将 Requests 发起的 HTTP 请求伪装成浏览器，我们通常是使用headers关键字参数。headers 参数同样也是一个字典类型。具体用法见以下代码： 12345678910111213141516171819202122232425262728import requestsurl = 'https://tieba.baidu.com/'headers = { 'user-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36' # 还可以设置其他字段。}response = requests.get(url, headers=headers)print(response.url)print(response.text)``` ### 使用 data 参数提交数据data 参数通常结合 POST 请求方式一起使用。如果我们需要用 POST 方式提交表单数据或者JSON数据，我们只需要传递一个字典给 data 参数。- 提交表单数据我们使用测试网页`http://httpbin.org/post`来提交表单数据作为例子进行展示。```pythonimport requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, data=data) print(response.text) 运行结果如下：我们会看到http://httpbin.org/post页面打印我们的请求内容中，有form字段。 12345678910111213141516171819202122{ \"args\": {}, \"data\": \"\", \"files\": {}, \"form\": { \"pass\": \"admin\", \"user\": \"admin\" }, \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Cache-Control\": \"max-age=259200\", \"Connection\": \"close\", \"Content-Length\": \"21\", \"Content-Type\": \"application/x-www-form-urlencoded\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.11.1\" }, \"json\": null, \"origin\": \"14.*.*.*\", \"url\": \"http://httpbin.org/post\"} 提交 JSON 数据 在HTTP 请求中，JSON 数据是被当作字符串文本。所以，我们使用 data 参数的传递 JSON 数据时，需要将其转为为字符串。我们继续使用上文的代码做演示。 12345678910import jsonimport requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, data=json.dumps(data))print(response.text) 你可以拿下面的运行结果和提交表单数据的运行结果做下对比，你会了解更加清楚两者的差异。 123456789101112131415161718192021{ \"args\": {}, \"data\": \"{\\\"pass\\\": \\\"admin\\\", \\\"user\\\": \\\"admin\\\"}\", \"files\": {}, \"form\": {}, \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Cache-Control\": \"max-age=259200\", \"Connection\": \"close\", \"Content-Length\": \"34\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.11.1\" }, \"json\": { \"pass\": \"admin\", \"user\": \"admin\" }, \"origin\": \"14.*.*.*\", \"url\": \"http://httpbin.org/post\"} 那是否有更加简便的方法来传递 JSON 数据？Requests 在 2.4.2 版本新增该功能。我们可以使用 json 参数直接传递，然后它会被自动编码。 123456789import requestsurl = 'http://httpbin.org/post'data = { 'user': 'admin', 'pass': 'admin'}response = requests.post(url, json=data)print(response.text) 使用代理有些网站做了浏览频率限制。如果我们请求该网站频率过高，该网站会被封掉我们的 IP，禁止我们的访问。所以我们需要使用代理来突破这“枷锁”。这里需要用到proxies参数，proxies 也是一个字典类型。具体用法如下： 123456789101112import requestsurl = 'https://tieba.baidu.com/'proxies = { 'http':\"web-proxy.oa.com:8080\", 'https':\"web-proxy.oa.com:8080\" # 若你的代理需要使用HTTP Basic Auth，可以使用 http://user:password@host/ 语法： # \"http\": \"http://user:pass@27.154.181.34:43353/\"}response = requests.get(url, proxies=proxies)print(response.url)print(response.text) 除了支持 HTTP 代理，Requests 在 2.10 版本新增支持 SOCKS 协议的代理。也就是说，Requests 能够使用 ShadowSocks 代理。看到这里，你的内心是不是有点小激动？使用 SOCKS 代理，需要额外安装一个第三方库，我们就使用 pip 来安装。 1pip install requests[socks] 安装成功之后，就可以正常使用了，用法跟 HTTP 代理相关。具体见代码： 1234proxies = { 'http': 'socks5://user:pass@host:port', 'https': 'socks5://user:pass@host:port'} 设置请求超时我们使用代理发起请求，经常会碰到因代理失效导致请求失败的情况。因此，我们对请求超时做下设置。当发现请求超时，更换代理再重连。 1response = requests.get(url, timeout=3) 如果你要同时设置 connect 和 read 的超时时间，可以传入一个元组进行设置。 1response = requests.get(url, timeout=(3, 30)) 使用 Cookie想在响应结果中获取 cookie 的一些值，可以直接访问。 1response.cookies['key'] # key 为 Cookie 字典中键 想发送 cookies 到服务器，可以使用cookies参数。同样该参数也是字典类型 1234567url = 'http://httpbin.org/cookies'# cookies = dict(domain='httpbin.org')cookies = { 'domain':'httpbin.org',}response = requests.get(url, cookies=cookies)print(response.text) 响应结果我们跟Python 打交道，摆脱不了编码的问题。使用 Requests 请求，我们无需担心编码问题。感觉 Requests 真的是太人性化了。请求发出后，Requests 会基于 HTTP 头部对响应的编码作出有根据的推测。当你访问 response .text 之时，Requests 会使用其推测的文本编码。 12response = requests.get(url)print(response.text) 如果你想改变 response 的编码格式，可以这么做： 1response.encoding = 'UTF-8' 二进制响应内容对于非文本请求， 我们能以字节的方式访问请求响应体。Requests 会自动为我们解码 gzip 和 deflate 传输编码的响应数据。例如，以请求返回的二进制数据创建一张图片，你可以使用如下代码： 1234from PIL import Imagefrom io import BytesIOi = Image.open(BytesIO(response.content)) JSON 响应内容Requests 中也有一个内置的 JSON 解码器，助我们处理 JSON 数据： 12345import requestsurl = 'https://github.com/timeline.json'response = requests.get(url)print(response.json()) 如果 JSON 解码失败， response .json() 就会抛出一个异常。例如，响应内容是 401 (Unauthorized)，尝试访问 response .json() 将会抛出 ValueError: No JSON object could be decoded 异常。 响应状态码我们需要根据响应码来判断请求的结果，具体是这样获取状态码： 1response.status_code Requests 内部提供了一个状态表，如果有需要对状态码进行判断，可以看下requests.codes的源码。 高级用法重定向与请求历史有些页面会做一些重定向的处理。Requests 又发挥人性化的特性。它在默认情况下，会帮我们自动处理所有重定向，包括 301 和 302 两种状态码。我们可以使用response .history来追踪重定向。 Response.history是一个 Response 对象的列表，为了完成请求而创建了这些对象。这个对象列表按照从最老到最近的请求进行排序。 如果我们要禁用重定向处理，可以使用allow_redirects参数： 1response = requests.get(url, allow_redirects=False) 会话Requests 支持 session 来跟踪用户的连接。例如我们要来跨请求保持一些 cookie，我们可以这么做： 1234567s = requests.Session()s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')r = s.get(\"http://httpbin.org/cookies\")print(r.text)# '{\"cookies\": {\"sessioncookie\": \"123456789\"}}' 身份认证有些 web 站点都需要身份认证成功之后才能访问。urllib 具备这样的功能，Requests 也不例外。Requests 支持基本身份认证（HTTP Basic Auth）、netrc 认证、摘要式身份认证、OAuth 1 认证等。 基本身份认证 许多要求身份认证的web服务都接受 HTTP Basic Auth。这是最简单的一种身份认证，并且 Requests 对这种认证方式的支持是直接开箱即可用。HTTP Basic Auth 用法如下： 123456from requests.auth import HTTPBasicAuthurl = 'http://httpbin.org/basic-auth/user/passwd'requests.get(url, auth=HTTPBasicAuth('user', 'pass'))# 简写的使用方式requests.get(url, auth=('user', 'pass')) 摘要式身份认证 摘要式是 HTTP 1.1 必需的第二种身份验证机制。这种身份验证由用户名和密码组成。随后将用 MD5（一种单向哈希算法）对摘要式身份验证进行哈希运算，并将其发送到服务器。具体用法如下： 1234from requests.auth import HTTPDigestAuthurl = 'http://httpbin.org/digest-auth/auth/user/passwd/MD5'requests.get(url, auth=HTTPDigestAuth('user', 'pass')) OAuth 认证 OAuth（开放授权）认证在我们的生活中随处可见。Requests 同样也支持这中认证方式，其中包括 OAuth 1.0 和 OAuth 2.0。如果你需要用到该认证，你需要安装一个支持库requests-oauthlib。我以 OAuth 1.0 认证作为例子进行讲解： 123456789import requestsfrom requests_oauthlib import OAuth1url = 'https://api.twitter.com/1.1/account/verify_credentials.json'auth = OAuth1('YOUR_APP_KEY', 'YOUR_APP_SECRET', 'USER_OAUTH_TOKEN', 'USER_OAUTH_TOKEN_SECRET')requests.get(url, auth=auth)","link":"/711.html"},{"title":"干将莫邪” —— Xpath 与 lxml 库","text":"前面的文章，我们已经学会正则表达式以及 BeautifulSoup库的用法。我们领教了正则表达式的便捷，感受 beautifulSoup 的高效。本文介绍也是内容提取的工具 —— Xpath，它一般和 lxml 库搭配使用。所以，我称这两者为“干将莫邪”。 Xpath 和 lxml Xpath XPath即为XML路径语言，它是一种用来确定XML（标准通用标记语言的子集）文档中某部分位置的语言。XPath 基于 XML 的树状结构，提供在数据结构树中找寻节点的能力。 Xpath 原本是用于选取 XML 文档节点信息。XPath 是于 1999 年 11 月 16 日 成为 W3C 标准。因其既简单方便又容易，所以它逐渐被人说熟知。 lxml lxml 是功能丰富又简单易用的，专门处理 XML 和 HTML 的 Python 官网标准库。 Xpath 的语法正则表达式的枯燥无味又学习成本高，Xpath 可以说是不及其万分之一。所以只要花上 10 分钟，掌握 Xpath 不在话下。Xpath 的语言以及如何从 HTML dom 树中提取信息，我将其归纳为“主干 - 树支 - 绿叶”。 “主干” —— 选取节点抓取信息，我们需知道要从哪里开始抓取。因此，需要找个起始节点。Xpath 选择起始节点有以下可选： 表达式 描述 nodename 选取标签节点的所有子节点。 / 从根节点选取，html DOM 树的节点就是 html。 // 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。 . 选择当前节点，一般用于二级提取。 .. 选取当前节点的父节点，在二级提取时用到。 @ 选取属性。 我们通过以下实例来了解其用法： 路径表达式 描述 xpath(‘//div’) 选取 div 元素的所有子节点。 xpath(‘/div’) 选取 div 元素作为根节点。如果同级有多个 div ，那么所有 div 都会被选为根节点。 xpath(‘/div/span’) 选取属于 div 元素下所有 span 元素节点。如果 span 有多个，也会被选中。 xpath(‘//div’) 选取所有 div 元素节点，不管它们在文档的位置。 xpath(‘//div/span’) 选取 div 元素下的所有 span 元素节点，不管位于 div 之下的什么位置 xpath(“//@[class=’content’]”) 选取包含属性 class 的值为 content 的节点，不管是 div 元素还是其他元素 xpath(“//@[id=’center’]”) 选取属性 id 的值为 center 的节点，不管是 div 元素还是其他元素 如果你对于提取节点没有头绪的时候，可以使用通配符来暂时替代。等查看输出内容之后再进一步确认。 路径表达式 描述 xpath(‘/div/*’) 选取 div 元素节点下的所有节点 xpath(‘/div[@*]’) 选取所有带属性的 div 元素节点 “分支” —— 关系节点与谓语这一步的过程其实是通过起点一步步来寻找最终包含我们所需内容的节点。我们有时需要使用到相邻节点信息。因此，我们需要了解关系节点或者谓语。 关系节点 一般而言，DOM 树中一个普通节点具有父节点、兄弟节点、子节点。当然也有例外的情况。这些有些节点比较特殊，可能没有父节点，如根节点；也有可能是没有子节点，如深度最大的节点。Xpath 也是有支持获取关系节点的语法。 关系 路径表达式 描述 parent（父） xpath(‘./parent::*’) 选取当前节点的父节点 ancestor（先辈） xpath(‘./ancestor::*’) 选取当前节点的所有先辈节点，包括父、祖父等 ancestor-or-self（先辈及本身） xpath(‘./ancestor-or-self::*’) 选取当前节点的所有先辈节点以及节点本身 child（子） xpath(‘./child::*’) 选取当前节点的所有子节点 descendant（后代） xpath(‘./descendant::*’) 选取当前节点的所有后代节点，包括子节点、孙节点等 following xpath(‘./following ::*’) 选取当前节点结束标签后的所有节点 following-sibing xpath(‘./following-sibing::*’) 选取当前节点之后的兄弟节点 preceding xpath(‘./preceding::*’) 选取当前节点开始标签前的所有节点 preceding-sibling xpath(‘./parent::*’) 选取当前节点之前的兄弟节点 self（本身） xpath(‘./self::*’) 选取当前节点本身 谓语 谓语用来查找某个特定的节点或者包含某个指定的值的节点。同时，它是被嵌在方括号中的。 路径表达式 描述 xpath(‘./body/div[1]’) 选取 body 元素节点下的第一个 div 子节。 xpath(‘./body/div[last()]’) 选取 body 元素节点下的最后一个 div 子节。 xpath(‘./body/div[last()-1]’) 选取 body 元素节点下的倒数第二个 div 子节。 xpath(‘./body/div[position()-3]’) 选取 body 元素节点下的前二个 div 子节。 xpath(‘./body/div[@class]’) 选取 body 元素节点下的所有带有 class 属性的 div 子节。 xpath(“./body/div[@class=’content’]”) 选取 body 元素节点下的 class 属性值为 centent 的 div 子节。 “绿叶” —— 节点内容以及属性到了这一步，我们已经找到所需内容的节点了。接下来就是获取该节点中的内容了。Xpath 语法提供了提供节点的文本内容以及属性内容的功能。 路径表达式 描述 text() 获取节点的本文内容 @属性 获取节点的属性内容 具体用法见以下实例： 路径表达式 描述 xpath(‘./a/text()’) 获取当前节点中 a 元素节点中的本文内容 xpath(‘./a/@href’) 获取当前节点中 a 元素节点中的 href 属性的内容 lxml 的用法安装 lxmlpip 是安装库文件的最简便的方法，具体命令如下： 1234pip install lxml# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install lxml 使用 lxmllxml 使用起来是比较简单的。我们首先要使用 lxml 的 etree 将 html 页面进行初始化，然后丢给 Xpath 匹配即可。具体用法如下： 12345from lxml import etreehtml = requests.get(url) # 使用 requests 请求网页selector = etree.HTML(html.text)content = selector.xpath('//a/text()') 没错，就这短短几行代码即可完成信息提取。值得注意的是：xpath 查找匹配返回的类型有可能是一个值，也有可能是一个存放多个值的列表。这个取决于你的路径表达式是如何编写的。","link":"/813.html"},{"title":"应该如何阅读？","text":"我最近阅读民主与建设出版社出版的《如何阅读一本书》，自己收获颇多。这本书算是经典之作。以通俗的语言告诉我们如何选择书籍？究竟要以什么方法来阅读一本书？我将自己学到的分享出来。希望能帮助大家提高阅读速度，把书籍读得更加明白，记得更牢固。 为什么要阅读？在进入本文主题之前，我们先思考一个问题 —— 我们为什么要阅读？你可以不必急着回答这个问题，带着问题来往下读。 古人有很多名句鼓励学子多读书，例如宋真宗赵恒的《劝学诗》，其中有我们耳熟能详的语句： 书中自有千种粟书中自有黄金屋书中车马多如簇书中自有颜如玉 由此可见，阅读是手段。我们可以通过读书来获得赖以谋能的技能。那么问题来了？我们要阅读什么书？随便阅读一本书就能获取知识吗？答案是否定的。所以我们要读好书，还要掌握些阅读的技巧。 阅读什么书？市面上书籍种类琳琅满目。我们该如何选择书籍？先来看看书籍的分类 第一类：如同主食 能够解决职业、生活、生理、心理等方面的现实问题的书籍都可以称为“主食”。”主食”是我们的刚需。所以我们就应该花大量时间去阅读。举个栗子，假如你是一名 Android 粉丝，想通过学习 Android 开发来谋生。那么你应该阅读 Android 开发的书籍，例如《第一行代码》、《Android 源码设计模式解析与实战》等。 第二类：如同美食 这类书籍是不求针对人生的现实问题，却可以满足思想要求。对于这些书籍，我们应该重“质”不重“量”。我们不知道怎么选择这类书籍时，可以根据一些名家推荐或者订阅一些名家的微信公众号。例如，张哥的 stomzhang 公众号。张哥的每篇推文，我基本上都有仔细阅读。我自己订阅张哥的公众号一年多了，提高不仅仅是专业技能，更是视野。 第三类：如同蔬菜、水果 可以理解为工具书。这类书籍不仅可以帮助我们查找不了解的字词、概念、数据等信息，也可以帮助我们掌握通用的方法与技巧。 第四类：如同甜点、零食 这类书籍是用于娱乐、消遣、满足休闲需求。一些网络小说和娱乐性图书，包括一部分畅销书都属于此列。对于这类书籍，我们只可偶尔阅读，但不能过。 阅读方法阅读可以分为四个层次，不同的阅读层次适用不同的阅读方法。具体分类如下：1）基础阅读（Elementray Reading）2）检视阅读（Inspectional Reading）3）分析阅读（Analytical Reading）4）主题阅读（Syntopical Reading） 第一层：基础阅读基础阅读，即处于四肢阶段的阅读。所以又被称为初级阅读。我们在小学阶段已经学会了，因为这一层次的阅读要求是认识文字并了解文字的意思。 第二层：检视阅读检视阅读，即系统化略读。类似我们初高中做语文的阅读理解题目。 检视阅读是非常有价值的阅读方式。通过检视阅读，我们可以了解一本书“主要讲什么内容”、“书的结构如何”、“各章重点讲什么”，进而判断这本书是否值得分析阅读或主题阅读。 对一本书进行检视阅读，可以按照以下步骤：A、看书名、副题、作者简介、序言，大致清楚作者的写作风格、作者在什么背景下著作本书的。B、研究目录，了解作者的写作路线。C、粗略地阅读一下各章的内容，遇到不懂的部分就跳过去。 看到这里，你可能有这样的疑问：我没有将一本书读完，只阅读其中几章，这算是阅读吗？算。诸葛孔明的“略其大意”，陶渊明的“不求甚解”都算是这种阅读方法。 第三层：分析阅读分析阅读，即完整阅读。也就是精读。通过分析阅读，我们可以对全书有更精准的把握，复述全书各部分的大意及重要细节，然后使之成为自己的知识。 这种阅读方法更适合“主食”和“美食”类图书。使用分析阅读就是带着四个问题去阅读： 1）这本书究竟讲了什么？ 回到这个问题，事实上就是做到以下三步：A、对书的体裁和主题进行确认。B、自己组织语言表述整本书的内容C、按照全书的结构顺序或逻辑顺序列举全书的大纲，并将各个部分的大纲也列出来。思维导图就可以派上用场了。 2）作者通过这本书解决了什么问题？ 回答这个问题，也需要一步一步找出作者的主要想法、声明与论点，并形成自己的判断。首先，找出可以表达作者观点的关键字，与作者达成共识。然后，在最重要的语句中，提炼关键字，抓住作者的主旨。最后，根据书中的内容确定作者已经解决了哪些问题，还有哪些是没有解决的。 3）这本书说得有道理吗？ 对本书进行评论就是这个问题的答案。但在没有对问题 2 进行完整的解答之前，不要轻易去尝试。评论一本书不要带有个人感情色彩。 我个人觉得评论一本书类似写议论文，要有理有据，求同存异。 4）这本书与我有什么关系？ 这个问题与阅读效用有密切相关，简单地回答，就是“有用”或者“部分有用”。 如果一本书告诉我们一些咨询，我们一定要问一问这些咨询有什么意义；如果一本书不仅提供咨询，还对我们有所启发，就更应该找出书中更深的含意或其他相关的建议，以获得更多启示。 第四层：主题阅读 主题阅读是主动的、专一的、大量的阅读。 主题阅读，顾名思义就是定个主题，然后使用检视阅读来筛选与主题有关的书籍，再对每本书中与主题极为相关的具体章节进行精读来建立自己的主旨（论点）。 这种阅读方法带有很强的阅读性，不能短时间能掌握的，需要长期的阅读积累以及阅读训练。","link":"/812.html"},{"title":"用 Python 学习数据结构, 有它就不用愁","text":"数据结构，我们对它已经是耳熟能详。对于计算机相关专业的大学生来说，它是一门专业必修课。从事软件开发的人员则把它作为谋生必备技能。这充分体现数据结构的重要性。因此，我们对数据结构是不得不学。 虽然数据结构的实现不限制语言，但市面上很多教程书籍都是以 C 语言作为编程语言进行讲解。如果你喜欢且在学习 Python，可能会陷入苦于这样的烦恼中。那就是没有 Python 版本的数据结构实现代码。莫慌！我给大家推荐一个第三方库，它能让你这种烦恼立刻云消雾散。 它就是Pygorithm 地址：https://github.com/OmkarPathak/pygorithm Pygorithm 是由一个热心肠的印度小哥编写的开源项目。他编写创建该库的初衷是处于教学目的。我们不仅可以阅读源码的方式学习数据结构，而且可以把它当做现成工具来使用。 安装安装 python 库，我推荐使用 pip 方式，方便又省事。 1234pip install Pygorithm# 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install Pygorithm 支持的类型Pygorithm 实现的数据结构类型有以下这几种，括号中表示包名。 栈 栈 (data_structures.stack.Stack) 中缀表达式转换为后缀表达式 (data_structures.stack.InfixToPostfix) 队列 队列 (data_structures.queue.Queue) 双端队列 (data_structures.queue.Deque) 链表 单向链表 (data_structures.linked_list.SinglyLinkedList) 双向链表 (data_structures.linked_list.DoublyLinkedList) 树 二叉树 (data_structures.tree.BinaryTree) 搜索二叉树 (data_structures.tree.BinarySearchTree) 图 图 (data_structures.graph.Graph) 拓扑排序 (data_structures.graph.TopologicalSort) 有向图 (data_structures.graph.CheckCycleDirectedGraph) 无向图 (data_structures.graph.CheckCycleUndirectedGraph) 堆 堆 (data_structures.heap.Heap) 字典树 字典树 （data_structures.trie.Trie） 常见算法你也许没有想到吧。Pygorithm 中也实现一些常见的路径搜索、查找、排序等算法。 常见的路径搜索算法： Dijkstra(迪杰斯特拉) Unidirectional AStar（单向 A*算法） BiDirectional AStar（双向 A*算法） 常见的查找算法： Linear Search (线性查找) Binary Search (二分法查找) Breadth First Search (广度优先搜索) Depth First Search (深度优先搜索) 常见的排序算法： bubble_sort（冒泡算法） bucket_sort（桶排序） counting_sort （计数排序） heap_sort（堆排序） insertion_sort（插入排序） merge_sort（归并排序） quick_sort （快速排序） selection_sort（选择排序） shell_sort（希尔排序）","link":"/815.html"},{"title":"学会运用爬虫框架 Scrapy (三)","text":"上篇文章介绍 Scrapy 框架爬取网站的基本用法。但是爬虫程序比较粗糙，很多细节还需打磨。本文主要是讲解 Scrapy 一些小技巧，能让爬虫程序更加完善。 设置 User-agentScrapy 官方建议使用 User-Agent 池, 轮流选择其中一个常用浏览器的 User-Agent来作为 User-Agent。scrapy 发起的 http 请求中 headers 部分中 User-Agent 字段的默认值是Scrapy/VERSION (+http://scrapy.org)，我们需要修改该字段伪装成浏览器访问网站。 1) 同样在 setting.py 中新建存储 User-Agent 列表, 1234567891011121314151617181920212223242526272829303132333435UserAgent_List = [ \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2224.3 Safari/537.36\", \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\", \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 4.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\", \"Mozilla/5.0 (X11; OpenBSD i386) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1944.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.3319.102 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2309.372 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2117.157 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1866.237 Safari/537.36\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.137 Safari/4E423F\", \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1\", \"Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10; rv:33.0) Gecko/20100101 Firefox/33.0\", \"Mozilla/5.0 (X11; Linux i586; rv:31.0) Gecko/20100101 Firefox/31.0\", \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20130401 Firefox/31.0\", \"Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20100101 Firefox/31.0\", \"Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16\", \"Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14\", \"Mozilla/5.0 (Windows NT 6.0; rv:2.0) Gecko/20100101 Firefox/4.0 Opera 12.14\", \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0) Opera 12.14\", \"Opera/9.80 (Windows NT 5.1; U; zh-sg) Presto/2.9.181 Version/12.00\"] 2) 在 middlewares.py 文件中新建一个名为RandomUserAgentMiddleware的代理中间层类 123456789101112import randomfrom scrapy_demo.settings import UserAgent_Listclass RandomUserAgentMiddleware(object): '''动态随机设置 User-agent''' def process_request(self, request, spider): ua = random.choice(UserAgent_List) if ua: request.headers.setdefault('User-Agent', ua) print(request.headers) 3) 在 settings.py 中配置 RandomUserAgentMiddleware , 激活中间件 123456789DOWNLOADER_MIDDLEWARES = { # 第二行的填写规则 # yourproject.middlewares(文件名).middleware类 # 设置 User-Agent 'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None, 'scrapy_demo.middlewares.RandomUserAgentMiddleware': 400, # scrapy_demo 是你项目的名称} 禁用cookies有些站点会使用 cookies 来发现爬虫的轨迹。因此，我们最好禁用 cookies 在 settings.py 文件中新增以下配置。 123# 默认是被注释的, 也就是运行使用 cookies# Disable cookies (enabled by default)COOKIES_ENABLED = False 设置下载延迟当 scrapy 的下载器在下载同一个网站下一个页面前需要等待的时间。我们设置下载延迟, 可以有效避免下载器获取到下载地址就立刻执行下载任务的情况发生。从而可以限制爬取速度, 减轻服务器压力。 在 settings.py 文件中新增以下配置。 1234567# 默认是被注释的# Configure a delay for requests for the same website (default: 0)# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay# See also autothrottle settings and docsDOWNLOAD_DELAY = 3 # 单位是秒, 上述设置是延迟 3s。 # 同时还支持设置小数, 例 0.3, 延迟 300 ms 设置代理有些网站设置反爬虫机制，这使得我们的爬虫程序可能爬到一定数量网页就爬取不下去了。我们需要装饰下爬虫，让它访问网站行为更像类人行为。使用 IP 代理池能突破大部分网站的限制。 1) 我们可以通过国内一些知名代理网站(例如：迅代理、西刺代理)获取代理服务器地址。 我将自己收集一些代理地址以列表形式保存到 settings.py 文件中 1234567891011# 代理地址具有一定的使用期限, 不保证以下地址都可用。PROXY_LIST = [ \"https://175.9.77.240:80\", \"http://61.135.217.7:80\", \"http://113.77.101.113:3128\" \"http://121.12.42.180:61234\", \"http://58.246.59.59:8080\", \"http://27.40.144.98:808\", \"https://119.5.177.167:4386\", \"https://210.26.54.43:808\",] 2) 在 middlewares.py 文件中新建一个名为ProxyMiddleware的代理中间层类 12345678910import randomfrom scrapy_demo.settings import PROXY_LISTclass ProxyMiddleware(object): # overwrite process request def process_request(self, request, spider): # Set the location of the proxy # request.meta['proxy'] = \"https://175.9.77.240:80\" request.meta['proxy'] = random.choice(PROXY_LIST) 3) 在 settings.py 文件中增加代理配置： 123456789DOWNLOADER_MIDDLEWARES = { # 第二行的填写规则 # yourproject.middlewares(文件名).middleware类 # 设置代理 'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 110, 'scrapy_demo.middlewares.ProxyMiddleware': 100, # scrapy_demo 是你项目的名称} 除此之外，如果你比较狠的话，可以采用 VPN + Tor 方式来突破反爬虫机制。 减小下载超时如果您对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)， 减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。 在 settings.py 文件中增加配置： 1DOWNLOAD_TIMEOUT = 15 页面跟随规则在爬取网站时，可能一些页面是我们不想爬取的。如果使用 最基本的 Spider，它还是会将这些页面爬取下来。因此，我们需要使用更加强大的爬取类CrawlSpider。 我们的爬取类继承 CrawlSpider，必须新增定义一个 rules 属性。rules 是一个包含至少一个 Rule（爬取规则）对象的 list。 每个 Rule 对爬取网站的动作定义了特定表现。CrawlSpider 也是继承 Spider 类，所以具有Spider的所有函数。 Rule 对象的构造方法如下： 1Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None) 我们在使用 Rule 一般只会用到前面几个参数，它们作用如下： link_extractor： 它是一个 Link Extractor 对象。 其定义了如何从爬取到的页面提取链接。link_extractor既可以自己定义，也可以使用已有LinkExtractor类，主要参数为： allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。 deny：与这个正则表达式(或正则表达式列表)不匹配的 Url 一定不提取。 allow_domains：会被提取的链接的domains。 deny_domains：一定不会被提取链接的domains。 restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。还有一个类似的restrict_css callback：从 link_extractor 中每获取到链接时将会调用该函数。它指定一个回调方法。会返回一个包含 Item 对象的列表。 follow：它 是一个布尔(boolean)值，指定了根据该规则从 response 提取的链接是否需要跟进。 如果 callback 为None， follow 默认设置为 True ，否则默认为 False 。 process_links：从link_extractor中获取到链接列表时将会调用该函数。它同样需要指定一个方法，该方法主要用来过滤 Url。 我以爬取豆瓣电影 Top 250 页面为例子进行讲解如何利用 rules 进行翻页爬取。 在页面的底部，有这样的分页。我们想通过抓取翻页 url 进行下一个页面爬取。 通过分析页面可知，链接的规则是 1https://movie.douban.com/top250?start=当前分页第一个电影序号&amp;filter=分页数 我使用 xpath 来匹配，当然你也可以使用正则表达式或者 CSS 选择器。rules 可以这样定义： 1234567rules = ( Rule(LinkExtractor(allow=(), restrict_xpaths=('//div[@class=\"paginator\"]',)), follow=True, callback='parse_item', process_links='process_links', ),) 完整的 spider 代码如下： 12345678910111213141516171819202122# -*- coding: utf-8 -*-from scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorclass DoubanTop250(CrawlSpider): name = 'movie_douban' allowed_domains = ['douban.com'] start_urls = [\"https://movie.douban.com/top250\"] rules = ( Rule(LinkExtractor(allow=(), restrict_xpaths=('//div[@class=\"paginator\"]',)), follow=True, callback='parse_item', process_links='process_links', ), ) def parse_item(self, response): # 解析 response, 将其转化为 item yield item 另外，LinkExtractor 参数中的 allow() 和 deny() ，我们也是经常使用到。规定爬取哪些页面是否要进行爬取。 7 动态创建Item类对于有些应用，item的结构由用户输入或者其他变化的情况所控制。我们可以动态创建class。 1234567from scrapy.item import DictItem, Fielddef create_item_class(class_name, field_list): fields = { field_name: Field() for field_name in field_list } return type(class_name, (DictItem,), {'fields': fields})","link":"/918.html"},{"title":"学会运用爬虫框架 Scrapy (二)","text":"上篇文章介绍了爬虫框架 Scrapy 如何安装，以及其特性、架构、数据流程。相信大家已经对 Scrapy 有人了初步的认识。本文是 Scrapy 系列文章的第二篇，主要通过一个实例讲解 scrapy 的用法。 选取目标网络爬虫，顾名思义是对某个网站或者系列网站，按照一定规则进行爬取信息。爬取程序的首要工作当然是选定爬取目标。本次爬取目标选择是V电影，网址是http://www.vmovier.com/。爬取内容是[最新推荐]栏目的前15条短视频数据信息。具体信息包括封面、标题、详细说明以及视频播放地址。 定义 Item为什么将爬取信息定义清楚呢？因为接下来 Item 需要用到。在 Item.py 文件中，我们以类的形式以及 Field 对象来声明。其中 Field 对象其实是一个字典类型，用于保存爬取到的数据。而定义出来的字段，可以简单理解为数据库表中的字段，但是它没有数据类型。Item 则复制了标准的 dict API，存放以及读取跟字典没有差别。 V电影的 Item，我们可以这样定义： 12345678910111213import scrapyclass ScrapyDemoItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 封面 cover = scrapy.Field() # 标题 title = scrapy.Field() # 简述 dec = scrapy.Field() # 播放地址 playUrl = scrapy.Field() 编写 SpiderSpider 目录是我们爬虫程序爬取网站以及提取信息的模块。我们首先在目录下新建一个名为 VmoiveSpider 的文件。同时，该类继承scrapy.Spider。 这里我们用到的scrapy.spider.Spider 是 Scrapy 中最简单的内置 spider。继承 spider 的类需要定义父类中的属性以及实现重要的方法。 name 这个属性是非常重要的，所以必须定义它。定义 name 目的是为爬虫程序命名。因此，还要保持 name 属性是唯一的。它是 String 类型，我们在 VmoiveSpider 可以定义： 1name = 'vmoive' start_urls start_urls 是 Url 列表，也是必须被定义。可以把它理解为存放爬虫程序的主入口 url 地址的容器。 allowed_domains 可选字段。包含了spider允许爬取的域名(domain)列表(list)。 当 OffsiteMiddleware 启用时， 域名不在列表中的URL不会被跟进。根据 V 电影的 url 地址，我们可以这样定义： 1allowed_domains = ['vmovier.com'] parse(response) parser 方法是Scrapy处理下载的response的默认方法。它同样必须被实现。parse 主要负责处理 response 并返回处理的数据以及跟进的URL。该方法及其他的Request回调函数必须返回一个包含 Request 及(或) Item 的可迭代的对象。 在 scrapy_demo/sipders/VmoiveSpider 的完整代码如下： 12345678910111213141516171819202122232425262728#!/usr/bin/env python# coding=utf-8import scrapyfrom scrapy_demo.items import ScrapyDemoItemclass VmoiveSpider(scrapy.Spider): # name是spider最重要的属性, 它必须被定义。同时它也必须保持唯一 name = 'vmoive' # 可选定义。包含了spider允许爬取的域名(domain)列表(list) allowed_domains = ['vmovier.com'] start_urls = [ 'http://www.vmovier.com/' ] def parse(self, response): self.log('item page url is ==== ' + response.url) moivelist = response.xpath(\"//li[@class='clearfix']\") for m in moivelist: item = ScrapyDemoItem() item['cover'] = m.xpath('./a/img/@src')[0].extract() item['title'] = m.xpath('./a/@title')[0].extract() item['dec'] = m.xpath(\"./div/div[@class='index-intro']/a/text()\")[0].extract() print(item) yield item 运行程序在项目目录下打开终端，并执行以下命令。我们没有pipelines.py中将爬取结果进行存储，所以我们使用 scrapy 提供的导出数据命令，将 15 条电影信息导出到名为 items.json 文件中。其中 vmoive 为刚才在 VmoiveSpider 中定义的 name 属性的值。 1scrapy crawl vmoive -o items.json 运行的部分结果如下： 12345{'cover': 'http://cs.vmoiver.com/Uploads/cover/2017-09-08/59b25a504e4e0_cut.jpeg@600w_400h_1e_1c.jpg', 'dec': '15年过去了，但我依然有话说', 'title': '灾难反思纪录短片《“911”之殇》' } 深究在阅读上述代码过程中，大家可能会有两个疑问。第一，为什么要在 xpath 方法后面添加[0]？ 第二，为什么要在 [0] 后面添加 extract()方法 ? 请听我慢慢道来。 1) 添加个[0], 因为 xpath() 返回的结果是列表类型。我以获取标题内容为例子讲解不添加[0]会出现什么问题。那么代码则变为 1m.xpath('./a/@title').extract() 运行结果会返回一个列表，而不是文本信息。 1['灾难反思纪录短片《“911”之殇》'] 2）这里涉及到内建选择器 Selecter 的知识。extract()方法的作用是串行化并将匹配到的节点返回一个unicode字符串列表。看了定义，是不是更加懵逼了。那就看下运行结果来压压惊。不加上 extract() 的运行结果如下： 1&lt;Selector xpath='./a/@title' data='灾难反思纪录短片《“911”之殇》'&gt; 进阶上述代码只是在 V电影主页中提取信息，而进入电影详情页面中匹配搜索信息。因此，我们是获取不到电影的播放地址的。如何搞定这难题？我们可以在 parse 方法中做文章。parse() 前文提到它必须返回一个 Reuqest 对象或者 Item。再者， Request 中就包含 url。换句话说，我们只有获取到电影详情页的 url 地址，并在传递给返回的 Request 对象中。 因此，代码可以这么改进： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/usr/bin/env python# coding=utf-8import scrapyfrom scrapy_demo.items import ScrapyDemoItemclass VmoiveSpider(scrapy.Spider): # name是spider最重要的属性, 它必须被定义。同时它也必须保持唯一 name = 'vmoive' # 可选定义。包含了spider允许爬取的域名(domain)列表(list) allowed_domains = ['vmovier.com'] start_urls = [ 'http://www.vmovier.com/' ] def parse(self, response): self.log('item page url is ==== ' + response.url) moivelist = response.xpath(\"//li[@class='clearfix']\") for m in moivelist: item = ScrapyDemoItem() item['cover'] = m.xpath('./a/img/@src')[0].extract() item['title'] = m.xpath('./a/@title')[0].extract() item['dec'] = m.xpath(\"./div/div[@class='index-intro']/a/text()\")[0].extract() # print(item) # 提取电影详细页面 url 地址 urlitem = m.xpath('./a/@href')[0].extract() url = response.urljoin(urlitem) # 如果你想将上面的 item 字段传递给 parse_moive, 使用 meta 参数 yield scrapy.Request(url, callback=self.parse_moive, meta={ 'cover':item['cover'], 'title': item['title'], 'dec': item['dec'], }) def parse_moive(self, response): item = ScrapyDemoItem() item['cover'] = response.meta['cover'] item['title'] = response.meta['title'] item['dec'] = response.meta['dec'] item['playUrl'] = response.xpath(\"//div[@class='p00b204e980']/p/iframe/@src\")[0].extract() yield item 再次运行程序，查看运行结果。 数据持久化在实际生产中，我们很少把数据导出到 json 文件中。因为后期维护、数据查询、数据修改都是一件麻烦的事情。我们通常是将数据保存到数据库中。 我们先定义并创建数据库表 12345678DROP TABLE IF EXISTS vmoiveCREATE TABLE vmoive( id INT(6) NOT NULL AUTO_INCREMENT PRIMARY KEY , cover VARCHAR(255), title VARCHAR(255), mdec VARCHAR(255), playUrl VARCHAR(255)) DEFAULT CHARSET=utf8; 在 settings 文件中增加数据库的配置 123456789101112# Configure item pipelines 这里默认是注释的# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = { 'scrapy_demo.pipelines.ScrapyDemoPipeline': 300, # 保存到 mysql 数据库中}# Mysql 配置信息# 根据你的环境修改MYSQL_HOST = '127.0.0.1'MYSQL_DBNAME = 'vmoive' # 数据库名MYSQL_USER = 'root' # 数据库用户MYSQL_PASSWORD = '123456' # 数据库密码 在 scrapy 中，我们要在 pipeline 文件中编写处理数据存储的代码。 12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*-import pymysqlfrom scrapy_demo import settingsclass ScrapyDemoPipeline(object): def __init__(self,): self.conn = pymysql.connect( host = settings.MYSQL_HOST, db = settings.MYSQL_DBNAME, user = settings.MYSQL_USER, passwd = settings.MYSQL_PASSWORD, charset = 'utf8', # 编码要加上，否则可能出现中文乱码问题 use_unicode = False ) self.cursor = self.conn.cursor() # pipeline 默认调用 def process_item(self, item, spider): # 调用插入数据的方法 self.insertData(item) return item # 插入数据方法 def insertData(self, item): sql = \"insert into vmoive(cover, title, mdec, playUrl) VALUES(%s, %s, %s, %s);\" params = (item['cover'], item['title'], item['dec'], item['playUrl']) self.cursor.execute(sql, params) self.conn.commit()","link":"/917.html"},{"title":"Python 多进程与多线程","text":"前言：为什么有人说 Python 的多线程是鸡肋，不是真正意义上的多线程？ 看到这里，也许你会疑惑。这很正常，所以让我们带着问题来阅读本文章吧。 问题：1、Python 多线程为什么耗时更长？2、为什么在 Python 里面推荐使用多进程而不是多线程？ 基础知识现在的 PC 都是多核的，使用多线程能充分利用 CPU 来提供程序的执行效率。 线程线程是一个基本的 CPU 执行单元。它必须依托于进程存活。一个线程是一个execution context（执行上下文），即一个 CPU 执行时所需要的一串指令。 进程进程是指一个程序在给定数据集合上的一次执行过程，是系统进行资源分配和运行调用的独立单位。可以简单地理解为操作系统中正在执行的程序。也就说，每个应用程序都有一个自己的进程。 每一个进程启动时都会最先产生一个线程，即主线程。然后主线程会再创建其他的子线程。 两者的区别 线程必须在某个进行中执行。 一个进程可包含多个线程，其中有且只有一个主线程。 多线程共享同个地址空间、打开的文件以及其他资源。 多进程共享物理内存、磁盘、打印机以及其他资源。 线程的类型线程的因作用可以划分为不同的类型。大致可分为： 主线程 子线程 守护线程（后台线程） 前台线程 Python 多线程GIL其他语言，CPU 是多核时是支持多个线程同时执行。但在 Python 中，无论是单核还是多核，同时只能由一个线程在执行。其根源是 GIL 的存在。 GIL 的全称是 Global Interpreter Lock(全局解释器锁)，来源是 Python 设计之初的考虑，为了数据安全所做的决定。某个线程想要执行，必须先拿到 GIL，我们可以把 GIL 看作是“通行证”，并且在一个 Python 进程中，GIL 只有一个。拿不到通行证的线程，就不允许进入 CPU 执行。 而目前 Python 的解释器有多种，例如： CPython：CPython 是用C语言实现的 Python 解释器。 作为官方实现，它是最广泛使用的 Python 解释器。 PyPy：PyPy 是用RPython实现的解释器。RPython 是 Python 的子集， 具有静态类型。这个解释器的特点是即时编译，支持多重后端（C, CLI, JVM）。PyPy 旨在提高性能，同时保持最大兼容性（参考 CPython 的实现）。 Jython：Jython 是一个将 Python 代码编译成 Java 字节码的实现，运行在JVM (Java Virtual Machine) 上。另外，它可以像是用 Python 模块一样，导入 并使用任何Java类。 IronPython：IronPython 是一个针对 .NET 框架的 Python 实现。它 可以用 Python 和 .NET framewor k的库，也能将 Python 代码暴露给 .NET 框架中的其他语言。 GIL 只在 CPython 中才有，而在 PyPy 和 Jython 中是没有 GIL 的。 每次释放 GIL锁，线程进行锁竞争、切换线程，会消耗资源。这就导致打印线程执行时长，会发现耗时更长的原因。 并且由于 GIL 锁存在，Python 里一个进程永远只能同时执行一个线程(拿到 GIL 的线程才能执行)，这就是为什么在多核CPU上，Python 的多线程效率并不高的根本原因。 创建多线程Python提供两个模块进行多线程的操作，分别是thread和threading，前者是比较低级的模块，用于更底层的操作，一般应用级别的开发不常用。 方法1：直接使用threading.Thread() 1234567891011import threading# 这个函数名可随便定义def run(n): print(\"current task：\", n)if __name__ == \"__main__\": t1 = threading.Thread(target=run, args=(\"thread 1\",)) t2 = threading.Thread(target=run, args=(\"thread 2\",)) t1.start() t2.start() 方法2：继承threading.Thread来自定义线程类，重写run方法 12345678910111213141516import threadingclass MyThread(threading.Thread): def __init__(self, n): super(MyThread, self).__init__() # 重构run函数必须要写 self.n = n def run(self): print(\"current task：\", n)if __name__ == \"__main__\": t1 = MyThread(\"thread 1\") t2 = MyThread(\"thread 2\") t1.start() t2.start() 线程合并Join函数执行顺序是逐个执行每个线程，执行完毕后继续往下执行。主线程结束后，子线程还在运行，join函数使得主线程等到子线程结束时才退出。 1234567891011121314import threadingdef count(n): while n &gt; 0: n -= 1if __name__ == \"__main__\": t1 = threading.Thread(target=count, args=(\"100000\",)) t2 = threading.Thread(target=count, args=(\"100000\",)) t1.start() t2.start() # 将 t1 和 t2 加入到主线程中 t1.join() t2.join() 线程同步与互斥锁线程之间数据共享的。当多个线程对某一个共享数据进行操作时，就需要考虑到线程安全问题。threading模块中定义了Lock 类，提供了互斥锁的功能来保证多线程情况下数据的正确性。 用法的基本步骤： 123456#创建锁mutex = threading.Lock()#锁定mutex.acquire([timeout])#释放mutex.release() 其中，锁定方法acquire可以有一个超时时间的可选参数timeout。如果设定了timeout，则在超时后通过返回值可以判断是否得到了锁，从而可以进行一些其他的处理。 具体用法见示例代码： 123456789101112131415161718192021import threadingimport timenum = 0mutex = threading.Lock()class MyThread(threading.Thread): def run(self): global num time.sleep(1) if mutex.acquire(1): num = num + 1 msg = self.name + ': num value is ' + str(num) print(msg) mutex.release()if __name__ == '__main__': for i in range(5): t = MyThread() t.start() 可重入锁（递归锁）为了满足在同一线程中多次请求同一资源的需求，Python 提供了可重入锁（RLock）。RLock内部维护着一个Lock和一个counter变量，counter 记录了 acquire 的次数，从而使得资源可以被多次 require。直到一个线程所有的 acquire 都被 release，其他的线程才能获得资源。 具体用法如下： 1234567891011#创建 RLockmutex = threading.RLock()class MyThread(threading.Thread): def run(self): if mutex.acquire(1): print(\"thread \" + self.name + \" get mutex\") time.sleep(1) mutex.acquire() mutex.release() mutex.release() 守护线程如果希望主线程执行完毕之后，不管子线程是否执行完毕都随着主线程一起结束。我们可以使用setDaemon(bool)函数，它跟join函数是相反的。它的作用是设置子线程是否随主线程一起结束，必须在start() 之前调用，默认为False。 定时器如果需要规定函数在多少秒后执行某个操作，需要用到Timer类。具体用法如下： 12345678from threading import Timer def show(): print(\"Pyhton\")# 指定一秒钟之后执行 show 函数t = Timer(1, hello)t.start() Python 多进程创建多进程Python 要进行多进程操作，需要用到muiltprocessing库，其中的Process类跟threading模块的Thread类很相似。所以直接看代码熟悉多进程。 方法1：直接使用Process, 代码如下： 123456789from multiprocessing import Process def show(name): print(\"Process name is \" + name)if __name__ == \"__main__\": proc = Process(target=show, args=('subprocess',)) proc.start() proc.join() 方法2：继承Process来自定义进程类，重写run方法, 代码如下： 123456789101112131415161718from multiprocessing import Processimport timeclass MyProcess(Process): def __init__(self, name): super(MyProcess, self).__init__() self.name = name def run(self): print('process name :' + str(self.name)) time.sleep(1)if __name__ == '__main__': for i in range(3): p = MyProcess(i) p.start() for i in range(3): p.join() 多进程通信进程之间不共享数据的。如果进程之间需要进行通信，则要用到Queue模块或者Pipi模块来实现。 Queue Queue 是多进程安全的队列，可以实现多进程之间的数据传递。它主要有两个函数,put和get。 put() 用以插入数据到队列中，put 还有两个可选参数：blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，该方法会阻塞 timeout 指定的时间，直到该队列有剩余的空间。如果超时，会抛出 Queue.Full 异常。如果 blocked 为 False，但该 Queue 已满，会立即抛出 Queue.Full 异常。 get()可以从队列读取并且删除一个元素。同样，get 有两个可选参数：blocked 和 timeout。如果 blocked 为 True（默认值），并且 timeout 为正值，那么在等待时间内没有取到任何元素，会抛出 Queue.Empty 异常。如果blocked 为 False，有两种情况存在，如果 Queue 有一个值可用，则立即返回该值，否则，如果队列为空，则立即抛出 Queue.Empty 异常。 具体用法如下： 1234567891011from multiprocessing import Process, Queue def put(queue): queue.put('Queue 用法') if __name__ == '__main__': queue = Queue() pro = Process(target=put, args=(queue,)) pro.start() print(queue.get()) pro.join() Pipe Pipe的本质是进程之间的用管道数据传递，而不是数据共享，这和socket有点像。pipe() 返回两个连接对象分别表示管道的两端，每端都有send() 和recv()函数。 如果两个进程试图在同一时间的同一端进行读取和写入那么，这可能会损坏管道中的数据。 具体用法如下： 123456789101112from multiprocessing import Process, Pipe def show(conn): conn.send('Pipe 用法') conn.close() if __name__ == '__main__': parent_conn, child_conn = Pipe() pro = Process(target=show, args=(child_conn,)) pro.start() print(parent_conn.recv()) pro.join() 进程池创建多个进程，我们不用傻傻地一个个去创建。我们可以使用Pool模块来搞定。 Pool 常用的方法如下： 方法 含义 apply() 同步执行（串行） apply_async() 异步执行（并行） terminate() 立刻关闭进程池 join() 主进程等待所有子进程执行完毕。必须在close或terminate()之后使用 close() 等待所有进程结束后，才关闭进程池 具体用法见示例代码： 12345678910111213from multiprocessing import Pooldef show(num): print('num : ' + str(num))if __name__==\"__main__\": pool = Pool(processes = 3) for i in xrange(6): # 维持执行的进程总数为processes，当一个进程执行完毕后会添加新的进程进去 pool.apply_async(show, args=(i, )) print('====== apply_async ======') pool.close() #调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束 pool.join() 选择多线程还是多进程？在这个问题上，首先要看下你的程序是属于哪种类型的。一般分为两种 CPU 密集型 和 I/O 密集型。 CPU 密集型：程序比较偏重于计算，需要经常使用 CPU 来运算。例如科学计算的程序，机器学习的程序等。 I/O 密集型：顾名思义就是程序需要频繁进行输入输出操作。爬虫程序就是典型的 I/O 密集型程序。 如果程序是属于 CPU 密集型，建议使用多进程。而多线程就更适合应用于 I/O 密集型程序。","link":"/710.html"},{"title":"爬虫实战二：爬取电影天堂的最新电影","text":"前面两篇文章介绍 requests 和 xpath 的用法。我们推崇学以致用，所以本文讲解利用这两个工具进行实战。 爬取目标本次爬取的站点选择电影天堂，网址是： www.ydtt8.net。爬取内容是整个站点的所有电影信息，包括电影名称，导演、主演、下载地址等。具体抓取信息如下图所示： 设计爬虫程序确定爬取入口电影天堂里面的电影数目成千上万，电影类型也是让人眼花缭乱。我们为了保证爬取的电影信息不重复， 所以要确定一个爬取方向。目前这情况真让人无从下手。但是，我们点击主页中的【最新电影】选项，跳进一个新的页面。蓦然有种柳暗花明又一村的感觉。 由图可知道，电影天堂有 5 个电影栏目，分别为最新电影、日韩电影、欧美电影、国内电影、综合电影。每个栏目又有一定数量的分页，每个分页有 25 条电影信息。那么程序的入口可以有 5 个 url 地址。这 5 个地址分别对应每个栏目的首页链接。 爬取思路知道爬取入口，后面的工作就容易多了。我通过测试发现这几个栏目除了页面的 url 地址不一样之外，其他例如提取信息的 xpath 路径是一样的。因此，我把 5 个栏目当做 1 个类，再该类进行遍历爬取。 我这里“最新电影”为例说明爬取思路。1）请求栏目的首页来获取到分页的总数，以及推测出每个分页的 url 地址；2）将获取到的分页 url 存放到名为 floorQueue 队列中；3）从 floorQueue 中依次取出分页 url，然后利用多线程发起请求；4）将获取到的电影页面 url 存入到名为 middleQueue 的队列；5）从 middleQueue 中依次取出电影页面 url，再利用多线程发起请求；6）将请求结果使用 Xpath 解析并提取所需的电影信息；7）将爬取到的电影信息存到名为 contentQueue 队列中；8）从 contentQueue 队列中依次取出电影信息，然后存到数据库中。 设计爬虫架构根据爬取思路，我设计出爬虫架构。如下图所示： 代码实现主要阐述几个重要的类的代码 main 类 主要工作两个：第一，实例化出一个dytt8Moive对象，然后开始爬取信息。第二，等爬取结束，将数据插入到数据库中。 处理爬虫的逻辑代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 截止到2017-08-08, 最新电影一共才有 164 个页面LASTEST_MOIVE_TOTAL_SUM = 6 #164# 请求网络线程总数, 线程不要调太好, 不然会返回很多 400THREAD_SUM = 5def startSpider(): # 实例化对象 # 获取【最新电影】有多少个页面 LASTEST_MOIVE_TOTAL_SUM = dytt_Lastest.getMaxsize() print('【最新电影】一共 ' + str(LASTEST_MOIVE_TOTAL_SUM) + ' 有个页面') dyttlastest = dytt_Lastest(LASTEST_MOIVE_TOTAL_SUM) floorlist = dyttlastest.getPageUrlList() floorQueue = TaskQueue.getFloorQueue() for item in floorlist: floorQueue.put(item, 3) # print(floorQueue.qsize()) for i in range(THREAD_SUM): workthread = FloorWorkThread(floorQueue, i) workthread.start() while True: if TaskQueue.isFloorQueueEmpty(): break else: pass for i in range(THREAD_SUM): workthread = TopWorkThread(TaskQueue.getMiddleQueue(), i) workthread.start() while True: if TaskQueue.isMiddleQueueEmpty(): break else: pass insertData() if __name__ == '__main__': startSpider() 创建数据库以及表，接着再把电影信息插入到数据库的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def insertData(): DBName = 'dytt.db' db = sqlite3.connect('./' + DBName, 10) conn = db.cursor() SelectSql = 'Select * from sqlite_master where type = \"table\" and name=\"lastest_moive\";' CreateTableSql = ''' Create Table lastest_moive ( 'm_id' INTEGER PRIMARY KEY, 'm_type' varchar(100), 'm_trans_name' varchar(200), 'm_name' varchar(100), 'm_decade' varchar(30), 'm_conutry' varchar(30), 'm_level' varchar(100), 'm_language' varchar(30), 'm_subtitles' varchar(100), 'm_publish' varchar(30), 'm_IMDB_socre' varchar(50), 'm_douban_score' varchar(50), 'm_format' varchar(20), 'm_resolution' varchar(20), 'm_size' varchar(10), 'm_duration' varchar(10), 'm_director' varchar(50), 'm_actors' varchar(1000), 'm_placard' varchar(200), 'm_screenshot' varchar(200), 'm_ftpurl' varchar(200), 'm_dytt8_url' varchar(200) ); ''' InsertSql = ''' Insert into lastest_moive(m_type, m_trans_name, m_name, m_decade, m_conutry, m_level, m_language, m_subtitles, m_publish, m_IMDB_socre, m_douban_score, m_format, m_resolution, m_size, m_duration, m_director, m_actors, m_placard, m_screenshot, m_ftpurl, m_dytt8_url) values(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?); ''' if not conn.execute(SelectSql).fetchone(): conn.execute(CreateTableSql) db.commit() print('==== 创建表成功 ====') else: print('==== 创建表失败, 表已经存在 ====') count = 1 while not TaskQueue.isContentQueueEmpty(): item = TaskQueue.getContentQueue().get() conn.execute(InsertSql, Utils.dirToList(item)) db.commit() print('插入第 ' + str(count) + ' 条数据成功') count = count + 1 db.commit() db.close() TaskQueue 类 维护 floorQueue、middleQueue、contentQueue 三个队列的管理类。之所以选择队列的数据结构，是因为爬虫程序需要用到多线程，队列能够保证线程安全。 dytt8Moive 类 dytt8Moive 类是本程序的主心骨。程序最初的爬取目标是 5 个电影栏目，但是目前只现实了爬取最新栏目。如果你想爬取全部栏目电影，只需对 dytt8Moive 稍微改造下即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252class dytt_Lastest(object): # 获取爬虫程序抓取入口 breakoutUrl = 'http://www.dytt8.net/html/gndy/dyzz/index.html' def __init__(self, sum): self.sum = sum # 获取【最新电影】有多少个页面 # 截止到2017-08-08, 最新电影一共才有 164 个页面 @classmethod def getMaxsize(cls): response = requests.get(cls.breakoutUrl, headers=RequestModel.getHeaders(), proxies=RequestModel.getProxies(), timeout=3) # 需将电影天堂的页面的编码改为 GBK, 不然会出现乱码的情况 response.encoding = 'GBK' selector = etree.HTML(response.text) # 提取信息 optionList = selector.xpath(\"//select[@name='sldd']/text()\") return len(optionList) - 1 # 因首页重复, 所以要减1 def getPageUrlList(self): ''' 主要功能：目录页url取出，比如：http://www.dytt8.net/html/gndy/dyzz/list_23_'+ str(i) + '.html ''' templist = [] request_url_prefix = 'http://www.dytt8.net/html/gndy/dyzz/' templist = [request_url_prefix + 'index.html'] for i in range(2, self.sum + 1): templist.append(request_url_prefix + 'list_23_' + str(i) + '.html') for t in templist: print('request url is ### ' + t + ' ###') return templist @classmethod def getMoivePageUrlList(cls, html): ''' 获取电影信息的网页链接 ''' selector = etree.HTML(html) templist = selector.xpath(\"//div[@class='co_content8']/ul/td/table/tr/td/b/a/@href\") # print(len(templist)) # print(templist) return templist @classmethod def getMoiveInforms(cls, url, html): ''' 解析电影信息页面的内容, 具体如下： 类型 : 疾速特攻/疾速追杀2][BD-mkv.720p.中英双字][2017年高分惊悚动作] ◎译名 : ◎译\\u3000\\u3000名\\u3000疾速特攻/杀神John Wick 2(港)/捍卫任务2(台)/疾速追杀2/极速追杀：第二章/约翰·威克2 ◎片名 : ◎片\\u3000\\u3000名\\u3000John Wick: Chapter Two ◎年代 : ◎年\\u3000\\u3000代\\u30002017 ◎国家 : ◎产\\u3000\\u3000地\\u3000美国 ◎类别 : ◎类\\u3000\\u3000别\\u3000动作/犯罪/惊悚 ◎语言 : ◎语\\u3000\\u3000言\\u3000英语 ◎字幕 : ◎字\\u3000\\u3000幕\\u3000中英双字幕 ◎上映日期 ：◎上映日期\\u30002017-02-10(美国) ◎IMDb评分 : ◎IMDb评分\\xa0 8.1/10 from 86,240 users ◎豆瓣评分 : ◎豆瓣评分\\u30007.7/10 from 2,915 users ◎文件格式 : ◎文件格式\\u3000x264 + aac ◎视频尺寸 : ◎视频尺寸\\u30001280 x 720 ◎文件大小 : ◎文件大小\\u30001CD ◎片长 : ◎片\\u3000\\u3000长\\u3000122分钟 ◎导演 : ◎导\\u3000\\u3000演\\u3000查德·史塔赫斯基 Chad Stahelski ◎主演 : ◎简介 : 暂不要该字段 ◎获奖情况 : 暂不要该字段 ◎海报 影片截图 下载地址 ''' # print(html) contentDir = { 'type': '', 'trans_name': '', 'name': '', 'decade': '', 'conutry': '', 'level': '', 'language': '', 'subtitles': '', 'publish': '', 'IMDB_socre': '', 'douban_score': '', 'format': '', 'resolution': '', 'size': '', 'duration': '', 'director': '', 'actors': '', 'placard': '', 'screenshot': '', 'ftpurl': '', 'dytt8_url': '' } selector = etree.HTML(html) content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/p/text()\") # 匹配出来有两张图片, 第一张是海报, 第二张是电影画面截图 imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/p/img/@src\") # print(content) # 为了兼容 2012 年前的页面 if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/span/text()\") # 有些页面特殊, 需要用以下表达式来重新获取信息 # 电影天堂页面好混乱啊~ if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/div/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/font/text()\") if len(content) &lt; 5: content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/p/font/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/span/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/span/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/font/text()\") if not len(content): content = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/text()\") # print(content) # 不同渲染页面要采取不同的抓取方式抓取图片 if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/img/@src\") if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/img/@src\") if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/img/@src\") if not len(imgs): imgs = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/div/img/@src\") # 类型 if content[0][0:1] != '◎': contentDir['type'] = '[' + content[0] actor = '' for each in content: if each[0:5] == '◎译\\u3000\\u3000名': # 译名 ◎译\\u3000\\u3000名\\u3000 一共占居6位 contentDir['trans_name'] = each[6: len(each)] elif each[0:5] == '◎片\\u3000\\u3000名': # 片名 contentDir['name'] = each[6: len(each)] elif each[0:5] == '◎年\\u3000\\u3000代': # 年份 contentDir['decade'] = each[6: len(each)] elif each[0:5] == '◎产\\u3000\\u3000地': # 产地 contentDir['conutry'] = each[6: len(each)] elif each[0:5] == '◎类\\u3000\\u3000别': # 类别 contentDir['level'] = each[6: len(each)] elif each[0:5] == '◎语\\u3000\\u3000言': # 语言 contentDir['language'] = each[6: len(each)] elif each[0:5] == '◎字\\u3000\\u3000幕': # 字幕 contentDir['subtitles'] = each[6: len(each)] elif each[0:5] == '◎上映日期': # 上映日期 contentDir['publish'] = each[6: len(each)] elif each[0:7] == '◎IMDb评分': # IMDb评分 contentDir['IMDB_socre'] = each[9: len(each)] elif each[0:5] == '◎豆瓣评分': # 豆瓣评分 contentDir['douban_score'] = each[6: len(each)] elif each[0:5] == '◎文件格式': # 文件格式 contentDir['format'] = each[6: len(each)] elif each[0:5] == '◎视频尺寸': # 视频尺寸 contentDir['resolution'] = each[6: len(each)] elif each[0:5] == '◎文件大小': # 文件大小 contentDir['size'] = each[6: len(each)] elif each[0:5] == '◎片\\u3000\\u3000长': # 片长 contentDir['duration'] = each[6: len(each)] elif each[0:5] == '◎导\\u3000\\u3000演': # 导演 contentDir['director'] = each[6: len(each)] elif each[0:5] == '◎主\\u3000\\u3000演': # 主演 actor = each[6: len(each)] for item in content: if item[0: 4] == '\\u3000\\u3000\\u3000\\u3000': actor = actor + '\\n' + item[6: len(item)] # 主演 contentDir['actors'] = actor # 海报 if imgs[0] != None: contentDir['placard'] = imgs[0] # 影片截图 if imgs[1] != None: contentDir['screenshot'] = imgs[1] # 下载地址 ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/table/tbody/tr/td/a/text()\") # 为了兼容 2012 年前的页面 if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/table/tbody/tr/td/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/div/table/tbody/tr/td/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/td/table/tbody/tr/td/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/p/span/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/div/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/span/table/tbody/tr/td/font/a/text()\") if not len(ftp): ftp = selector.xpath(\"//div[@class='co_content8']/ul/tr/td/div/div/td/div/span/div/table/tbody/tr/td/font/a/text()\") contentDir['ftpurl'] = ftp[0] # 页面链接 contentDir['dytt8_url'] = url print(contentDir) return contentDir getMoiveInforms 方法是主要负责解析电影信息节点并将其封装成字典。在代码中，你看到 Xpath 的路径表达式不止一条。因为电影天堂的电影详情页面的排版参差不齐，所以单单一条内容提取表达式、海报和影片截图表达式、下载地址表达式远远无法满足。 选择字典类型作为存储电影信息的数据结构，也是自己爬坑之后才决定的。这算是该站点另一个坑人的地方。电影详情页中有些内容节点是没有，例如类型、豆瓣评分，所以无法使用列表按顺序保存。 爬取结果我这里展示自己爬取最新栏目中 4000 多条数据中前面部分数据。 源码如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/814.html"},{"title":"爬虫与反爬虫的博弈","text":"今天猴哥给大家说说爬虫与反爬虫的博弈。 前言近来这两三个月，我陆续将自己学到的爬虫技术分享出来。以标准网络库 urllib 的用法起笔，接着介绍各种内容提供工具，再到后续的 scrapy 爬虫框架系列。我的爬虫分享之旅已经接近尾声了。本文就来聊聊如何防止爬虫被 ban 以及如何限制爬虫。 介绍我们编写的爬虫在爬取网站的时候，要遵守 robots 协议，爬取数据做到“盗亦有道”。在爬取数据的过程中，不要对网站的服务器造成压力。尽管我们做到这么人性化。对于网络维护者来说，他们还是很反感爬虫的。因为爬虫的肆意横行意味着自己的网站资料泄露，甚至是自己刻意隐藏在网站的隐私的内容也会泄露。所以，网站维护者会运用各种方法来拦截爬虫。 攻防战 场景一 防：检测请求头中的字段，比如：User-Agent、referer等字段。 攻：只要在 http 请求的 headers 中带上对于的字段即可。下图中的七个字段被大多数浏览器用来初始化所有网络请求。建议将以下所有字段都带上。 场景二 防：后台对访问的 IP 进行统计，如果单个 IP 访问超过设定的阈值，给予封锁。虽然这种方法效果还不错， 但是其实有两个缺陷。 一个是非常容易误伤普通用户， 另一个就是 IP 其实不值钱， 各种代理网站都有出售大量的 IP 代理地址。 所以建议加大频率周期,每小时或每天超过一定次数屏蔽 IP 一段时间（不提示时间）。 攻：针对这种情况，可通过使用代理服务器解决。同时，爬虫设置下载延迟，每隔几次请求，切换一下所用代理的IP地址。 场景三 防：后台对访问进行统计， 如果单个 userAgent 访问超过阈值， 予以封锁。这种方法拦截爬虫效果非常明显，但是杀伤力过大，误伤普通用户概率非常高。所以要慎重使用。攻：收集大量浏览器的 userAgent 即可。 场景四 防：网站对访问有频率限制，还设置验证码。增加验证码是一个既古老又相当有效果的方法。能够让很多爬虫望风而逃。而且现在的验证码的干扰线, 噪点都比较多，甚至还出现了人类肉眼都难以辨别的验证码（12306 购票网站）。攻：python+tesseract 验证码识别库模拟训练，或使用类似 tor 匿名中间件（广度遍历IP） 场景五 防：网站页面是动态页面，采用 Ajax 异步加载数据方式来呈现数据。这种方法其实能够对爬虫造成了绝大的麻烦。 攻：首先用 Firebug 或者 HttpFox 对网络请求进行分析。如果能够找到 ajax 请求，也能分析出具体的参数和响应的具体含义。则直接模拟相应的http请求，即可从响应中得到对应的数据。这种情况，跟普通的请求没有什么区别。 能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把 ajax 请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求，请看场景六。 场景六 防：基于 JavaScript 的反爬虫手段，主要是在响应数据页面之前，先返回一段带有JavaScript 代码的页面，用于验证访问者有无 JavaScript 的执行环境，以确定使用的是不是浏览器。例如淘宝、快代理这样的网站。 这种反爬虫方法。通常情况下，这段JS代码执行后，会发送一个带参数key的请求，后台通过判断key的值来决定是响应真实的页面，还是响应伪造或错误的页面。因为key参数是动态生成的，每次都不一样，难以分析出其生成方法，使得无法构造对应的http请求。 攻：采用 selenium+phantomJS 框架的方式进行爬取。调用浏览器内核，并利用phantomJS 执行 js 来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。","link":"/1021.html"},{"title":"Python 绘图,我只用 Matplotlib(二)","text":"上篇文章，我们了解到 Matplotlib 是一个风格类似 Matlab 的基于 Python 的绘图库。它提供了一整套和matlab相似的命令API，十分适合交互式地进行制图。而且我们也可以方便地将它作为绘图控件，嵌入GUI应用程序中。本文主要走进 Matplotlib 的世界，初步学会绘制图形。 基础知识在学习绘制之前，先来了解下 Matplotlib 基础概念。 库我们绘制图形主要用到两个库，matplotlib.pyplot和numpy。在编码过程中，这两个库的使用频率较高，而这两个库的名字较长。这难免会给我们带来不便。所以我们一般给其设置别名， 大大减少重复性工作量。具体如下代码： 12import matplotlib.pyplot as plt # 导入模块 matplotlib.pyplot，并简写成 plt import numpy as np # 导入模块 numpy，并简写成 np numpy 是 Python 用于数学运算的库，它是在安装 matplotlib 时候顺带安装的。pyplot 是 matplotlib 一个子模块，主要为底层的面向对象的绘图库提供状态机界面。状态机隐式地自动创建数字和坐标轴以实现所需的绘图。 matplotlib 中的所有内容都按照层次结果进行组织。顶层就是由 pyplot 提供的 matplotlib “状态机环境”。基于这个状态机环境，我们就可以创建图形。 图形组成标签我在 matplotlib 官网上找图像组件说明图并在上面增加中文翻译。通过这张图，我们对 matplotlib 整体地认识。 接下来，我主要讲解 matplotlib 中几个重要的标签。 Figure Figure 翻译成中文是图像窗口。Figure 是包裹 Axes、tiles、legends 等组件的最外层窗口。它其实是一个 Windows 应用窗口 。Figure 中最主要的元素是 Axes（子图）。一个 Figure 中可以有多个子图，但至少要有一个能够显示内容的子图。 Axes Axes 翻译成中文是轴域/子图。Axes 是带有数据的图像区域。从上文可知，它是位于 Figure 里面。那它和 Figure 是什么关系？这里可能文字难以表述清楚，我以图说文。用两图带你彻底弄清它们的关系。 在看运行结果之前，我先呈上代码给各位看官品尝。 12345fig = plt.figure() # 创建一个没有 axes 的 figurefig.suptitle('No axes on this figure') # 添加标题以便我们辨别fig, ax_lst = plt.subplots(2, 2) # 创建一个以 axes 为单位的 2x2 网格的 figure plt.show() 根据运行结果图，我们不难看出。左图的 Figure1 中没有 axes，右图的 Figure2 中有 4 个 axes。因此，我们可以将 Axes 理解为面板，而面板是覆在窗口(Figure) 上。 Axis Axis 在中文的意思是轴。官网文档对 Axis 定义解释不清楚，让我们看得云里雾里的。如果你有留意前文的组成说明图，可以看到 X Axis 和 Y Axis 的字样。按照平常人的见识， 观察该图就能明白 Axis 是轴的意思。此外，Axis 和 Axes 以及 Figure 这三者关系，你看完下图，会恍然大悟。 绘制第一张图按照剧本发展，我接下来以绘制曲线并逐步美化它为例子，一步步讲解如何绘制图形。在这过程中，我也会逐一说明各个函数的作用。 初步绘制曲线12345678910import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-2, 6, 50)y1 = x + 3 # 曲线 y1y2 = 3 - x # 曲线 y2plt.figure() # 定义一个图像窗口plt.plot(x, y1) # 绘制曲线 y1plt.plot(x, y2) # 绘制曲线 y2plt.show() 调用np.linspace是创建一个 numpy 数组，并记作 x。x 包含了从 -2 到 6 之间等间隔的 50 个值。y1 和 y2 则分别是这 50 个值对应曲线的函数值组成的 numpy 数组。前面的操作还处于设置属性的阶段，还没有开始绘制图形。plt.figure() 函数才意味着开始执行绘图操作。最后别忘记调用show()函数将图形呈现出来。 简单修饰我们已经绘制出两条直线，但样式比较简陋。所以我给两条曲线设置鲜艳的颜色、线条类型。同时，还给纵轴和横轴的设置上下限，增加可观性。 123456789101112131415161718192021222324import matplotlib.pyplot as pltimport numpy as np# 创建一个点数为 8 x 6 的窗口, 并设置分辨率为 80像素/每英寸plt.figure(figsize=(8, 6), dpi=80)# 再创建一个规格为 1 x 1 的子图plt.subplot(1，1，1)x = np.linspace(-2, 6, 50)y1 = x + 3 # 曲线 y1y2 = 3 - x # 曲线 y2# 绘制颜色为蓝色、宽度为 1 像素的连续曲线 y1plt.plot(x, y1, color=\"blue\", linewidth=1.0, linestyle=\"-\")# 绘制颜色为紫色、宽度为 2 像素的不连续曲线 y2plt.plot(x, y2, color=\"#800080\", linewidth=2.0, linestyle=\"--\")# 设置横轴的上下限plt.xlim(-1, 6)# 设置纵轴的上下限plt.ylim(-2, 10)plt.show() 设置纵横轴标签在图像中，我们不能一味地认为横轴就是 X 轴，纵轴就是 Y 轴。图形因内容数据不同，纵横轴标签往往也会不同。这也体现了给纵横轴设置标签说明的重要性。 1234567...# 设置横轴标签plt.xlabel(\"X\")# 设置纵轴标签plt.ylabel(\"Y\")plt.show() 设置精准刻度matplotlib 画图设置的刻度是由曲线以及窗口的像素点等因素决定。这些刻度精确度无法满足需求，我们需要手动添加刻度。上图中，纵轴只显示 2 的倍数的刻度，横轴只显示 1 的倍数的刻度。我们为其添加精准刻度，纵轴变成单位间隔为 1 的刻度，横轴变成单位间隔为 0.5 的刻度。 1234567...# 设置横轴精准刻度plt.xticks([-1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5])# 设置纵轴精准刻度plt.yticks([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])plt.show() xticks() 和 yticks() 需要传入一个列表作为参数。 该方法默认是将列表的值来设置刻度标签，如果你想重新设置刻度标签，则需要传入两个列表参数给 xticks() 和 yticks() 。第一个列表的值代表刻度，第二个列表的值代表刻度所显示的标签。 12345678...# 设置横轴精准刻度plt.xticks([-1, 0, 1, 2, 3, 4, 5, 6], [\"-1m\", \"0m\", \"1m\", \"2m\", \"3m\", \"4m\", \"5m\", \"6m\"])# 设置纵轴精准刻度plt.yticks([-2, 0, 2, 4, 6, 8, 10], [\"-2m\", \"0m\", \"2m\", \"4m\", \"6m\", \"8m\", \"10m\"])plt.show() 添加图例如果需要在图的左上角添加一个图例。我们只需要在 plot() 函数里以「键 - 值」的形式增加一个参数。首先我们需要在绘制曲线的时候，增加一个 label 参数，然后再调用 plt.legend() 绘制出一个图例。plt.legend() 需要传入一个位置值。loc 的值可选如下： 值 说明 best 自动选择最佳位置，默认是左上 upper right 右上 upper left 左上 lower right 右下 lower left 左下 right 右边，默认是右上。如果因图形挡住右上，会自动往下选择空白地方绘制 center right 垂直居中且靠右 center left 垂直居中且靠左 lower center 垂直居中且靠底部 upper center 垂直居中且靠顶部 center 居中 1234567...# 绘制颜色为蓝色、宽度为 1 像素的连续曲线 y1plt.plot(x, y1, color=\"blue\", linewidth=1.0, linestyle=\"-\", label=\"y1\")# 绘制颜色为紫色、宽度为 2 像素的不连续曲线 y2plt.plot(x, y2, color=\"#800080\", linewidth=2.0, linestyle=\"--\", label=\"y2\")plt.legend(loc=\"upper left\")... 注释特殊点位有时某些数据点非常关键，需要突显出来。我们需要将改点绘制出来，即绘制散点图，再对其做注释。实现上述需求，我们要用到scatter()和annotate()函数。scatter() 是用于绘制散图，这里我们只是用其来绘制单个点。scatter() 用法，后续文章会详细对其用法做说明。annotate()则是添加标注 。 scatter() 函数必须传入两个参数 x 和 y。值得注意得是，它们的数据类型是列表。x 代表要标注点的横轴位置，y 代表要标注点的横轴位置。x 和 y 列表中下标相同的数据是对应的。例如 x 为 [3, 4]，y 为 [6, 8]，这表示会绘制点（3，6），（4， 8）。因此，x 和 y 长度要一样。 annotate函数同样也有两个必传参数，一个是标注内容，另一个是 xy。标注内容是一个字符串。xy 表示要在哪个位置（点）显示标注内容。xy 位置地选定。一般是在scatter() 绘制点附近，但不建议重合，这样会影响美观。 1234567891011121314151617181920...# 绘制颜色为蓝色、宽度为 1 像素的连续曲线 y1plt.plot(x, y1, color=\"blue\", linewidth=1.0, linestyle=\"-\", label=\"y1\")# 绘制散点(3, 6)plt.scatter([3], [6], s=30, color=\"blue\") # s 为点的 size# 对(3, 6)做标注plt.annotate(\"(3, 6)\", xy=(3.3, 5.5), # 在(3.3, 5.5)上做标注 fontsize=16, # 设置字体大小为 16 xycoords='data') # xycoords='data' 是说基于数据的值来选位置# 绘制颜色为紫色、宽度为 2 像素的不连续曲线 y2plt.plot(x, y2, color=\"#800080\", linewidth=2.0, linestyle=\"--\", label=\"y2\")# 绘制散点(3, 0)plt.scatter([3], [0], s=50, color=\"#800080\")# 对(3, 0)做标注plt.annotate(\"(3, 0)\", xy=(3.3, 0), # 在(3.3, 0)上做标注 fontsize=16, # 设置字体大小为 16 xycoords='data') # xycoords='data' 是说基于数据的值来选位置 点已经被标注出来了，如果你还想给点添加注释。这需要使用text()函数。text(x，y，s) 作用是在点(x，y) 上添加文本 s。matplotlib 目前好像对中午支持不是很友好， 中文均显示为乱码。 1234567···# 绘制散点(3, 0)plt.scatter([3], [0], s=50, color=\"#800080\")# 对(3, 0)做标注plt.annotate(\"(3, 0)\", xy=(3.3, 0))plt.text(4, -0.5, \"this point very important\", fontdict={'size': 12, 'color': 'green'}) # fontdict设置文本字体 到此为止，我们基本上完成了绘制直线所有工作。Matplotlib 能绘制种类繁多且绘图功能强大，所以我接下来的文章将单独对每种类型图做分享讲解。","link":"/1123.html"},{"title":"学会运用爬虫框架 Scrapy (一)","text":"对于规模小、爬取数据量小、对爬取速度不敏感的爬虫程序， 使用 Requests 能轻松搞定。这些爬虫程序主要功能是爬取网页、玩转网页。如果我们需要爬取网站以及系列网站，要求爬虫具备爬取失败能复盘、爬取速度较高等特点。很显然 Requests 不能完全满足我们的需求。因此，需要一功能更加强大的第三方爬虫框架库 —— Scrapy 简介 ScrapyScrapy 是一个为了方便人们爬取网站数据，提取结构性数据而编写的分布式爬取框架。它可以应用在包括数据挖掘， 信息处理或存储历史数据等一系列的程序中。因其功能颇多，所以学会它需要一定的时间成本。 Scrapy 的特性Scrapy 是一个框架。因此，它集一些各功能强大的 python 库的优点于一身。下面列举其一些特性： HTML, XML源数据 选择及提取 的内置支持 提供了一系列在spider之间共享的可复用的过滤器(即 Item Loaders)，对智能处理爬取数据提供了内置支持。 通过 feed导出 提供了多格式(JSON、CSV、XML)，多存储后端(FTP、S3、本地文件系统)的内置支持 提供了media pipeline，可以 自动下载 爬取到的数据中的图片(或者其他资源)。 高扩展性。您可以通过使用 signals ，设计好的API(中间件, extensions, pipelines)来定制实现您的功能。 内置的中间件及扩展为下列功能提供了支持: cookies and session 处理 HTTP 压缩 HTTP 认证 HTTP 缓存 user-agent模拟 robots.txt 爬取深度限制 健壮的编码支持和自动识别，用于处理外文、非标准和错误编码问题 针对多爬虫下性能评估、失败检测，提供了可扩展的 状态收集工具 。 内置 Web service, 使您可以监视及控制您的机器。 安装 ScrapyScrapy 是单纯用 Python 语言编写的库。所以它有依赖一些第三方库，如lxml, twisted,pyOpenSSL等。我们也无需逐个安装依赖库，使用 pip 方式安装 Scrapy 即可。pip 会自动安装 Scrapy 所依赖的库。随便也说下 Scrapy 几个重要依赖库的作用。 lxml：XML 和 HTML 文本解析器，配合 Xpath 能提取网页中的内容信息。如果你对 lxml 和 Xpath 不熟悉，你可以阅读我之前介绍该库用法的文章。 Twisted：Twisted 是 Python 下面一个非常重要的基于事件驱动的IO引擎。 pyOpenSSL：pyopenssl 是 Python 的 OpenSSL 接口。 在终端执行以下命令来安装 Scrapy 1234pip install Scrapy # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install Scrapy 你在安装过程中也许会报出安装 Twisted 失败的错误： 1234567running build_extbuilding 'twisted.test.raiser' extensionerror: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": http://landinghub.visualstudio.com/visual-cpp-build-tools----------------------------------------Failed building wheel for TwistedRunning setup.py clean for TwistedFailed to build Twisted 原因是 Twisted 底层是由 C 语言编写的，所以需要安装C语言的编译环境。对于Python3.5来说，可以通过安装 Visual C++ Build Tools 来安装这个环境。打开上面报错文本中的链接，下载并安装 visualcppbuildtools_full 。等安装完成，再执行 安装 Scrapy 命令。 安装成功之后如下图： 初探 ScrapyScrapy 项目解析Scrapy 新建项目需通过命令行操作。在指定文件夹中，打开终端执行以下命令： 1scrapy startproject 项目的名字 我新建一个名为 scrapy_demo，执行结果如下。 使用 Pycharm 打开该项目，我们会发现项目的层级架构以及文件。 这些文件的作用是： scrapy.cfg：项目的配置文件，开发无需用到。 scrapy_demo：项目中会有两个同名的文件夹。最外层表示 project，里面那个目录代表 module（项目的核心）。 scrapy_demo/items.py：以字段形式定义后期需要处理的数据。 scrapy_demo/pipelines.py：提取出来的 Item 对象返回的数据并进行存储。 scrapy_demo/settings.py：项目的设置文件。可以对爬虫进行自定义设置，比如选择深度优先爬取还是广度优先爬取，设置对每个IP的爬虫数，设置每个域名的爬虫数，设置爬虫延时，设置代理等等。 scrapy_demo/spider： 这个目录存放爬虫程序代码。 __init__.py：python 包要求，对 scrapy 作用不大。 Scrapy 的架构我们刚接触到新事物，想一下子就熟悉它。这明显是天方夜谭。应按照一定的顺序层次、逐步深入学习。学习 Scrapy 也不外乎如此。在我看来，Scrapy 好比由许多组件拼装起来的大机器。因此，可以采取从整体到局部的顺序学习 Scrapy。下图是 Scrapy 的架构图，它能让我们对 Scrapy 有了大体地认识。后续的文章会逐个介绍其组件用法。 我按照从上而下，从左往右的顺序阐述各组件的作用。 Scheduler：调度器。负责接受 Engine 发送过来的 Requests 请求，并将其队列化； Item Pipeline：Item Pipeline负责处理被spider提取出来的item。其有典型应用，如清理 HTML 数据、验证爬取的数据（检查 item 包含某些字段）、查重（并丢弃）、爬取数据持久化（存入数据库、写入文件等）； Scrapy Engine：引擎是 Scrapy 的中枢。它负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件； Downloader Middlewares：下载中间件是 Engine 和 Downloader 的枢纽。负责处理 Downloader 传递给 Engine 的 responses；它还支持自定义扩展。 Downloader：负责下载 Engine 发送的所有 Requests 请求，并将其获取到的 responses 回传给 Scrapy Engine； Spider middlewares：Spider 中间件是 Engine 和 Spider 的连接桥梁；它支持自定义扩展来处理 Spider 的输入(responses) 以及输出 item 和 requests 给 Engine ； Spiders：负责解析 Responses 并提取 Item 字段需要的数据，再将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)； Scrapy 工作机制我们对 Scrapy 有了大体上的认识。接下来我们了解下 Scrapy 内部的工作流程。同样先放出一张图，然后我再细细讲解。 当引擎(Engine) 收到 Spider 发送过来的 url 主入口地址（其实是一个 Request 对象, 因为 Scrapy 内部是用到 Requests 请求库），Engine 会进行初始化操作。 Engine 请求调度器（Scheduler），让 Scheduler 调度出下一个 url 给 Engine。 Scheduler 返回下一个 url 给 Engine。 Engine 将 url通过下载中间件(请求(request)方向)转发给下载器(Downloader)。 一旦页面下载完毕，Downloader 生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给 Engine 引擎将从下载器中接收到 Response 发送给Spider处理。 Spider 处理 Response 并返回爬取到的 Item 及新的 Request 给引擎。 Engine 将 Spider 返回的爬取到的 Item 转发给Item Pipeline，顺便也将将 Request 给调度器。 重复（第2步)直到调度器中没有更多地request，引擎关闭该网站。","link":"/916.html"},{"title":"Python 绘图,我只用 Matplotlib(一)","text":"当我们的爬虫程序已经完成使命，帮我们抓取大量的数据。你内心也许会空落落的。或许你会疑惑，自己抓取这些数据有啥用？如果要拿去分析，那要怎么分析呢？ 说到数据分析，Python 完全能够胜任这方面的工作。Python 究竟如何在数据分析领域做到游刃有余？因为它有“四板斧”，分别是Matplotlib、NumPy、SciPy/Pandas。Matplotlib 是画图工具，NumPy 是矩阵运算库，SciPy 是数学运算工具，Pandas 是数据处理的工具。 为什么选择 Matplotlib？Python 有很多强大的画图库，为什么我偏偏独爱 Maplotlib？我先买个关子，先来看看还有哪些库。 SeabornSeaborn 是一个基于 Matplotlib 的高级可视化效果库， 偏向于统计作图。因此，针对的点主要是数据挖掘和机器学习中的变量特征选取。相比 Matplotlib ，它语法相对简化些，绘制出来的图不需要花很多功夫去修饰。但是它绘图方式比较局限，不过灵活。 BokehBokeh 是基于 javascript 来实现交互可视化库，它可以在WEB浏览器中实现美观的视觉效果。但是它也有明显的缺点。其一是版本时常更新，最重要的是有时语法还不向下兼容。这对于我们来说是噩梦。其二是语法晦涩，与 matplotlib做比较，可以说是有过之而无不及。 ggplotggplot 是 yhat 大神基于 R 语言的 ggplot2 制作的 python 版本库。 如果你使用 R 语言的话，ggplot2 可以算是必不可少的工具。所以，很多人都推荐使用该库。不过可惜的是，yhat 大神已经停止维护该库了。 PlotlyPlotly 也是一个做可视化交互的库。它不仅支持 Python 还支持 R 语言。Plotly 的优点是能提供 WEB 在线交互，配色也真心好看。如果你是一名数据分析师，Plotly 强大的交互功能能助你一臂之力完成展示。 MapboxMapbox 使用处理地理数据引擎更强的可视化工具库。如果你需要绘制地理图，那么它值得你信赖。 总之， Python 绘图库众多，各有特点。但是 Maplotlib 是最基础的 Python 可视化库。如果你将学习 Python 数据可视化。那么 Maplotlib 是非学不可，然后再学习其他库做纵横向的拓展。 Matplotlib 能绘制什么图？Matiplotlib 非常强大，所以最基本的图表自然不在话下。例如说：直线图 曲线图 柱状图 直方图 饼图 散点图 只能绘制这些最基础的图？显示是不可能的，还能绘制些高级点的图例如：高级点的柱状图 等高线图 类表格图形 不仅仅只有这些，还能绘制 3D 图形。例如三维柱状图 3D 曲面图 因此，Matplotlib 绘制的图种类能够满足我们做数据分析了。 安装 Matplotlib看到这里，你是否惊叹不已，很很迫不及待地想学习 Matplotlib。而工欲善其事，必先利其器。我们先来学习如何安装 Matplotlib。其实也是很简单，我们借助 pip 工具来安装。 在终端执行以下命令来安装 Matplotlib 1234pip install Matplotlib # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install Matplotlib","link":"/1022.html"},{"title":"彻底理解 Iterable、Iterator、generator","text":"本文介绍猴哥对于Python中的Iterable、Iterator、generator的理解。 Iterable我们一般称Iterable为可迭代对象。Python 中任意的对象，只要它定义了可以返回一个迭代器的__iter__方法，或者定义了可以支持下标索引的__getitem__方法，那么它就是一个可迭代对象。我们常用到的集合数据类型都是 Iterable。例如列表（list）、元组（tuple）、字典（dict）、集合（set）、字符串（str）等。 我定义了一个列表 numlist，打印出该列表的方法。 1234numlist = [1, 2, 3]print(numlist)print(numlist.__iter__) # 调用__iter__方法print(numlist.__getitem__) # 调用__getitem__方法 运行结果如下： 根据运行结果，我们可知列表就是个可迭代对象。Python 的collections库有个isinstance()函数。可以用来判断一个对象是否是 Iterable 对象。 12345from collections import Iterable isinstance({}, Iterable) isinstance((), Iterable) isinstance(999, Iterable) 运行结果为： 如果我们每次都要使用这个函数来判断一个对象是否为可迭代对象，这样操作有点麻烦。有没有快速判定的方法呢？答案是肯定的。可以直接使用 for 循环进行遍历的对象就是可迭代对象。 除此之外，generator(生成器) 和带 yield 的 generator function 也是可迭代的对象。 IteratorIterator是迭代器的意思。任意对象，只要定义了next()（Python 2 版本）或者__next__()（Python 3 版本） 方法，那么它就是一个迭代器。迭代器中还有另一个函数__iter__()，它和 next() 方法形成迭代器协议。 iter()返回主要是返回迭代器对象本身，即return self。如果你自己定义个迭代器，实现该函数就能使用for ... in ...语句遍历了。 next()获取容器中的下一个元素，当没有可访问元素后，就抛出StopIteration异常。 遍历迭代器有两个方式。一种是使用 next() 函数；另一种则是使用 for each 循环，本质上就是通过不断调用 next() 函数实现的。 1234567891011121314151617181920from collections import Iteratornumlist = [1, 2, 3]# 将数组转化为迭代器ite1 = iter(numlist)print(ite1)for i in ite1: print(i)print(\"=========\")ite2 = iter(numlist)while True: try: num = ite2.__next__() print(num) except StopIteration: break 值得注意的是一个 Iterator 只能遍历一次。 generatorgenerator 翻译成中文是生成器。生成器也是一种特殊迭代器。它其实是生成器函数返回生成器的迭代，“生成器的迭代器”这个术语通常被称作”生成器”。yield 是生成器实现__next__()方法的关键。它作为生成器执行的暂停恢复点，可以对 yield 表达式进行赋值，也可以将 yield 表达式的值返回。任何包含 yield 语句的函数被称为生成器。 yield是一个语法糖，内部实现支持了迭代器协议，同时yield内部是一个状态机，维护着挂起和继续的状态。 个人认为，生成器算是 Python 非常棒的特性。它的出现能帮助大大节省些内存空间。假如我们要生成从 1 到 10 这 10 个数字，采用列表的方式定义，会占用 10 个地址空间。采用生成器，只会占用一个地址空间。因为生成器并没有把所有的值存在内存中，而是在运行时生成值。所以生成器只能访问一次。 创建一个从包含 1 到 10 的生成器的例子。 1234gen = (i for i in range(10))print(gen)for i in gen: print(i) 运行结果如下： 带有 yield 关键字 的例子。重点关注运行结果，这能让你对 yield 有更深的认识。 12345678910111213def testYield(n): for i in range(n): print(\"当前值: \", i) yield doubeNumber(i) print(\"第 \", i, \" 次运行\") print(\"testYield 运行结束\")def doubeNumber(i): return i*2 if __name__ == '__main__': for i in testYield(3): print(i, \"===\", i) 运行结果如下：","link":"/1124.html"},{"title":"学会运用爬虫框架 Scrapy (五)  —— 部署爬虫","text":"本文是 Scrapy 爬虫系列的最后一篇文章。主要讲述如何将我们编写的爬虫程序部署到生产环境中。我们使用由 scrapy 官方提供的爬虫管理工具 scrapyd 来部署爬虫程序。 为什么使用 scrapyd?一是它由 scrapy 官方提供的，二是我们使用它可以非常方便地运用 JSON API来部署爬虫、控制爬虫以及查看运行日志。 使用 scrapyd原理选择一台主机当做服务器，安装并启动 scrapyd 服务。再这之后，scrapyd 会以守护进程的方式存在系统中，监听爬虫地运行与请求，然后启动进程来执行爬虫程序。 安装 scrapyd使用 pip 能比较方便地安装 scrapyd。 1234pip install scrapyd # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install scrapyd 启动 scrapyd在终端命令行下以下命令来启动服务： 1scrapyd 启动服务结果如下： scrapyd 也提供了 web 的接口。方便我们查看和管理爬虫程序。默认情况下 scrapyd 监听 6800 端口，运行 scrapyd 后。在本机上使用浏览器访问 http://localhost:6800/地址即可查看到当前可以运行的项目。 项目部署直接使用 scrapyd-client 提供的 scrapyd-deploy 工具 原理scrapyd 是运行在服务器端，而 scrapyd-client 是运行在客户端。客户端使用 scrapyd-client 通过调用 scrapyd 的 json 接口来部署爬虫项目。 安装 scrapyd-client在终端下运行以下安装命令： 1234pip install scrapyd-client # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install scrapyd-client 配置项目的服务器信息修改工程目录下的 scrapy.cfg 文件。 12345678910111213# Automatically created by: scrapy startproject## For more information about the [deploy] section see:# https://scrapyd.readthedocs.org/en/latest/deploy.html[settings]default = demo.settings # test 为 project 的名称, 默认创建的[deploy:server]# 默认情况下并没有 server，它只是为服务器指定一个名字url = http://localhost:6800/ # 这是部署到本地, 如果你使用其他机器远程部署程序, 需将本地地址换成服务器的 IP 地址。默认是注释的project = test # test 为 project 的名称, 默认创建的 如果你服务器有配置 HTTP basic authentication 验证，那么需要在 scrapy.cfg 文件增加用户名和密码。这是用于登录服务器用的。 123456789[settings]default = demo.settings # demo 为 project 的名称, 默认创建的[deploy:server]# 默认情况下并没有 server，它只是为服务器指定一个名字url = http://192.168.161.129:6800/ # 这是部署到远程服务器project = demo # demo 为 project 的名称, 默认创建的username = monkeypassword = 123456 # 如果不需要密码可以不写 部署爬虫程序在爬虫项目根目录下执行下面的命令: 1scrapyd-deploy &lt;target&gt; -p &lt;project&gt; 其中 target 为上一步配置的服务器名称，project 为项目名称，可以根据实际情况自己指定。 我指定 target 为 server，project 为 demo，所以我要执行的命令如下： 1scrapyd-deploy server -p demo 部署操作会打包你的当前项目，如果当前项目下有setup.py文件，就会使用它，没有的会就会自动创建一个。(如果后期项目需要打包的话，可以根据自己的需要修改里面的信息，也可以暂时不管它). 从返回的结果里面，我们可以看到部署的状态，项目名称，版本号和爬虫个数，以及当前的主机名称. 运行结果如下： 12345$ scrapyd-deploy server -p demoPacking version 1507376760Deploying to project \"demo\" in http://localhost:6800/addversion.jsonServer response (200):{\"status\": \"ok\", \"project\": \"demo\", \"version\": \"1507376760\", \"spiders\": 1, \"node_name\": \"james-virtual-machine\"} 使用以下命令检查部署爬虫结果： 1scrapyd-deploy -l 服务器名称 我指定服务器名称为 server，所以要执行命令如下： 123$ scrapyd-deploy -L severdefaultdemo 刷新 http://localhost:6800/ 页面, 也可以看到Available projects: demo的字样。 使用 API 管理爬虫scrapyd 的 web 界面比较简单，主要用于监控，所有的调度工作全部依靠接口实现。官方推荐使用 curl 来管理爬虫。所以要先安装 curl。 windows 用户可以到该网站https://curl.haxx.se/download.html下载 curl 安装包进行安装。 ubuntu/Mac 用户直接使用命令行安装即可。 开启爬虫 schedule在爬虫项目的根目录下，使用终端运行以下命令： 1curl http://localhost:6800/schedule.json -d project=demo -d spider=demo_spider 成功启动爬虫结果如下： 12curl http://localhost:6800/schedule.json -d project=tutorial -d spider=tencent{\"status\": \"ok\", \"jobid\": \"94bd8ce041fd11e6af1a000c2969bafd\", \"node_name\": \"james-virtual-machine\"} 取消爬虫1curl http://localhost:6800/cancel.json -d project=demo -d job=94bd8ce041fd11e6af1a000c2969bafd 列出项目1curl http://localhost:6800/listprojects.json 列出爬虫、版本、job 信息1curl http://localhost:6800/listspiders.json?project=demo 删除爬虫项目1curl http://localhost:6800/delproject.json -d project=demo","link":"/1020.html"},{"title":"学会运用爬虫框架 Scrapy (四)  —— 高效下载图片","text":"爬虫程序爬取的目标通常不仅仅是文字资源，经常也会爬取图片资源。这就涉及如何高效下载图片的问题。这里高效下载指的是既能把图片完整下载到本地又不会对网站服务器造成压力。也许你会这么做，在 pipeline 中自己实现下载图片逻辑。但 Scrapy 提供了图片管道ImagesPipeline，方便我们操作下载图片。 为什么要选用 ImagesPipeline ？ImagesPipeline 具有以下特点： 将所有下载的图片转换成通用的格式（JPG）和模式（RGB） 避免重新下载最近已经下载过的图片 缩略图生成 检测图像的宽/高，确保它们满足最小限制 具体实现定义字段在 item.py 文件中定义我们两个字段image_urls 和images_path 12345678import scrapyclass PicsDownloadItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() image_urls = scrapy.Field() # 图片的下载地址， 该字段是存储图片的列表 image_path = scrapy.Field() # 图片本地存储路径(相对路径) 编写 spider我以爬取 freebuf 首页部分图片为例子讲解。具体代码如下： 12345678910111213141516171819202122import scrapyfrom pics_download.items import PicsDownloadItemclass freebuf_pic_spider(scrapy.Spider): name = 'freebuf' allowed_domains = ['freebuf.com'] start_urls = [ 'http://www.freebuf.com/' ] def parse(self, response): self.log(response.headers) # 获取 freebuf 首页所有的图片, 以列表形式保存到 image_urls 字段中。 piclist = response.xpath(\"//div[@class='news-img']/a/img/@src\").extract() if piclist: item = PicsDownloadItem() item['image_urls'] = piclist yield item 实现 Pipeline我新建一个名为PicsDownloadPipeline的类。需要注意一点的是： Scrapy 默认生成的类是继承Object， 要将该类修改为继承ImagesPipeline。然后实现get_media_requests和item_completed这两个函数。 get_media_requests(item, info) ImagePipeline 根据 image_urls 中指定的 url 进行爬取，可以通过 get_media_requests 为每个 url 生成一个 Request。具体实现如下： 123def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) item_completed(self, results, item, info) 当一个单独项目中的所有图片请求完成时，该方法会被调用。处理结果会以二元组的方式返回给 item_completed() 函数。这个二元组定义如下：(success, image_info_or_failure)其中，第一个元素表示图片是否下载成功；第二个元素是一个字典，包含三个属性： 1) url - 图片下载的url。这是从 get_media_requests() 方法返回请求的url。2) path - 图片存储的路径（类似 IMAGES_STORE）3) checksum - 图片内容的 MD5 hash 具体实现如下： 1234567def item_completed(self, results, item, info): # 将下载的图片路径（传入到results中）存储到 image_paths 项目组中，如果其中没有图片，我们将丢弃项目: image_path = [x['path'] for ok, x in results if ok] if not image_path: raise DropItem(\"Item contains no images\") item['image_path'] = image_path return item 综合起来，PicsDownloadPipeline 的实现下载图片逻辑的代码如下： 1234567891011121314151617import scrapyfrom scrapy.exceptions import DropItemfrom scrapy.pipelines.images import ImagesPipelineclass PicsDownloadPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield scrapy.Request(image_url) def item_completed(self, results, item, info): # 将下载的图片路径（传入到results中）存储到 image_paths 项目组中，如果其中没有图片，我们将丢弃项目: image_path = [x['path'] for ok, x in results if ok] if not image_path: raise DropItem(\"Item contains no images\") item['image_path'] = image_path return item 配置设置在 setting.py 配置存放图片的路径以及自定义下载的图片管道。 123456789# 设置存放图片的路径IMAGES_STORE = 'D:\\\\freebuf'# 配置自定义下载的图片管道， 默认是被注释的ITEM_PIPELINES = { # 第二行的填写规则 # yourproject.middlewares(文件名).middleware类 'pics_download.pipelines.PicsDownloadPipeline': 300, # pics_download 是你项目的名称} 运行程序在 Scrapy 项目的根目录下，执行以下命令： 1scrapy crawl freebuf # freebuf 是我们在 spider 定义的 name 属性 如果你使用的 Python 版本是 3.x 的，可能会报出以下的错误。 123 File \"c:\\program files (x86)\\python36-32\\lib\\site-packages\\scrapy\\pipelines\\images.py\", line 15, in &lt;module&gt; from PIL import ImageModuleNotFoundError: No module named 'PIL' 这是因为 Scrapy 框架用到这个Python Imaging Library (PIL)图片加载库，但是这个库只支持 2.x 版本，所以会运行出错。对于使用 Python 3.x 版本的我们，难道就束手无策？Scrapy 的开发者建议我们使用更好的图片加载库Pillow。为什么说更好呢？一方面是兼容了 PIL，另一方面在该库支持生成缩略图。 因此，我们安装 Pillow 就能解决运行报错的问题。具体安装 Pillow命令如下： 1234pip install pillow # 如果出现因下载失败导致安装不上的情况，可以先启动 ss 再执行安装命令# 或者在终端中使用代理pip --proxy http://代理ip:端口 install pillow 安装之后，重新运行爬虫程序。Scrapy 会运行结果中显示我们定义的image_urls 和images_path字段。 运行结果我们会发现在 D 盘有个名为freebuf的文件夹。在该文件夹中有个full文件夹，里面存放我们刚才爬取到的图片。 如果有在 setting.py 文件中设置生成缩略图。 1234IMAGES_THUMBS = { 'small': (50, 50), # (宽， 高) 'big': (270, 270),} 那么到时候，与full同级的目录下会多出个thumbs文件夹。里面会有两个文件夹small和big，分别对应小分辨率的图片和大分辨率的图片。 ##优化 避免重复下载在 setting.py 中新增以下配置可以避免下载最近已经下载的图片。 12# 90天的图片失效期限IMAGES_EXPIRES = 90 设置该字段，对于已经完成爬取的网站，重新运行爬虫程序。爬虫程序不会重新下载新的图片资源。 自动限速（AutoTrottle）下载图片是比较消耗服务器的资源以及流量。如果图片资源比较大，爬虫程序一直在下载图片。这会对目标网站造成一定的影响。同时，爬虫有可能遭到封杀的情况。 因此，我们有必要对爬虫程序做爬取限速处理。Scrapy 已经为我们提供了AutoTrottle功能。 只要在 setting.py 中开启AutoTrottle功能并配置限速算法即可。我采用默认的配置，具体配置如下： 12345678910# 启用AutoThrottle扩展AUTOTHROTTLE_ENABLED = True# 初始下载延迟(单位:秒)AUTOTHROTTLE_START_DELAY = 5# 在高延迟情况下最大的下载延迟(单位秒)AUTOTHROTTLE_MAX_DELAY = 60# 设置 Scrapy应该与远程网站并行发送的平均请求数, 目前是以1个并发请求数AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0# 启用AutoThrottle调试模式#AUTOTHROTTLE_DEBUG = False 值得注意的是，启用AutoThrottle扩展时，仍然受到DOWNLOAD_DELAY（下载延迟）和CONCURRENT_REQUESTS_PER_DOMAIN（对单个网站进行并发请求的最大值）以及CONCURRENT_REQUESTS_PER_IP（对单个IP进行并发请求的最大值）的约束。 源码如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/919.html"},{"title":"Python 绘图,我只用 Matplotlib(三)","text":"上篇文章，我已经讲解绘制图像大致步骤，接下来的系列文章将分别对各种图形做讲解。其实就是了解各个图种的绘图 API。文章就讲解第一种图形，柱状图。 基础绘制柱状图，我们主要用到bar()函数。只要将该函数理解透彻，我们就能绘制各种类型的柱状图。 我们先看下bar()的构造函数：bar(x，height， width，*，align='center'，**kwargs) x包含所有柱子的下标的列表 height包含所有柱子的高度值的列表 width每个柱子的宽度。可以指定一个固定值，那么所有的柱子都是一样的宽。或者设置一个列表，这样可以分别对每个柱子设定不同的宽度。 align柱子对齐方式，有两个可选值：center和edge。center表示每根柱子是根据下标来对齐, edge则表示每根柱子全部以下标为起点，然后显示到下标的右边。如果不指定该参数，默认值是center。 其他可选参数有： color每根柱子呈现的颜色。同样可指定一个颜色值，让所有柱子呈现同样颜色；或者指定带有不同颜色的列表，让不同柱子显示不同颜色。 edgecolor每根柱子边框的颜色。同样可指定一个颜色值，让所有柱子边框呈现同样颜色；或者指定带有不同颜色的列表，让不同柱子的边框显示不同颜色。 linewidth每根柱子的边框宽度。如果没有设置该参数，将使用默认宽度，默认是没有边框。 tick_label每根柱子上显示的标签，默认是没有内容。 xerr每根柱子顶部在横轴方向的线段。如果指定一个固定值，所有柱子的线段将一直长；如果指定一个带有不同长度值的列表，那么柱子顶部的线段将呈现不同长度。 yerr每根柱子顶端在纵轴方向的线段。如果指定一个固定值，所有柱子的线段将一直长；如果指定一个带有不同长度值的列表，那么柱子顶部的线段将呈现不同长度。 ecolor设置 xerr 和 yerr 的线段的颜色。同样可以指定一个固定值或者一个列表。 capsize这个参数很有趣, 对xerr或者yerr的补充说明。一般为其设置一个整数，例如 10。如果你已经设置了yerr 参数，那么设置 capsize 参数，会在每跟柱子顶部线段上面的首尾部分增加两条垂直原来线段的线段。对 xerr 参数也是同样道理。可能看说明会觉得绕，如果你看下图就一目了然了。 error_kw设置 xerr 和 yerr 参数显示线段的参数，它是个字典类型。如果你在该参数中又重新定义了 ecolor 和 capsize，那么显示效果以这个为准。 log这个参数，我暂时搞不懂有什么用。 orientation设置柱子是显示方式。设置值为 vertical ，那么显示为柱形图。如果设置为 horizontal 条形图。不过 matplotlib 官网不建议直接使用这个来绘制条形图，使用barh来绘制条形图。 下面我就调用 bar 函数绘制一个最简单的柱形图。 123456789101112131415161718192021222324252627282930313233343536373839import matplotlib.pyplot as pltimport numpy as np# 创建一个点数为 8 x 6 的窗口, 并设置分辨率为 80像素/每英寸plt.figure(figsize=(8, 6), dpi=80)# 再创建一个规格为 1 x 1 的子图plt.subplot(1, 1, 1)# 柱子总数N = 6# 包含每个柱子对应值的序列values = (25, 32, 34, 20, 41, 50)# 包含每个柱子下标的序列index = np.arange(N)# 柱子的宽度width = 0.35# 绘制柱状图, 每根柱子的颜色为紫罗兰色p2 = plt.bar(index, values, width, label=\"rainfall\", color=\"#87CEFA\")# 设置横轴标签plt.xlabel('Months')# 设置纵轴标签plt.ylabel('rainfall (mm)')# 添加标题plt.title('Monthly average rainfall')# 添加纵横轴的刻度plt.xticks(index, ('Jan', 'Fub', 'Mar', 'Apr', 'May', 'Jun'))plt.yticks(np.arange(0, 81, 10))# 添加图例plt.legend(loc=\"upper right\")plt.show() 运行结果为： 进阶bar 函数的参数很多，你可以使用这些参数绘制你所需要柱形图的样式。如果你还不会灵活使用这样参数，那就让我们来学习 matplotlib 官方提供的例子。 123456789101112131415161718192021222324252627282930313233343536373839404142# Credit: Josh Hemannimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib.ticker import MaxNLocatorfrom collections import namedtuplen_groups = 5means_men = (20, 35, 30, 35, 27)std_men = (2, 3, 4, 1, 2)means_women = (25, 32, 34, 20, 25)std_women = (3, 5, 2, 3, 3)fig, ax = plt.subplots()index = np.arange(n_groups)bar_width = 0.35opacity = 0.4error_config = {'ecolor': '0.3'}rects1 = ax.bar(index, means_men, bar_width, alpha=opacity, color='b', yerr=std_men, error_kw=error_config, label='Men')rects2 = ax.bar(index + bar_width, means_women, bar_width, alpha=opacity, color='r', yerr=std_women, error_kw=error_config, label='Women')ax.set_xlabel('Group')ax.set_ylabel('Scores')ax.set_title('Scores by group and gender')ax.set_xticks(index + bar_width / 2)ax.set_xticklabels(('A', 'B', 'C', 'D', 'E'))ax.legend()fig.tight_layout()plt.show() 运行结果如下： 开动你的大脑，想想还能绘制出什么样式的柱形图。","link":"/1125.html"},{"title":"Python 实现识别弱图片验证码","text":"目前，很多网站为了防止爬虫肆意模拟浏览器登录，采用增加验证码的方式来拦截爬虫。验证码的形式有多种，最常见的就是图片验证码。其他验证码的形式有音频验证码，滑动验证码等。图片验证码越来越高级，识别难度也大幅提高，就算人为输入也经常会输错。本文主要讲解识别弱图片验证码。 图片验证码强度图片验证码主要采用加干扰线、字符粘连、字符扭曲方式来增强识别难度。 加干扰线加干扰线也分为两种，一种是线条跟字符同等颜色，另一种则线条的颜色是五颜六色。) 字符粘连各个字符之间的间隔比较小，互相依靠，能以分割。 字符扭曲字符显示的位置相对标准旋转一定角度。) 其中最弱的验证码为不具备以上的特征，干扰因素比较小。如下： 识别思路首先对图片做二值化来降噪处理，去掉图片中的噪点，干扰线等。然后将图片中的单个字符切分出来。最后识别每个字符。 图片的处理，我采用 Python 标准图像处理库 PIL。图片分割，我暂时采用谷歌开源库 Tesseract-OCR。字符识别则使用 pytesseract 库。 安装 Pillow 我使用的 Python 版本是 3.6， 而标准库 PIL 不支持 3.x。所以需要使用 Pillow 来替代。Pillow 是专门兼容 3.x 版本的 PIL 的分支。使用 pip 包管理工具安装 Pillow 是最方便快捷的。 123pip install Pillow# 如果出现因下载失败导致安装不上的情况，建议使用代理pip --proxy http://代理ip:端口 install Pillow Tesseract-OCR Tesseract：开源的OCR识别引擎，初期Tesseract引擎由HP实验室研发，后来贡献给了开源软件业，后经由Google进行改进，消除bug，优化，重新发布。这才让其重焕新生。 我们可以在 GitHub 上找到该库并下载。我是下载最新的 4.0 版本。github 的下载地址是：https://github.com/tesseract-ocr/tesseract/wiki/4.0-with-LSTM#400-alpha-for-windows pytesseract pytesseract 是 Tesseract-OCR 对进行包装，提供 Python 接口的库。同样可以使用 pip 方式来安装。 123pip install pytesseract# 如果出现因下载失败导致安装不上的情况，建议使用代理pip --proxy http://代理ip:端口 install pytesseract 代码实现获取并打开图片获取图片验证码，你可以通过使用网络请求库下载。我为了方便，将图片下载到本地并放在项目目录下。 123456789101112from PIL import Image'''获取图片'''def getImage(): fileName = '16.jpg' img = Image.open() # 打印当前图片的模式以及格式 print('未转化前的: ', img.mode, img.format) # 使用系统默认工具打开图片 # img.show() return img 预处理这一步主要是将图片进行降噪处理, 把图片从 “RGB” 模式转化为 “L” 模式，也就是把彩色图片变成黑白图片。再处理掉背景噪点，让字符和背景形成黑白的反差。 123456789101112131415161718192021'''1) 将图片进行降噪处理, 通过二值化去掉后面的背景色并加深文字对比度'''def convert_Image(img, standard=127.5): ''' 【灰度转换】 ''' image = img.convert('L') ''' 【二值化】 根据阈值 standard , 将所有像素都置为 0(黑色) 或 255(白色), 便于接下来的分割 ''' pixels = image.load() for x in range(image.width): for y in range(image.height): if pixels[x, y] &gt; standard: pixels[x, y] = 255 else: pixels[x, y] = 0 return image 打开彩色图片，PIL 会将图片解码为三通道的 “RGB” 图像。调用 convert(‘L’) 才会把图片转化为黑白图片。其中模式 “L” 为灰色图像, 它的每个像素用 8 个bit表示, 0 表示黑, 255 表示白, 其他数字表示不同的灰度。 在 PIL 中，从模式 “RGB” 转换为 “L” 模式是按照下面的公式转换的：L = R 的值 x 299/1000 + G 的值 x 587/1000+ B 的值 x 114/1000 图像的二值化，就是将图像上的像素点的灰度值两极分化(设置为 0 或 255，0表示黑，255表示白)，也就是将整个图像呈现出明显的只有黑和白的视觉效果。目的是加深字符与背景的颜色差，便于 Tesseract 的识别和分割。对于阈值的选取，我采用比较暴力的做法，直接使用 0 和 255 的平均值。 识别经过上述处理，图片验证码中的字符已经变成很清晰了。最后一步是直接用 pytesseract 库识别。 1234567891011121314import pytesseract'''使用 pytesseract 库来识别图片中的字符'''def change_Image_to_text(img): ''' 如果出现找不到训练库的位置, 需要我们手动自动 语法: tessdata_dir_config = '--tessdata-dir \"&lt;replace_with_your_tessdata_dir_path&gt;\"' ''' testdata_dir_config = '--tessdata-dir \"C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tessdata\"' textCode = pytesseract.image_to_string(img, lang='eng', config=testdata_dir_config) # 去掉非法字符，只保留字母数字 textCode = re.sub(\"\\W\", \"\", textCode) return textCode Tesseract-ORC 默认是没有指定安装路径。我们需要手动指定本地 Tesseract 的路径。不然会报出这样的错误： 1FileNotFoundError: [WinError 2] 系统找不到指定的文件 具体解决方案是：使用文本编辑器打开 pytesseract 库的 pytesseract.py 文件，一般路径如下：C:\\Program Files (x86)\\Python35-32\\Lib\\site-packages\\pytesseract\\pytesseract.py 将 tesseract_cmd 修改成你电脑本地的 Tesseract-OCR 的安装路径。 12# CHANGE THIS IF TESSERACT IS NOT IN YOUR PATH, OR IS NAMED DIFFERENTLYtesseract_cmd = 'C:/Program Files (x86)/Tesseract-OCR/tesseract.exe' 最后执行字符识别的实例代码 123456def main(): img = convert_Image(getImage(fileName)) print('识别的结果：', change_Image_to_text(img))if __name__ == '__main__': main() 运行结果如下： 12未转化前的: RGB JPEG识别的结果： 9834 总结Tesseract-ORC 对于这种弱验证码识别率还是可以，大部分字符能够正确识别出来。只不过有时候会将数字 8 识别为 0。如果图片验证码稍微变得复杂点，识别率大大降低，会经常识别不出来的情况。我自己也尝试收集 500 张图片来训练 Tesseract-ORC，识别率会有所提升，但识别率还是很低。 如果想要做到识别率较高，那么需要使用 CNN (卷积神经网络)或者 RNN (循环神经网络)训练出自己的识别库。正好机器学习很火爆很流行，学习一下也无妨。","link":"/1228.html"},{"title":"Python 定时任务(下)","text":"上篇文章，我们了解到有三种办法能实现定时任务，但是都无法做到循环执行定时任务。因此，需要一个能够担当此重任的库。它就是APScheduler。 简介APScheduler的全称是Advanced Python Scheduler。它是一个轻量级的 Python 定时任务调度框架。APScheduler 支持三种调度任务：固定时间间隔，固定时间点（日期），Linux 下的 Crontab 命令。同时，它还支持异步执行、后台执行调度任务。 安装使用 pip 包管理工具安装 APScheduler 是最方便快捷的。 123pip install APScheduler# 如果出现因下载失败导致安装不上的情况，建议使用代理pip --proxy http://代理ip:端口 install APScheduler 使用步骤APScheduler 使用起来还算是比较简单。运行一个调度任务只需要以下三部曲。 1) 新建一个 schedulers (调度器) 。2) 添加一个调度任务(job stores)。3) 运行调度任务。 下面是执行每 2 秒报时的简单示例代码： 12345678910111213141516171819import datetimeimport timefrom apscheduler.schedulers.background import BackgroundSchedulerdef timedTask(): print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])if __name__ == '__main__': # 创建后台执行的 schedulers scheduler = BackgroundScheduler() # 添加调度任务 # 调度方法为 timedTask，触发器选择 interval(间隔性)，间隔时长为 2 秒 scheduler.add_job(timedTask, 'interval', seconds=2) # 启动调度任务 scheduler.start() while True: print(time.time()) time.sleep(5) 基础组件APScheduler 有四种组件，分别是：调度器(scheduler)，作业存储(job store)，触发器(trigger)，执行器(executor)。 schedulers（调度器）它是任务调度器，属于控制器角色。它配置作业存储器和执行器可以在调度器中完成，例如添加、修改和移除作业。 triggers（触发器）描述调度任务被触发的条件。不过触发器完全是无状态的。 job stores（作业存储器）任务持久化仓库，默认保存任务在内存中，也可将任务保存都各种数据库中，任务中的数据序列化后保存到持久化数据库，从数据库加载后又反序列化。 executors（执行器）负责处理作业的运行，它们通常通过在作业中提交指定的可调用对象到一个线程或者进城池来进行。当作业完成时，执行器将会通知调度器。 schedulers（调度器）我个人觉得 APScheduler 非常好用的原因。它提供 7 种调度器，能够满足我们各种场景的需要。例如：后台执行某个操作，异步执行操作等。调度器分别是： BlockingScheduler : 调度器在当前进程的主线程中运行，也就是会阻塞当前线程。 BackgroundScheduler : 调度器在后台线程中运行，不会阻塞当前线程。 AsyncIOScheduler : 结合 asyncio 模块（一个异步框架）一起使用。 GeventScheduler : 程序中使用 gevent（高性能的Python并发框架）作为IO模型，和 GeventExecutor 配合使用。 TornadoScheduler : 程序中使用 Tornado（一个web框架）的IO模型，用 ioloop.add_timeout 完成定时唤醒。 TwistedScheduler : 配合 TwistedExecutor，用 reactor.callLater 完成定时唤醒。 QtScheduler : 你的应用是一个 Qt 应用，需使用QTimer完成定时唤醒。 triggers（触发器）APScheduler 有三种内建的 trigger:1）date 触发器date 是最基本的一种调度，作业任务只会执行一次。它表示特定的时间点触发。它的参数如下： 参数 说明 run_date (datetime 或 str) 作业的运行日期或时间 timezone (datetime.tzinfo 或 str) 指定时区 date 触发器使用示例如下： 12345678910111213141516from datetime import datetimefrom datetime import datefrom apscheduler.schedulers.background import BackgroundSchedulerdef job_func(text): print(text)scheduler = BackgroundScheduler()# 在 2017-12-13 时刻运行一次 job_func 方法scheduler .add_job(job_func, 'date', run_date=date(2017, 12, 13), args=['text'])# 在 2017-12-13 14:00:00 时刻运行一次 job_func 方法scheduler .add_job(job_func, 'date', run_date=datetime(2017, 12, 13, 14, 0, 0), args=['text'])# 在 2017-12-13 14:00:01 时刻运行一次 job_func 方法scheduler .add_job(job_func, 'date', run_date='2017-12-13 14:00:01', args=['text'])scheduler.start() 2）interval 触发器固定时间间隔触发。interval 间隔调度，参数如下： 参数 说明 weeks (int) 间隔几周 days (int) 间隔几天 hours (int) 间隔几小时 minutes (int) 间隔几分钟 seconds (int) 间隔多少秒 start_date (datetime 或 str) 开始日期 end_date (datetime 或 str) 结束日期 timezone (datetime.tzinfo 或str) 时区 interval 触发器使用示例如下： 12345678910111213import datetimefrom apscheduler.schedulers.background import BackgroundSchedulerdef job_func(text): print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])scheduler = BackgroundScheduler()# 每隔两分钟执行一次 job_func 方法scheduler .add_job(job_func, 'interval', minutes=2)# 在 2017-12-13 14:00:01 ~ 2017-12-13 14:00:10 之间, 每隔两分钟执行一次 job_func 方法scheduler .add_job(job_func, 'interval', minutes=2, start_date='2017-12-13 14:00:01' , end_date='2017-12-13 14:00:10')scheduler.start() 3）cron 触发器 在特定时间周期性地触发，和Linux crontab格式兼容。它是功能最强大的触发器。 我们先了解 cron 参数： 参数 说明 year (int 或 str) 年，4位数字 month (int 或 str) 月 (范围1-12) day (int 或 str) 日 (范围1-31 week (int 或 str) 周 (范围1-53) day_of_week (int 或 str) 周内第几天或者星期几 (范围0-6 或者 mon,tue,wed,thu,fri,sat,sun) hour (int 或 str) 时 (范围0-23) minute (int 或 str) 分 (范围0-59) second (int 或 str) 秒 (范围0-59) start_date (datetime 或 str) 最早开始日期(包含) end_date (datetime 或 str) 最晚结束时间(包含) timezone (datetime.tzinfo 或str) 指定时区 这些参数是支持算数表达式，取值格式有如下： cron 触发器使用示例如下： 1234567891011import datetimefrom apscheduler.schedulers.background import BackgroundSchedulerdef job_func(text): print(\"当前时间：\", datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])scheduler = BackgroundScheduler()# 在每年 1-3、7-9 月份中的每个星期一、二中的 00:00, 01:00, 02:00 和 03:00 执行 job_func 任务scheduler .add_job(job_func, 'cron', month='1-3,7-9',day='0, tue', hour='0-3')scheduler.start() 作业存储(job store)该组件是对调度任务的管理。1）添加 job有两种添加方法，其中一种上述代码用到的 add_job()， 另一种则是scheduled_job()修饰器来修饰函数。 这个两种办法的区别是：第一种方法返回一个 apscheduler.job.Job 的实例，可以用来改变或者移除 job。第二种方法只适用于应用运行期间不会改变的 job。 第二种添加任务方式的例子： 123456789import datetimefrom apscheduler.schedulers.background import BackgroundScheduler@scheduler.scheduled_job(job_func, 'interval', minutes=2)def job_func(text): print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3])scheduler = BackgroundScheduler()scheduler.start() 2）移除 job移除 job 也有两种方法：remove_job() 和 job.remove()。remove_job() 是根据 job 的 id 来移除，所以要在 job 创建的时候指定一个 id。job.remove() 则是对 job 执行 remove 方法即可 12345scheduler.add_job(job_func, 'interval', minutes=2, id='job_one')scheduler.remove_job(job_one)job = add_job(job_func, 'interval', minutes=2, id='job_one')job.remvoe() 3）获取 job 列表通过 scheduler.get_jobs() 方法能够获取当前调度器中的所有 job 的列表 4) 修改 job如果你因计划改变要对 job 进行修改，可以使用Job.modify() 或者 modify_job()方法来修改 job 的属性。但是值得注意的是，job 的 id 是无法被修改的。 12345678scheduler.add_job(job_func, 'interval', minutes=2, id='job_one')scheduler.start()# 将触发时间间隔修改成 5分钟scheduler.modify_job('job_one', minutes=5)job = scheduler.add_job(job_func, 'interval', minutes=2)# 将触发时间间隔修改成 5分钟job.modify(minutes=5) 5）关闭 job默认情况下调度器会等待所有正在运行的作业完成后，关闭所有的调度器和作业存储。如果你不想等待，可以将 wait 选项设置为 False。 12scheduler.shutdown()scheduler.shutdown(wait=false) 执行器(executor)执行器顾名思义是执行调度任务的模块。最常用的 executor 有两种：ProcessPoolExecutor 和 ThreadPoolExecutor 下面是显式设置 job store(使用mongo存储)和 executor 的代码的示例。注：本代码来源于网络 1234567891011121314151617181920212223242526272829303132from pymongo import MongoClientfrom apscheduler.schedulers.blocking import BlockingSchedulerfrom apscheduler.jobstores.mongodb import MongoDBJobStorefrom apscheduler.jobstores.memory import MemoryJobStorefrom apscheduler.executors.pool import ThreadPoolExecutor, ProcessPoolExecutor def my_job(): print 'hello world'host = '127.0.0.1'port = 27017client = MongoClient(host, port) jobstores = { 'mongo': MongoDBJobStore(collection='job', database='test', client=client), 'default': MemoryJobStore()}executors = { 'default': ThreadPoolExecutor(10), 'processpool': ProcessPoolExecutor(3)}job_defaults = { 'coalesce': False, 'max_instances': 3}scheduler = BlockingScheduler(jobstores=jobstores, executors=executors, job_defaults=job_defaults)scheduler.add_job(my_job, 'interval', seconds=5) try: scheduler.start()except SystemExit: client.close()","link":"/1227.html"},{"title":"Python 定时任务(上)","text":"在项目中，我们可能遇到有定时任务的需求。其一：定时执行任务。例如每天早上 8 点定时推送早报。其二：每隔一个时间段就执行任务。比如：每隔一个小时提醒自己起来走动走动，避免长时间坐着。今天，我跟大家分享下 Python 定时任务的实现方法。 第一种办法是最简单又最暴力。那就是在一个死循环中，使用线程睡眠函数 sleep()。 12345678910111213from datetime import datetimeimport time'''每个 10 秒打印当前时间。'''def timedTask(): while True: print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) time.sleep(10)if __name__ == '__main__': timedTask() 这种方法能够执行固定间隔时间的任务。如果timedTask()函数之后还有些操作，我们还使用死循环 + 阻塞线程。这会使得timedTask()一直占有 CPU 资源，导致后续操作无法执行。我建议谨重使用。 ##既然第一种方法暴力，那么有没有比较优雅地方法？答案是肯定的。Python 标准库 threading 中有个 Timer 类。它会新启动一个线程来执行定时任务，所以它是非阻塞函式。 如果你有使用多线程的话，需要关心线程安全问题。那么你可以选使用threading.Timer模块。 123456789101112131415161718192021222324from datetime import datetimefrom threading import Timerimport time'''每个 10 秒打印当前时间。'''def timedTask(): ''' 第一个参数: 延迟多长时间执行任务(单位: 秒) 第二个参数: 要执行的任务, 即函数 第三个参数: 调用函数的参数(tuple) ''' Timer(10, task, ()).start()# 定时任务def task(): print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))if __name__ == '__main__': timedTask() while True: print(time.time()) time.sleep(5) 运行结果： 12341512486945.11963751512486950.1198732017-12-05 23:15:501512486955.133385 ##第三种方式是也是使用标准库中sched模块。sched 是事件调度器，它通过 scheduler 类来调度事件，从而达到定时执行任务的效果。 sched库使用起来也是非常简单。1）首先构造一个sched.scheduler类它接受两个参数：timefunc 和 delayfunc。timefunc 应该返回一个数字，代表当前时间，delayfunc 函数接受一个参数，用于暂停运行的时间单元。 一般使用默认参数就行，即传入这两个参数 time.time 和 time.sleep.当然，你也可以自己实现时间暂停的函数。 2）添加调度任务scheduler 提供了两个添加调度任务的函数: enter(delay, priority, action, argument=(), kwargs={}) 该函数可以延迟一定时间执行任务。delay 表示延迟多长时间执行任务，单位是秒。priority为优先级，越小优先级越大。两个任务指定相同的延迟时间，优先级大的任务会向被执行。action 即需要执行的函数，argument 和 kwargs 分别是函数的位置和关键字参数。 scheduler.enterabs(time, priority, action, argument=(), kwargs={}) 添加一项任务，但这个任务会在 time 这时刻执行。因此，time 是绝对时间.其他参数用法与 enter() 中的参数用法是一致。 3）把任务运行起来调用 scheduler.run()函数就完事了。 下面是 sche 使用的简单示例： 123456789101112131415161718192021from datetime import datetimeimport schedimport time'''每个 10 秒打印当前时间。'''def timedTask(): # 初始化 sched 模块的 scheduler 类 scheduler = sched.scheduler(time.time, time.sleep) # 增加调度任务 scheduler.enter(10, 1, task) # 运行任务 scheduler.run()# 定时任务def task(): print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))if __name__ == '__main__': timedTask() 值得注意的是： scheduler 中的每个调度任务只会工作一次，不会无限循环被调用。如果想重复执行同一任务， 需要重复添加调度任务即可。","link":"/1226.html"},{"title":"pustil-获取系统信息库","text":"运维工程师经常使用 Python 编写脚本程序来做监控系统运行的状态。如果自己手动使用 Python 的标准库执行系统命令来获取信息，会显得非常麻烦。既要兼容不同操作系统，又要自己处理解析信息。为了解决的痛点问题，psutil 就横空出世。它的出现无疑是运维工程师的福音。运维小伙伴通过它执行一两行代码即可实现系统监控。 简介psutil全称是process and system utilities。psutil 是一个跨平台的应用于系统监控、分析、以及对系统进程进行一定管理的 Python 第三方库。它不仅能够轻松获取系统中正常运行的进程和系统利用率（例如 CPU、内存、磁盘、网络等）信息，还实现了跟 UNIX 系统命令行工具类似的功能。可以说是运维工作的“必备品”。 它功能强大，操作简单。这也促使很多开源项目都集成它到自己项目中，不妨有谷歌的 GRR 项目、脸书的 osquery 项目等。 github 地址：https://github.com/giampaolo/psutil 安装安装 psutil 是有多种办法：通过 pip 安装，通过源码方式安装，通过下载 tar 压缩包来安装。其中通过 pip 的方式是最简单的。 123pip install psutil# 如果出现因下载失败导致安装不上的情况，建议使用代理pip --proxy http://代理ip:端口 install psutil 使用前面说到 psutil 能监听到 CPU、内存、磁盘、网络、传感器、进程等，现在跟着我来学习下。 获取 CPU 信息1）我先获取自己电脑 CPU 的核心数，我电脑的 CPU 型号是 I5 4590。我通过搜索引擎得知该型号 CPU 是四核四线。 12345678import psutilpsutil.cpu_count() # 获取 CPU 的逻辑核心数，默认logical=Truepsutil.cpu_count(logical=False) # 获取 CPU 的物理核心数&gt;&gt; 4&gt;&gt; 4# 这说明该 CPU 型号是真四核。 2）统计 CPU 的时间： 1234import psutilpsutil.cpu_times() &gt;&gt; scputimes(user=9276.365234375, system=5034.5390625, idle=96077.0703125, interrupt=181.78796863555908, dpc=298.227108001709) cpu_times() 返回的是带有系统所有逻辑 CPU 运行时间的元组，单位是秒。返回元组的字段中有这几个常用字段： user：执行用户进程的时间，Linux 系统还包括访客的时间 system：执行内核进程时间 idle：闲置时间 iowait（Linux 特有）：等待 I/O 操作的时间 irp（Linux 特有）：打断服务硬件的时间 interrupt（Windows 特有）：跟 irp 字段类似 dpc（Windows 特有）：服务延迟程序调用（DPCs）的时间 如果增加参数 percpu=True， cpu_times() 会以列表的形式输出每个逻辑 CPU 的时间。 3）获取当前 CPU 的利用率的百分比： 1234import psutilpsutil.cpu_percent()&gt;&gt; 16.5 cpu_percent() 默认的参数是 interval=None, percpu=False。当 interval 为0或者None时，表示的是 interval 时间内的sys的利用率。当 percpu 为 False 表示所有逻辑 CPU 的使用率。 如果你想统计 15 秒中内，间隔 5 秒每个逻辑 CPU 的使用率，你可以这么做： 12345678import psutilfor i in range(3): psutil.cpu_percent(interval=5, percpu=True)&gt;&gt; [9.1, 7.2, 7.2, 6.2]&gt;&gt; [5.9, 3.8, 9.0, 8.4]&gt;&gt; [12.3, 8.4, 3.4, 4.1] 4）获取 CPU 频率信息： 1234import psutilpsutil.cpu_freq()&gt;&gt; [scpufreq(current=931.42925, min=0.0, max=3301.0)] 可知 cpu_freq() 返回的带有所有逻辑 CPU 频率的元组，包括当前、最小和最大频率。 获取内存信息1）获取物理内存信息： 12345import psutilpsutil.virtual_memory()&gt;&gt; svmem(total=8509177856, available=1692307456, percent=80.1, used=6816870400, free=1692307456) virtual_memory() 返回一个记载当前电脑设备中可用的物理内存信息的元组，单位是字节。从返回结果得知，当前内存总大小为 8509177856 Byte = 8 GB，可用内存（闲置内存） 1692307456 Byte = 1.6 GB，当前内存使用率为 80.1%。值得注意的是，内存总大小不等于 Used 和 available 两者的总和 available 字段在 Linux 系统下，计算方式则不同。available = free + buffers +cached。buffers 指的是 Linux 系统下的 Buffers 内存, 表示块设备(block device)所占用的缓存页; 而 cached 指的是 Linux 系统下的 Cache 内存，顾名思义为高速缓存。 2）获取交换内存信息： 1234import psutilpsutil.swap_memory()&gt;&gt; sswap(total=17016451072, used=7407996928, free=9608454144, percent=43.5, sin=0, sout=0) swap_memory() 获取的是系统的交换内存的信息，也就是我们常说的虚拟内存。前面四个字段跟物理内存含义一样。而 sin 表示从磁盘调入是 swap 的大小， sout 表示从swap调出到 disk 的大小。这两个字段在 Windows 系统下是没有意义。因此，获取结果为 0。 获取磁盘信息1）获取磁盘分区信息： 123456789import psutilpsutil.disk_partitions()&gt;&gt; [sdiskpart(device='C:\\\\', mountpoint='C:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='D:\\\\', mountpoint='D:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='E:\\\\', mountpoint='E:\\\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='F:\\\\', mountpoint='F:\\\\', fstype='NTFS', opts='rw,fixed')] disk_partitions(all=False) 返回所有挂载磁盘分区信息的列表。有点类似 Linux 的 df 命令。各个字段的含有分别为： device：分区 mountpoint：挂载点 fstype：文件系统格式 opts：挂载参数 大部分人都对 Windows 系统的分区信息了解比较多，对 Linux 系统所知甚少。因此，我给出虚拟机 Ubuntu 系统的磁盘信息, 方便大家学习。 12[sdiskpart(device='/dev/sda1', mountpoint='/', fstype='ext4', opts='rw,errors=remount-ro'), sdiskpart(device='/dev/sda2', mountpoint='', fstype='swap', opts='rw')] 2）获取磁盘使用率：disk_usage() 统计参数中路径目录的磁盘使用情况。它需要传入一个路径参数；我传入的参数是 “/“，意味着获取当前整个硬盘的使用率。 12345import psutilpsutil.disk_usage('/')&gt;&gt; sdiskusage(total=128033574912, used=81997942784, free=46035632128, percent=64.0) 3）获取磁盘 IO 信息： 12345import psutilpsutil.disk_io_counters()&gt;&gt; sdiskio(read_count=510749, write_count=505110, read_bytes=13353246720, write_bytes=8962015232, read_time=275, write_time=238) disk_io_counters() 获取当前磁盘的 I/O 数据信息情况，也就是读取和写入信息。 获取网络信息1）获取整个系统的网络信息 12345import psutilpsutil.disk_io_counters()&gt;&gt; snetio(bytes_sent=77587966, bytes_recv=1113555204, packets_sent=500638, packets_recv=1048467, errin=0, errout=0, dropin=0, dropout=0) disk_io_counters() 返回整个系统所有网卡（包括有线网卡、无限网卡）的进行网络读写数据、发包数等信息。个人认为可以使用该方法来抓包。各个字段含义如下： bytes_sent：发送的字节数 bytes_recv：收的字节数 packets_sent：发送到数据包的个数 packets_recv：接受的数据包的个数 errout：发送数据包错误的总数 dropin：接收时丢弃的数据包的总数 dropout：发送时丢弃的数据包的总数( OSX 和 BSD 系统总是 0 ) 如果增加参数 pernic=True，disk_io_counters() 则会分别输出各个网卡的网络信息数据。 2）获取当前网络连接信息net_connections() 的作用跟系统命令 netstat -an 是一样的。输出当前系统中所有类型的网络连接数据。 1234import psutilpsutil.net_connections()# 数据过多，我就不打印输出结果。 3）获取网络接口信息我们能通过 net_if_addrs() 函数来获取到各个网卡的 IP 地址、网关等信息 12import psutilpsutil.net_if_addrs() 4）获取网络接口状态net_if_stats() 获取的是各个网卡的状态信息，例如网卡是否处于激活状态、当前网速等 12import psutilpsutil.net_if_stats() 获取系统相关信息1) 获取当前登录用户信息 12345import psutilpsutil.users()&gt;&gt; [suser(name='monkey', terminal=None, host='0.125.2.117', started=1515585444.0, pid=None)] users() 是返回当前登录用户的信息。例如用户的名称 name、运行的终端 terminal， 在 Windows 系统下就是我们常说的 CMD 窗口、登录的 IP 地址 host、登录的时长 started以及登录的进程 pid，在 Windows 和 OpenBSD 系统中，该字段为 None。 2）获取系统启动时间psutil.boot_time() 获取的是系统启动的时间点，而不会启动消耗时长。 获取进程信息如果查看当前系统的所有进程信息，你可以使用 test() 方法。跟 Windows 系统下的 tasklist 命令作用类似。 12import psutilpsutil.test() psutil 还提供一列的方法来查看系统进程的信息。 1234567891011121314151617181920212223242526272829303132333435import psutilpsutil.pids() # 列出所有进程的PID&gt;&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 40, 41, 71, 199, 203, 218, 219, 220, 325, 327, 344, 345, 364, 417, 569, 764, 765, 817, 1042, 1058, 1070, 1110, 1186, 1194, 1197, 1209, 1211, 1213, 1215, 1217, 1219, 1228, 1229, 1231, 1298, 1643, 1745, 1794, 1796, 1829, 1846, 2004, 2039, 2154, 2175]p = psutil.Process(2154) # 实例化一个Process对象，参数为一个进程的PIDp.name() # 获取进程名&gt;&gt; 'python'p.exe() # 获取进程 bin 路径, 即安装路径&gt;&gt;'/usr/bin/python'p.cwd() # 获取进程的工作目录&gt;&gt; '/root'p.status() # 获取进程的运行状态&gt;&gt;'running'p.create_time() # 获取进程创建时间点&gt;&gt; 1427469528.49p.cpu_times() # 获取进程使用 CPU 时间信息&gt;&gt; pcputimes(user=0.081150144, system=0.053269812, children_user=0.0, children_system=0.0)p.memory_info() # 获取进程使用的内存&gt;&gt; pmem(rss=8310784, vms=2481725440, pfaults=3207, pageins=18)p.open_files() # 获取进程打开的文件&gt;&gt; []p.connections() # 获取进程相关网络连接&gt;&gt; []p.num_threads() # 获取进程的线程数量&gt;&gt; 1","link":"/131.html"},{"title":"回顾 2017,展望 2018","text":"望着桌上的日历，我发现只剩下几张纸。自己蓦然意识到 2017 年已经即将离去，2018 年即将到来。已经到了年底，我们需要总结和回顾今年的历程。让我们盘点 2017 年涉及 Python 重要事件。 2017 年最热门的话题莫过于人工智能。人工智能是一块崭新的研究领域，所以很多公司都很注重人工智能的研究。走在前沿的，当属谷歌。谷歌不仅完善用于开源人工智能项目 TensorFlow 文档，建立相关社区，而且还在中国成立 AI 中国中心。虽然 TensorFlow 虽然是用 C++ 编写的，但是提供了一套 Python 的接口。另外，吴恩达教授又开设人工智能课程。这种种让 Python 的人气一路高涨， 成为今年世界上最流行的编程语言。地址: https://github.com/tensorflow/tensorflow Python 社区在几年前就一直在讨论是否迁移到 GitHub 以改进开发流程。就在今年 2 月份，Brett Cannon 在 Python 官方邮件组发消息，确定迁移到 GitHub 的日期。这让 Python 正式迁移到源码托管平台 GitHub，拥抱了 Git 版本控制系统。地址： https://github.com/python/cpython 由于历史原因，Python 2 和 Python 3 是互不兼容。所以 Python 核心团队是这几年来同时维护这两个版本。然而，Python 核心团队计划在 2020 年停止支持 Python 2。NumPy 项目团队也宣布停止支持 Python 2。 Django 作为 Python Web 流行的开发框架。它凭借文档资料丰富，开发迅速，内置辅助组件颇多等特点，一直深受人们的喜爱。国内很多知名网站也是基于 Django 来做开发的，不妨有知乎，果壳网，虎扑等。Django 官方在今年推出了 2.0 版本，其中最大的变化是停止支持 Python 2 系列。 微软也考虑将 Python 列为 Excel 官方脚本语言。如果获得批准，Excel 用户将能够像目前使用 VBA 脚本一样，使用 Python 脚本与 Excel 文档、数据以及一些 Excel 核心函数进行交互。同时，微软官网也是积极做出回应，通过发起投票来收集更多用户的反馈信息，在线调查用户是否想要在 Excel 中使用 Python。 全国计算机等级考试经过教育部批准，对全国等级考试做出调整。在二级考试中，取消“Visual FoxPro 数据库程序设计项目”科目，新增“Python 语言程序设计项目”。另外，Python 将被纳入高考内容，浙江省信息技术课程改革方案出台，确认 Python 进入浙江省信息技术高考。除此之外，山东省最新出版的《小学信息技术六年级教材》也加入了 Python 内容，设计开源硬件、人工智能、3D 创意设计等。 写在最后，我相信 Python 还会在 2018 年继续保持热度，甚至会更加火爆。所以学习 Python 是不会吃亏的。掌握多一项技能就多一条求生之路。","link":"/1230.html"},{"title":"Scrapy 框架插件之IP代理池","text":"现在很多网站都是对单个 IP 地址有访问次数限制，如果你在短时间内访问过于频繁。该网站会封掉你 IP，让你在一段时间内无法正常该网站。突破反爬虫机制的一个重要举措就是代理 IP。拥有庞大稳定的 IP 代理，在爬虫工作中将起到重要的作用,但是从成本的角度来说，一般稳定的 IP 池都很贵。因此，我为 Scrapy 爬虫编写个免费 IP 代理池插件。 特点该插件适用的程序是基于 Scrapy 框架编写的爬虫程序。插件通过爬取免费代理地址，然后过滤掉无效 IP 代理后存放到 Mysql 数据库。另外，它会每 10 分钟轮询数据库中的 IP 代理数量。如果代理地址因为连接失败次数超过 3 次被删除，从而导致代理不够，它会后台重新爬取新的 IP 代理。 收集的代理网站 无忧代理(data5u) ip181 代理 快代理 西刺代理 项目说明 startrun.py项目的主入口。它负责启动 Scrapy 爬虫和代理池。 your_scrapy_project该目录下主要存放两个文件：config.py 和 settings.py。config.py 是代理池的项目配置信息。而 settings.py 是你的 Scrapy 爬虫项目的配置参考代码。 ProxyPoolWorker.pyProxyPoolWorker.py 是 IP代理池模块的管理类，负责启动和维护 IP 代理池。 proxyDBManager.pyproxyDBManager.py 位于 dbManager 包下。它是数据库操作类。主要工作是创建数据库表、往数据库中插入 IP 代理、查询数据库中剩余的 IP 代理总数、从数据库中随机查询一个 IP 代理、对连接超时或失败的 IP 代理做处理。 proxyModel.pyproxyModel.py 在 model 包下。它是 IP 代理对象类。 requestEnginer.pyrequestEnginer.py 位于 requester 目录下。requestEnginer 是整个爬虫代理池的网络引擎。它采用 Session 的形式来发起 HTTP 请求。同时，它还负责验证代理地址有效性, 达到过滤掉无用 IP 代理的目的。 scrapyscrapy 目录是一些 Scrapy 框架的自定义中间件。RandomUserAgentMiddleware.py 是为 HTTP 请求随机设置个 User-agent。middlewares.py 有两个职责。一是为 HTTP 请求随机设置个 IP 代理。二是负责捕获并处理 HTTP 异常请求。 spiders该包主要是爬取各大代理网站的爬虫。 使用方法安装依赖使用本插件，你需要通过 pip 安装以下依赖： requests apscheduler pymysql 修改配置1) 将 startrun.py 放到你的 Scrapy 项目的主目录下。例如你项目名为 demo，那么你需要放到 demo 的目录下。 2) 修改 config.py 里面的 Mysql 相关配置信息。然后将其放到你的 Scrapy 项目的二级目录下。假如你项目名为 demo，那么你需要放到 demo /demo 的目录下。 3) 参考 setting.py，修改你的 Scrapy 项目中的 setting.py 文件。主要是在你项目中增加以下代码： 1234567891011121314151617# 默认使用 IP 代理池if IF_USE_PROXY: DOWNLOADER_MIDDLEWARES = { # 第二行的填写规则 # yourproject.myMiddlewares(文件名).middleware类 # 设置 User-Agent 'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None, 'proxyPool.scrapy.RandomUserAgentMiddleware.RandomUserAgentMiddleware': 400, # 设置代理 'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': None, 'proxyPool.scrapy.middlewares.ProxyMiddleware': 100, # 设置自定义捕获异常中间层 'proxyPool.scrapy.middlewares.CatchExceptionMiddleware': 105, } 写在最后本项目目前是第一版，可能有些不完善的地方。如果你有宝贵的完善建议或者有更多的代理网站，可以反馈给我。我会持续完善本项目。最后，本项目也在 github 上开源了。 源码如果你想获取网站的源码, 点击☞☞☞Github 仓库地址","link":"/1229.html"},{"title":"Django 学习笔记之环境搭建","text":"古人云：功遇善其事，必先利其器。在正式学习 Django Web 框架之前，我们要把准备工作做好。准备工作主要是搭建开发环境，具体工作是安装 Python、创建虚拟环境 venv、安装 Django、安装 IDE 工具（Pycharm）。 安装 Python如果你使用的桌面系统是 Windows，你需要到 Python 官网下载安装包。 Linux 和 Mac 系统都自带了 Python 运行环境。Python 分为 2 和 3 版本，目前 Python 团队即将停止维护 Python 2 版本，所以建议安装 Python 3。目前最新版本是 3.6.4。另外，本系列文章适合具备 Python 基础的同学。如果你对 Python 基本语法还是很懵懂，建议你先把基础知识夯实。 创建虚拟环境 venv一提到 Python 虚拟环境，你会惊叹说为什么不用 virtualenv？如果你生产或开发环境需同时支持 Python 2 和 Python 3 ，那就需要 virtualenv。我们是从零开始学习 Django，所以可以直接使用 venv。简单来说，venv 模块是 Python 3.3 版本之后，标准库自带的虚拟环境创建和管理工具，在 Python 3 版本是代替 virtualenv。 为什么要创建虚拟环境呢？你或许会从网上下载一些安全工具或者软件。你害怕这些程序带有后门，甚至是木马程序。所以不敢在自己当前的系统中直接运行。你会使用 VMware 创建一个虚拟机，然后在虚拟机中运行该程序。不管程序是否是病毒，都不会对我当前的系统造成影响。因为虚拟机和当前的系统相互隔离，互不影响。虚拟机出现问题，只要删除即可，不会影响到当前系统。使用 venv 创建虚拟环境也是同样的道理。在当前系统中创建出一个环境，该环境可以跟当前系统互不影响，你可以随意折腾。另外，有了 virtualenv 虚拟环境之后，我们就可以把那个文件夹整体拷贝了，部署起来方便很多。 venv 使用创建 Python 虚拟环境，其实是“创建” 一个文件夹。假如我们需要在 D 盘中创建一个名为 web_dev 的虚拟环境。打开终端，执行以下命令。 123// venv 后面接上创建虚拟环境的绝对路径，建议文件名不要事先存在。// Windows、Mac、Linux 执行命令都是一样，只不过路径不一样python -m venv D://web_dev 执行创建命令之后，你会发现多出了一个名为 web_dev 文件夹，这说明已经创建成功。进入目录，里面有四个文件夹。 创建虚拟环境的完成，只是完成了一半工作。革命还尚未成功，我们还需要激活虚拟环境。依然是打开终端，进入 Scripts 文件夹，运行 activate.bat 来激活虚拟环境。 Linux 下没有 Scripts 这个目录，取而代之的是 bin目录。而激活脚本名则是activate。激活完毕，我们下一步就是安装 Django 库。 安装 Django还是上述的虚拟环境中，我们通过 pip 方式来安装 Django。如果你把终端关闭了，这也意味着把虚拟环境给关闭了。你按照上述激活步骤重新进入虚拟环境即可。 123D:\\web_dev&gt;D://web_dev/Scripts/activate.bat(web_dev) D:\\web_dev&gt;pip install djangoCollecting django 安装 IDE 工具（Pycharm）我们需要到 Pycharm 官网下载安装包。安装版本一定要选择 Professional ！安装版本一定要选择 Professional ！安装版本一定要选择 Professional ！重要的话说三遍~ 因为这个版本集成了很多 Web 开发组件，无须手动。 另附上下载地址：https://www.jetbrains.com/pycharm/download/ Pycharm 是收费版本。如果你有条件的话，可以选择购买正版。或者到网上选择激活码。 初始 Django我之前写了 Django 初始系列文章。你可以先阅读下，这样你对 Django 有整体的认识以及掌握些基本知识（创建项目，运行项目等）。 写在最后我新建一个 Python Web 学习交流 QQ 群，群号：701534112。欢迎大家加群，一起交流，一起学习。","link":"/135.html"},{"title":"几个Python编程小技巧","text":"本文，猴哥分享几个 Python 编程的小技巧。 编码问题我们在爬取网站是，会经常抓取网页文本，但是打印文本会出现是一堆乱码。这是为什么呢？原因是 Python 中字符对象分为两种，一种是 Unicode 对象，另一种是 str 对象。字符在 Python 中又以 Unicode 对象为基础，所以我们定义的字符串在内存中以 Unicode 编码的形式存储。另外，str 对象又可以有多种编码形式，如 UTF-8、GBK-2312 等。虽然不同编码的 str 对象能被解码成 unicode 对象，但是不同编码的 str 对象直接不能直接转换。因此，如果字符串编码是 GB2312，将其存储到 list 中，再打印出来看到乱码是 Unicode 编码。 解决这个问题其实很简单，Unicode 作为中间编码。我们只要将一种字符编码（如 GB2312）的字符串解码为 Unicode 编码，再编码为另外一种字符编码（如 UTF-8）。 1234# 数据库以 UTF-8 形式保存字符串，而获取到的字符串是 GB2312str = getFromNetWork() # 获取网络字符串，字符编码为 GB2312str.encode('GB2312').decode('UTF-8') print(str) 值交换在 C 或者 Java 中，要将两个变量的值进行交换。我们需要一个临时变量来存储其中一个值。 1234int a=10, b=20,temp;temp = a;a = b;b = temp; 但是在 Python中，有个简单的办法能直接一步到位。 12345678a = 10b = 20print('a = ', a, ' b = ', b)b, a = a, bprint('a = ', a, ' b = ', b)&gt;&gt; a = 10 b = 20&gt;&gt; a = 20 b = 10 单例模式Python 不仅是面向过程的编程语言，而且是面向对象的编程语言。在 Python 中，一个类被初始化，那么 __new__() 函数一定会先被调用，然后再调用__init__()。我们可以采用 hasattr() 函数来判断对象是否包含对应的属性，也就是判断类是否被初始化。 123456789101112131415class Singleton(object): def __new__(cls): # 关键在于这，每一次实例化的时候，我们都只会返回这同一个instance对象 if not hasattr(cls, 'instance'): cls.instance = super(Singleton, cls).__new__(cls) return cls.instanceobj1 = Singleton()obj2 = Singleton()obj1.name = 'GeeMonkey'print(obj1.name, obj2.name)print(obj1 is obj2)&gt;&gt; GeeMonkey GeeMonkey&gt;&gt; True","link":"/133.html"},{"title":"Python 中各种时间类型的转换","text":"我们编码过程中经常需要获取当前时间。当然， 这也离不开对时间类型进行转换运算。本文主要讲解 Python 各种时间类型之间的转换。 处理时间的库Python 标准库中有两个处理时间的库。其中一个名为 datetime，另一个是time。 在 Python 官网文档中，datetime 是被定义为数据类型(Data Types)。由此可见，datetime 是主要提供处理日期和时间的数据类型的模块。它其中有几个常用的类型，例如：datetime.datetime、datetime.time、datetime.date 等，其中最主要的类是datetime.datetime。因为它携带了 datetime.time 和 datetime.date 这两个所带的信息，能够比较齐全地输出，即能一次性就输出年、月、日、时、分、秒等日期和时间信息。 time 模块是归属于通用操作系统服务（Generic Operating System Services）类目中。time 模块主要提供各种时间转换的函数。它服务于系统层次，Python 又是跨平台的，所以有些 API 只能在某些操作系统上使用。 时间类型对象在进行时间转换之前，我们要确认下时间对象是属于哪种数据类型。只有做到对症下药，根治病因。在 Python 中，涉及时间对象有 4 种：1）datetime2）timestamp3）time tuple4）string datetimedatetime 对象属于 datetime 模块。它的构造方式是 datetime(year, month, day, hour=0, minute=0, second=0, microsecond=0, tzinfo=None)。我们了解下它的构造方法即可，一般很少直接使用它的构造方法。我们一般使用它的 now() 函数来获取本地当前日期和时间。 12345678import datetimenow = datetime.datetime.now()print(now)print(type(now))&gt;&gt; 2018-01-17 16:49:24.314323&gt;&gt; &lt;class 'datetime.datetime'&gt; string在某些场景，我们可能需要使用到字符串类型的时间。我们在 now() 函数的基础上再调用 strftime() 函数即可。strftime() 返回的是一个表示日期和时间的字符串。最后显示结果由指定样式的参数决定。 1234567import datetimenow = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")print(now)print(type(now))&gt;&gt; 2018-01-17 17:30:45&gt;&gt; &lt;class 'str'&gt; timestamptimestamp 指的是 Unix 时间戳。它是格林威治时间 1970 年 01 月 01 日 00 时 00 分 00 秒(北京时间 1970 年 01 月 01 日 08 时 00 分 00 秒)起至现在的总秒数。我们使用 time 对象中的 time() 函数能获取到时间戳 。 12345678import timenow = time.time()print(now)print(type(now))&gt;&gt; 1516179935.351417&gt;&gt; &lt;class 'float'&gt; time tupletime tuple 表示时间元组。time tuple 是 time.struct_time 对象类型。获取本地当前时间，一般是使用 time 中的 localtime() 函数。localtime() 返回的是将时间戳经过格式化的本地时间。 1234567import timenow = time.localtime()print(now)print(type(now))&gt;&gt; time.struct_time(tm_year=2018, tm_mon=1, tm_mday=17, tm_hour=17, tm_min=20, tm_sec=34, tm_wday=2, tm_yday=17, tm_isdst=0)&gt;&gt; &lt;class 'time.struct_time'&gt; 时间类型转换上述四种时间类型是如何转换，我本打算以代码的形式加以说明。当后来我看到互联网上已经有前辈整理的关系转换图，我意识到自己这种方式不够简洁明了。所以，我直接献上前辈的宝图。","link":"/132.html"},{"title":"Django 学习笔记之初识","text":"上篇文章讲述 Django 环境搭建， 在文章最后部分还有 6 篇 Django 简单入门的文章。后来我自己以一个新手的角度来阅读文章，发现前面三篇文章能被够消化吸收。但是后三篇文章理解起来可能会有点费劲，可能是我漏写了很多细节。因此，本文先将前三盘文章的内容做一些补充说明，降低学习成本。 MVC 与 MTV在 Web 服务器开发领域，MVC 模式可以算是家喻户晓。有些书籍或者文章说 Django 是一个 MVC 开发框架，另一些文章或者博客则说 Django 是 MTV 模式。那么 MTV 模式是什么？Django 又究竟是哪种模式？ Django 是一个遵循 MVC 开发模式的框架 。我们先看下 MVC 的数据流向，了解 MVC 的工作流程。 M 是 Model 的意思 ，它是一个抽象层，用来构建和操作 Web 应用中的数据。同时，Model 层跟数据库打交道的层次，执行数据库数据的增删改查操作。在 Django 项目中，Model 层逻辑是体现在 models.py 中，models.py中定义的各种类代表数据模型 model 。每个 model 是对应数据库中唯一的一张表，每个 model 中的字段也对应表中的字段。 V 指的是 View 层。在 Django 项目中，templates 文件夹中各个模版文件代表视图（View），负责数据内容的显示。templates 文件夹中文件其实就是 HTML、CSS、Javascript 文件。但在 HTML 中使用一些 Django 中特定的特殊语法，就可以实现动态内容插入，从而实现动态页面。 C 全称是 Controller 。它通常是负责从视图读取数据，控制用户输入，并向模型发送数据。在 Django 项目中，urls.py （文件路由）中定义的各种 url 访问入口 和 view.py 中定义的各种处理函数（被称为 Django 视图函数）代表控制器（Controller ）。urls.py 接受用户在浏览器中输入不同 url 地址的请求，然后分发给 view.py 。view.py 再根据文件中对应的函数与数据模型和视图交互，响应用户的请求。即将数据填充到模板（templates）中，呈现给用户。 在实际开发过程中，开发者主要操作对象是 models.py、view.py、templates 文件夹中各个模版文件。这就弱化 C 层的概念， 更加注重关注的是模型（Model）、模板(Template)和视图（Views），所以 Django 也被称为 MTV 框架 。 Django 工作流程了解 Django 的模式，我们来了解 Django 程序是处理一个 HTTP 请求的流程。 图中显示 Django 程度接受到一个 HTTP 请求到返回请求内容的过程。各个路径的含义如下： 用户使用浏览器浏览网页，浏览器向 Web 服务器发起 HTTP 请求。 Web服务器（比如，Nginx）把这个请求转交到一个WSGI（比如，uWSGI），或者直接地文件系统能够取出一个文件（比如返回一个视频文件）。 不像 web 服务器那样，WSGI服务器可以直接运行Python应用。请求生成一个被称为 environ 的 Ptyhon 字典。而且可以选择传递过去几个中间件的层，最终达到 Django 应用。 Django 根据请求的路径，URLconf 将请求分配对应的视图文件。这个请求被封装到 HttpRequest 中。URLconf 可以理解为 URL 以及该 URL所调用的视图函数之间的映射表，通常是记录到 urls.py 中。 被选择的那个视图(Views.py 中的类)会根据页面的需求执行一些操作。例如通过模型（Model）与数据库进行通信；使用模板渲染 HTML或者任何格式化过的响应；访问页面出错，抛出一个异常等。在处理过程中，视图处理的对象主要是 HttpResponse。 当 HttpResponse 对象离开 Django 后，被压缩成二进制流传输给浏览器（HTTP 请求的传输的内容是二进制数据）。 浏览器收到 HTTP 的响应头，呈现给用户。","link":"/236.html"},{"title":"Django 学习笔记之模型(上)","text":"上片文章讲解模板。你本文将讲解 “MTV” 中 M 层次，即模型层（数据存取层）。模型这内容比较多，我将其拆分为 3 个部分来讲解。同时，文章也配套了例子，你可以通过 阅读原文 来查看。 编程环境因为 Django 近期推出 Django 2.0 版本， 所以有必要再说明下。如果你是按照本系列来学习 Django 框架的话，按照前面安装 Django 的方式，你安装 Django 版本应该是最新版本，即 2.0。 那么使用最新 Django 版本来学习可以吗？如果是学习的话，不用太在意版本。当然学习最新的较好，因为可以学习新的 API。同时，Django 2.0 不再兼容 Python 2 了，现在学习 Python 都建议采用 Python 3版本了。另外 Django 1.8 官方只维护到 2018 年的 4 月，1.11 是最后一个兼容 Python 2 的 Django版本。如果是项目需要升级 Django版本，需要兼容到 Python 2，那么要考虑用 1.11 版本了。 顺便补充下本文用的一些工具的版本：Python 版本是 3.6，Mysql 版本是 5.5. 模型是什么在 Web 应用中，数据一般存储到数据库中。Django 中的模型层是跟数据库打交道的层次。模型层中可能会有多个模型，每个模型（每个 app 中的 models.py 中每个类都是一个模型）都对应着数据库中的唯一一张表。 配置数据库在我们探索 Django 的模型层之前，我们需要配置下数据库；告诉 Django 视野什么数据库以及如何连接数据库。这一步要确保配置无误，不然后面难以执行。我们找到新项目中的 setting.py, 里面有个 DATABASES 选项。Django 默认是使用 sqlite 数据库，所以你会看到里面 sqlite 数据库的配置信息。 123456DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), }} 上面的代码中的 ENGINE 是选择哪个数据库引擎, NAME 是数据库的名字。至于选择哪种引擎，要看项目中使用了什么数据库。Django 目前支持以下这 MySQL、PostgreSQL、Oracle 等数据库，它们的数据库引擎设置如下： 设置字段 数据库 设配器(驱动) postgresql PostgreSQL psycopg 1.x版 postgresql_psycopg2 PostgreSQL psycopg 2.x版 mysql MySQL MySQLdb sqlite3 SQLite 不需要 oracle Oracle cx_Oracle 其中设置字段是填充 ENGINE 的值。如果你使用的 MySQL 数据库，那么你需要填写 django.db.backends.mysql。数据库驱动表示需要使用 pip 安装该库。如果你使用的 MySQL 数据库，那么你需要安装 MySQLdb 设配器。 但是这里有个坑，MySQLdb 在支持 Python 2 版本，不支持 Python 3 版本。所以你安装该设配器之后，运行项目会报出错误。Django 官网建议使用替代品 mysqlclient。mysqlclient 是 MySQLdb 的一个分支，最主要是它支持 Python 3。 本文中使用到 Mysql 数据库，那么 DATABASES 的配置如下： 12345678910DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'django', 'HOST': '127.0.0.1', 'PORT': '3306', 'USER': 'root', 'PASSWORD': '123456', }} 第一个模型我们先新建名为 Django_demo 的 projeact, 再新建名为 demo 的 app。最后，别忘记在 setting.py 中将新创建的 app 激活。 12345678910# Django 2.0 的初始配置内容INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'demo', # 我们新创建的 app] 创建模型打开我们刚才创建的 app 中的 models.py 文件，我们以后就主要在这里编写模型。假设现在有个需求：某个作者创作了一本书，本书由出版社出版了。我们可以设定三者的字段以及关系。 假设作者有姓名、Email 邮箱这两个数据属性。 假设出版社有出版社名称、地址这两个属性。 书籍有两四个属性：书名、出版日期、作者、出版社。有一个或多个作者（和作者是多对多的关联关系[many-to-many]）， 只有一个出版商（和出版商是一对多的关联关系[one-to-many]，也被称作外键[foreign key]） 所以我们编写代码如下： 1234567891011121314151617from django.db import modelsclass Author(models.Model): name = models.CharField(max_length=40) email = models.EmailField()class Publisher(models.Model): name = models.CharField(max_length=30) address = models.CharField(max_length=50)class Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author) # publisher = models.ForeignKey(Publisher) # Django 2.0 需要加上 on_delete=models.CASCADE publisher = models.ForeignKey(Publisher, on_delete=models.CASCADE) publication_date = models.DateField() 我们定义每个模型，即类（如 Author 和 Book）都需要继承 django.db.models.Model。Model 是 Django 做了一层包装以便我们更加方便地使用的类， 它其中包含了所有数据库交互的方法。上面代码中的每个类相当于单个数据库表，每个属性也是这个表中的一个字段。 属性名就是字段名，它的类型（例如 CharField ）相当于数据库的字段类型 （例如 varchar ）。例如， Publisher 模块等同于下面这张表 12345CREATE TABLE \"Publisher\" ( \"id\" serial NOT NULL PRIMARY KEY, \"name\" varchar(30) NOT NULL, \"address\" varchar(50) NOT NULL,); 所以我们在为属性命名的时候，要避免使用数据库的关键字。另外，我们只需要关心每个类的属性以及长度限制，不用关心怎么创建数据库表。Django 可以自动生成这些 CREATE TABLE 语句的。 创建数据表我们上面的创建了几个模型还处于定义上，Django 还没有正真创建数据库中的表。因此，我们需要执行两个命令来同步一下数据库。 在我们刚才创建的工程 Django_demo 目录下，我们打开终端，执行以下命令： 1python manage.py makemigrations 运行成功效果图如下： 这一步相当于 在该app下建立 migrations目录，并记录下你所有的关于modes.py的改动，比如0001_initial.py， 但是这个改动还没有作用到数据库文件 第二步，在之前的终端上继续执行 migrate 命令。 1python manage.py migrate 运行成功效果图如下： 这一步表示将该改动（当makemigrations之后产生了0001_initial.py 文件）作用到数据库文件，比如 create table 之类。 如果你用到 Pycharm 的 Database 功能，你会看到我们刚才创建的定义的几个模型。 字段类型在前面的例子中，我们需要对每个属性设置一个字段，例如 CharField。Django 内置了几十种内置字段类型。常用的类型如下： 1) AutoField：一个根据实际ID自动增长的 IntegerField 。如果表中没有设置主键时，将会自动添加一个自增主键。 2）IntegerField：一个整数。在 Django 所有支持的数据库中，-2147483648 到 2147483647 范围才是合法的。 3）BigIntegerField：一个64位整数, 和 IntegerField 类似，不过它的范围比较大。 4）BooleanField：一个 true/false 字段。这个字段的默认表单部件是 CheckboxInput。 5）CharField：字符字段。对于比较大的文本内容,请使用 TextField 类型。这个字段的默认表单部件是 TextInput。它有个参数 max_length。max_length 表示字段允许的最大字符串长度。这将在数据库中和表单验证时生效 6）TextField：大文本字段。默认的表单部件是一个 Textarea。 7）DateField：日期。它带有两个可选参数：auto_now 和 auto_now_add。auto_now 表示当对象保存时，该字段会自动设置成当前时间。一般用于记录“修改时间” 。auto_now_add 记录字段首次被创建的时间。 8）DateTimeField：时间和日期。它也带有两个可选参数，名字和用法跟 DateField 一样。 9）TimeField：时间字段, 类似于Python datetime.time 实例. 和 DateField 具有相同的选项。 10）URLField：一个 CharField 类型的URL，默认长度是200；默认的表单部件是一个 TextInput。 11）EmailField：一个检查输入的email地址是否合法的 CharField 类型。 12）FileField：上传文件字段。 13）ImageField：图片字段，它继承了 FileField 所以属性和方法。 关系字段关系字段(Relationship fileds) 也是属于字段，只不过三个字段比较特殊，所以单独拿出来说。我们按照上述的创建模型的例子来继续讲解。它们三者之间的关系应该这样：一本书由一家出版社出版，一家出版社可以出版很多书。一本书由多个作者合写，一个作者可以写很多书。 1）ForeignKey表示属于模型间关系中的多对一关系。在我们的范例模型中，一家出版社 publisher 可以出版很多书 Book。在数据库中, Django 使用 ForeignKey 字段名称＋ “_id” 做为数据库中的列名称。在上面的例子中, 书籍 model 对应的数据表中会有一个 publisher_id 列。你可以通过显式地指定 db_column 来改变该字段的列名称,不过，除非你想自定 义 SQL ，否则没必要更改数据库的列名称。 它第一个参数必须传入该模型关联的类。on_delete 现在可以用作第二个位置参数(之前它通常只是作为一个关键字参数传递). 在Django 2.0中，这将是一个必传的参数。 2）OneToOneField它属于 ForeignKey 中的特例。当 ForeignKey 中有个字段 unique 被设置为 True 时， 就表示一对一关系。 3）ManyToManyField属于模型间关系中的多对多关系。在我们的范例模型中， Book 有一个 多对多字段 叫做 authors。因为他们的关系是一本书由多个作者合写，一个作者可以写很多书。在数据库中Django 创建一个中间表来表示 ManyToManyField 关系。默认情况下，中间表的名称由两个关系表名结合而成。所以刚才我们创建数据库表的途中，会有四张表，而不是三表。 字段选项有些字段会有些特殊参数，但所有字段类型都又些通用的可选选项。先是常用的可选选项。1）null ：如果该参数设置为 True，Django将会把数据库中的空值保存为 NULL。不填写就默认为 False。 2）blank：如果为 True ，该字段允许为空值，不填写默认为 False。这个字段是用于处理表单数据输入验证。 3）primary_key：如果为 True，那么这个字段就是模型的主键。 4）unique：如果该值设置为 True, 这个数据字段在整张表中必须是唯一的。 5）default：设置该字段的默认值。 6）由二项元组构成的一个可迭代对象（列表或元组），用来给字段提供选择项。 如果设置了 choices，默认的表单将是一个选择框。具体使用例子如下： 12345678910from django.db import modelsclass Person(models.Model): SHIRT_SIZES = ( ('S', 'Small'), ('M', 'Medium'), ('L', 'Large'), ) name = models.CharField(max_length=60) shirt_size = models.CharField(max_length=1, choices=SHIRT_SIZES) 下篇文章，我们将讲解如果对这些模型（表）进行操作。 源码如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/239.html"},{"title":"Django 学习笔记之视图与URL配置","text":"本文章是自己学习 Django 框架的第三篇。前面两篇文章主要记录 Django 理论相关知识。从本篇文章开始，将以理论和实战方式讲述 Django 框架的知识。让我们一起来 coding 吧~ 新建项目我们开发 Web 程序是基于 Django 框架，所以要想创建 Django 项目。创建项目有两种方式，一种是使用 Django 管理任务 django-admin.py，另一种是借助 IDE 工具 Pycharm。 使用 django-admin.py 1）新建project在你准备存放项目的目录下，打开终端，执行新建命令。 123django-admin.py startproject Django_demo // Django_demo 为 project 的名称，你可随意命名// 如果执行失败，可以改用下面命令django-admin startproject Django_demo // Django_demo 为 project 的名称，你可随意命名 新建 Project 成功之后，你会发现目录下会多出一些文件。这些文件主要跟工程配置有关系，跟具体业务逻辑没啥关系。 2）新建 application新建了项目，为什么还要新建 application ？简单来说 project **可以理解为一个容器，application** 可以理解为 project 中的一个网站应用。对于每个Django项目有且只有一个 project, 而一个 project 可以包含多个 application。 到最外层的 Django_demo 目录下新建我们的 application 1python manage.py startapp test // test 为 application 的名称，你可随意命名 新建成功后，会发现多了一个名为 test 目录。看到里面有些文件，是否心中有种似曾相识的感觉？ 3）配置应用最后一步，我们需要把刚才新建的 application 加到 settings.py 中的 INSTALL_APPS 中 。具体操作是修改 Django_demo/Django_demo/settings.py 文件， 12345678910INSTALLED_APPS = ( 'django.contrib.admin', // 管理站点 'django.contrib.auth', // 认证系统 'django.contrib.contenttypes', // 用于内容类型框架 'django.contrib.sessions', // 回话框架 'django.contrib.messages', // 消息框架 'django.contrib.staticfiles', // 管理静态文件的框架 // Djaogo 默认包含上面的应用 'test', // 刚才新建的 application，如果有多个 application，依次在后面添加即可。) 借助 Pycharm 有些小伙伴可能使用命令工具方式比较复杂，是否有比较简便的方式？当然，那借助 Pycharm，直接一步搞定。打开 PyCharm IDE 工具，点击 File -&gt; New Project， 左边选择Django。新建如下图所示： 新建成功之后会看到这样的目录结构 ，Pycharm 已经帮我们搞定了大部分工作。 视图与URL配置第一个页面URL配置(URLconf.py) 是文件路由，是 URL 和 URL 对应视图的映射表 。换句话说，就是由它来分发网络请求，将每个 Web 请求根据 URL 地址来调用视图来显示。URL 模式的语法是： 12345678910urlpatterns = [ ''' url(路径匹配, view 函数, 可选参数, 可选别名), 路径匹配： 一个正则表达式字符串。一般写法是 **r'正则表达式'**。如果你对正则表达式不太熟悉，建议你先补习补习下。 view 函数： 一个可调用对象，通常为一个视图函数或一个指定视图函数路径的字符串 可选参数： 可选的要传递给视图函数的默认参数（字典形式） 可选别名： 可选参数，一般结合模板方便管理 ''' url(r'^admin/', admin.site.urls),] 一般在 URLconf.py 中新增一个 URL 表达式，就需要在 view.py 中增加一个视图函数。 我们来根据上述规则创建下首个页面。首先在 view.py 中增加首页视图函数。 12345# Create your views here.from django.http import HttpResponsedef index(request): return HttpResponse(\"Hello ！这是我第一个 Django 项目\") 视图中的函数名 index 对应是 URL 地址中的 path 部分。 那什么是 path 呢？URL 地址定义是 协议://host:port/path 。假如有个地址是 http://127.0.0.1:8000/music。http 就是协议；127.0.0.1 是主机号，主机号可以是 IP 地址或者域名，不过域名是比较常见，因为容易记；port 是端口号，如果端口号是 80 可以忽略不写，其他则要填写完整；music 就是前面说的 path，端口号后面第一个 “/“ 符号后面的字符串都是路径（path）。index 比较特殊，我们通常把 index 设定为首页，所以访问首页的时候，path 不用填写。如果你还不很了解，我们等会运行下程序，你就会涣然大悟。 在创建视图函数之后，我们需要在 urls.py 中配置好 url 匹配规则。 12345678from django.conf.urls import urlfrom django.contrib import adminfrom demo import views # 导入我们的视图urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^$', views.index), # 添加首页 url 匹配规则。index 就是刚才创建的函数。] 配置了 url 匹配规则，剩下最后一步运行验证了。点击 pycharm 工具上的运行按钮（绿色的播放键），然后打开浏览器，在地址栏输入 http://127.0.0.1:8000。你会看到我们 index 函数返回的内容。 如果你访问的地址是 http://127.0.0.1:8000/，同样也是能正常看到页面内容。 pycharm 能启动一个 web 服务器，内部是使用到 manage.py 脚本。因此，我们也可以使用命令行的形式来启动一个 web 本地测试服务器。在最外层的 Django_demo 目录下，打开终端命令行工具，执行 python manage.py runserver 即可启动服务器。 返回 HTML 页面视图返回结果是一串字符串，我们只是用于做测试用的。但是实际开发中，返回结果通常是 html 页面。view 函数想要返回 html 页面，使用 render() 携带一个 html 页面即可。render() 内部返回的也是一个 HttpResponse 对象。 我们在 view.py 中增加名为 content 的视图函数，用来返回一个 html 页面。 12345# Create your views here.from django.http import HttpResponsedef content(request): return render(request, 'content.html') 接着在 application 目录下新建名为 templates 的文件夹，再创建一个 content.html 文件。 12345678910111213&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;content&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;这是 content 页面。&lt;/h1&gt; &lt;h2&gt;这是 content 页面。&lt;/h2&gt; &lt;h3&gt;这是 content 页面。&lt;/h3&gt;&lt;/body&gt;&lt;/html&gt; 还要配置下 url 路由规则。 12345678from django.conf.urls import urlfrom django.contrib import adminfrom demo import views # 导入我们的视图urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^content/$', views.content) # 添加 content 页面的 url 匹配规则。] 最后一步，使用浏览器浏览 content 的页面。 url 路由的命名组url 路由有另种命名组，一种是无名名组，另一种是有名分组。 无名分组是使用简单的、没有命名的正则表达式组（通过圆括号）来捕获 URL 中的值。上述两个例子和以下的一些都是都是无命名分组。 123456789from django.conf.urls import urlfrom demo import viewsurlpatterns = [ url(r'^content/2018/$', views.special_case_2018), url(r'^content/([0-9]{4})/$', views.year_content), url(r'^content/([0-9]{4})/([0-9]{2})/$', views.month_content), url(r'^content/([0-9]{4})/([0-9]{2})/([0-9]+)/$', views.detail_content)] 如果返回的 URL 的 path 为 /content/2018 时，是无法匹配到上面任何一个模式，因为每个模式要求 URL 以一个斜线结尾。paht 为 /content/2018/ 则能匹配到两个模式，是匹配到哪个呢？因为模式按照顺序来匹配的，所以第一个模式会被优先匹配。path 为 /content/2018/02/ 将匹配到第三个模式。Django 调用的是 views 文件中的 month_content(request, ‘2018’, ‘02’)。 无名分组的视图只能接受 python 中传入的固定值参数，如值 2018 等。但是无法获取到存放值的变量，而有名分组恰恰能解决这个痛点。有名分组只是在无名分组的正则表达式上增加一个参数即可。语法是(?Ppattern)，其中 name 是组的名称，pattern 是要匹配的模式。具体改造如下： 123456789from django.conf.urls import urlfrom demo import viewsurlpatterns = [ url(r'^content/2018/$', views.special_case_2018), url(r'^articles/(?P&lt;year&gt;[0-9]{4})/$', views.year_content), url(r'^articles/(?P&lt;year&gt;[0-9]{4})/(?P&lt;month&gt;[0-9]{2})/$', views.month_content), url(r'^articles/(?P&lt;year&gt;[0-9]{4})/(?P&lt;month&gt;[0-9]{2})/(?P&lt;day&gt;[0-9]{2})/$', views.detail_content)] 经过改造之后，当访问 path 为 /content/2018/02/ 时，请求将调用views.content(request, year=’2018’, month=’02’)函数，而不是views.month_content(request, ‘2018’, ‘02’)。 路由转发器（include）如果一个项目下有很多的 application，那么在 urls.py 里面就要写巨多的 urls 映射关系。这样看起来很不灵活，而且杂乱无章。这时候就要根据不同的 application 来分发不同的url请求。 假如在上述的 project 中，我又新建了一个新的 application，名为 app02。配置路由转发器操作如下：首先，在 urls.py 里写入 urls 映射条目。值得注意是，要导入Django 的 include 方法。 1234567from django.conf.urls import url, includefrom django.contrib import adminurlpatterns = [ url(r'^admin/', admin.site.urls), url(r'^app02/', include('app02.urls')), // 将 url 为”app02/“的请求都交给 app02 下的 urls 去处理] 其次，在 app02 目录下创建一个 urls.py 文件，用来处理请求的url，使之与 views 建立映射。 1234567from django.conf.urls import include, urlfrom app02 import views urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'index/$', views.index),]","link":"/237.html"},{"title":"Django 学习笔记之模型(下)","text":"上篇文章讲解了 Django 如何创建模型，本文将继续讲解如何对模型进行增删改查操作。 前言当我们建立好数据模型，Django 会自动为我们生成一套数据库接口相关的接口。这套接口称为 QuerySet API。为什么叫 QuerySet ? 因为从数据库中查询出来的结果一般是一个集合，这个集合叫做 QuerySet。 为了方便理解，我继续使用上篇文章的例子。另外方便我们在打印对象信息时，能得到对象的信息。所以我们需要对之前的代码做下修改。分别为每个模型类添加一个方法 unicode()。 unicode() 方法告诉 Python 如何将对象以 Unicode 的方式显示出来。 为以上三个模型添加 unicode() 函数后，就可以看到效果了： 123456789101112131415161718192021222324from django.db import modelsclass Author(models.Model): name = models.CharField(max_length=40) email = models.EmailField() def __unicode__(self): return self.nameclass Publisher(models.Model): name = models.CharField(max_length=30) address = models.CharField(max_length=50) def __unicode__(self): return self.nameclass Book(models.Model): title = models.CharField(max_length=100) authors = models.ManyToManyField(Author) publisher = models.ForeignKey(Publisher, on_delete=models.CASCADE) publication_date = models.DateField() def __unicode__(self): return self.title unicode() 方法可以进行任何处理来返回一个 unicode 对象。总所周知，Python 内部对字符串都是使用 Unicode 来保存的。不像字符串那样，有什么 UTF-8、GB2312 等编码。所以我们在Python 中处理 Unicode 对象的时候，你可以直接将它们混合使用和互相匹配而不必去考虑编码细节。 创建对象为了更加直观的操作数据库，我使用 Django 的 API 来讲解。在项目的目录下，打开终端执行以下命令： 1python manage.py shell 然后在终端中依次输入以下代码： 1234# 前面的 &gt;&gt;&gt; 是终端自带的&gt;&gt;&gt; from demo.models import Publisher&gt;&gt;&gt; p = Publisher(name='清华大学出版社', address='北京')&gt;&gt;&gt; p.save() 接着使用 Pycharm 的 Database 功能，查询 demo_publisher 表，你会发现新增一条数据。 现在来说说刚才发生了什么。第二行代码，即初始化一个 Publisher 实例， 这个实例并没有对数据库做修改。只有调用了 save() 函数，记录才会提交到数据库。所以， 使用这种方法创建实例，最后一定要调用 save() 函数。 另外上述方法来创建实例，另外还有 3 种方法：1）方法2这种办法可以算是方法 1 的变形。 123456from demo.models import Publisherp = Publisher()p.name='北京大学出版社'p.address='北京'p.save()# 别忘记最后调用 save() 函数 2）方法3这种办法是用到 objects.create() 函数 123from demo.models import PublisherPublisher.objects.create(name='人民邮电出版社', address='北京')# 这种办法无需调用 save() 3）方法4这种办法是用到 objects.get_or_create() 方法。使用这种办法有好处也有坏处。好处是可以防止重复插入；那么坏处就是插入速度要相对慢些，因为它要先查询。 12345from demo.models import PublisherPublisher.objects.get_or_create(name='清华大学出版社', address='北京')# 这种办法无需调用 save()# 执行结果如下：(&lt;Publisher: Publisher object (1)&gt;, False) 返回结果跟其他方法返回结果有点不同，它返回是一个元组。第一个是 Publisher 对象；第二个为一个布尔值，如果能新建成功为 True，已经存在则为 Flase。 如果模型中存在有一对多，多对一，多对多的关系，先把相关的对象查询出来或者创建出来，才能创建该模型。例如我们要创建 Book 对象，首先要创建 Author 和 Publisher 对象。具体代码如下： 12345678910111213import datetimefrom demo.models import Publisher, Author, Book# 获取 Publisher 对象pub = Publisher.objects.get(name='清华大学出版社')# 创建并获取 Publisher 对象Author.objects.create(name='令狐冲', email='makeTheFoxRush@xx.com')aut = Author.objects.get(name='令狐冲')# 创建 Book 对象, 要注意添加 id 字段book = Book(id=None,title='令狐传', publisher=pub, publication_date=datetime.date.today())# 一定要先保存数据到数据库，才能添加多对多关系的对象 authorbook.save()book.authors.add(aut)book.save() 查询对象Django 提供在查询数据功能方面做了很多优化工作， 这让我们查询数据有千万种方法。 查询单条数据其实在上面的例子， 我们已经运用到单条数据功能。没错，就是使用 get() 方法来获取单条数据。 1pub = Publisher.objects.get(name='清华大学出版社') 查询多条数据如果现在我们需要查询符合某个条件的数据，get() 只能返回一条数据，无法满足我们的需求。所以我们需要用到过滤器 filter。 12345from demo.models import PublisherPublisher.objects.filter(address='北京')# 运行结果&lt;QuerySet [&lt;Publisher: Publisher object (1)&gt;, &lt;Publisher: Publisher object (2)&gt;, &lt;Publisher: Publisher object (3)&gt;]&gt; 查询多条数据的返回结果为 QuerySet，这部分等会继续讲解。 另外 filter 还支持其他过滤条件，例如 123456789101112131415# 正则表达式Publisher.objects.filter(address__regex='^北京')# 部分查询# 查询地址中包含'北'字的出版社Publisher.objects.filter(address__contains='北')# 还有很多不一一例举# 如果上述条件的前面有个字母 'i', 表示不区分大小写# 正则表达式，但不区分大小写Publisher.objects.filter(address__iregex='^beijing')# 部分查询，但不区分大小写Publisher.objects.filter(address__icontains='bei') 如果要查询全部数据或者一段连续区间的数据，可以使用 all() 函数 12345# 查询所有数据Publisher.objects.all()# 查询一段连续区间的数据Publisher.objects.all()[:3] [:3] 是用到 Python 中的切片操作。因为上限从 0 开始可以忽略不写，所以它等同于 [0:3]。查询出来结果没有包含上限的值，即下标为 3 的值。[:3] 只查询下标为 0, 1 ,2 的数据。 但是这里比较特殊，QuerySet 对象的 id 是从 1 开始的，所以 [:3] 表示 [1:3], 返回 id 为 1, 2, 3 的对象。 另外，这种切片操作时可以节约内存的。 更新数据更新数据操作，一般是在查询数据后才执行。 更新单条数据更新单条数据也有两种方法，其中一种的用法跟使用方法 2 创建对象类似，另一种则是使用 update_or_create() 。具体代码如下： 12345678910# 方法1from demo.models import Publisherp = Publisher.objects.get(name=\"北京大学出版社\")p.name='上海大学出版社'p.address='上海'p.save()# 别忘记最后调用 save() 方法# 方法2p = Publisher.objects.update_or_create(name='北京大学出版社', address='上海') update_or_create() 方法是以模型的其中一个属性去匹配，如果数据库中有匹配数据就更新后面的值，否则则创建新的数据。 更新多条数据批量更新多条数据，一般是在 all()，filter() 后面执行 update() 函数 12from demo.models import Publisherp = Publisher.objects.filter(address='北京').update(address='北京海淀') 删除数据删除单条数据或多条数据，用法跟更新数据类似。具体就不逐一展开讲解了，大概说下用法即可。删除单条数据，获取数据，然后调用 delete() 函数。删除多条数据，同样在获取数据后调用 delete() 函数。 QuerySet 用法前面讲到，使用 all()，filter() 查询多条数据，返回的结果是一个 QuerySet 对象。它不是个列表，但是可以使用** list() 将其转变为列表**。 123from demo.models import Publisherpubs = Publisher.objects.all()pub_list = list(pubs) 可跌代性QuerySet 是一个可迭代的对象。因此，可以使用 for 循环来遍历。代码如下： 1234567891011# 在 views.py 中from django.http import HttpResponsefrom django.shortcuts import renderfrom demo.models import Publisher# Create your views here.def index(request): pubs = Publisher.objects.all() for p in pubs: print(p.name) return HttpResponse(\"Hello\") 你会在 Pycharm 的控制台上看到查询的数据。 支持排序QuerySet 支持对查询结果排序。例如将出版社按照名称来排序， 123456789101112131415161718# 在 views.py 中from django.http import HttpResponsefrom django.shortcuts import renderfrom demo.models import Publisher# Create your views here.def index(request): pubs = Publisher.objects.all().order_by('name') print(=== 正序排序 ===) for p in pubs: print(p.name) # 在 column name 前加一个负号，可以实现倒序 repubs = Publisher.objects.all().order_by('-name') print(=== 反序排序 ===) for p in repubs: print(p.name) return HttpResponse(\"Hello\") 组合查询QuerySet 还支持跟 SQL 语法中的组合查询，具体用法如下： 123456# 查询结果中同时满足 name=清华大学出版社 和 address=上海, 这两个条件Publisher.objects.filter(name=\"清华大学出版社\").filter(address=\"上海\")# 查询结果中同时满足 name=清华大学出版社 和 address 不是上海, 这两个条件# exclude() 函数排除指定的内容Publisher.objects.filter(name=\"清华大学出版社\").exclude(address=\"上海\") 其他前面说到切片操作的区间是从整数到无穷大的，在Python 语法中还有负查询，即区间是从负无穷大到 0。可惜的是 QuerySet 是不支持负查询。 但是也有替代方法 123# 使用 reverse() 将 QuerySet 的顺序倒置下# 再使用正查询， 下面的例子表示查询最后两条数据。Publisher.objects.all().reverse()[:2] 如果还要获取 QuerySet 里面存放对象的个数，可以使用 count() 函数来查询数量。内部实现是用执行 SELECT COUNT(*) SQL 语句。 12count = Publisher.objects.all().count()print('count == ', count) 源码如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/340.html"},{"title":"Django 学习笔记之后台管理","text":"本文是 Django 学习笔记系列的第七篇。前面 6 篇文章，我们已经了解了 Django MTV 模型中三个层的内容。这部分内容算是最基础，也是最重要。本文的内容相对简单，阅读起来会比较轻松些。主要是介绍下 Django 默认管理后台以及一些实用后台管理系统的第三方应用。 前言每个网站无论大小，大型电商网站也好，个人博客也罢，它们都是一个管理后台。管理后台可以看做一个窗口，管理员通过它来管理以及维护网站。 Django 作为一个全能型的框架，当然也自带了一个后台管理系统。登录后台管理希望能对前端或者数据库数据进行增加、修改、删除等工作。我们现在就激动该系统来学习。 激活管理界面其实 Django 默认帮我们激活 admin 管理后台。不知你还记得上次的操作？ 当新建创建应用的，需要将刚创建的 app 加入到 setting.py 文件中。在 setting.py 文件中，你会看到前面有很多应用。 123456789INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'demo', # 我们新创建的 app] 其中 django.contrib.admin 就是管理后台。因此，我们可以知道 admin 其实也是一个应用，只不过它是 Django 自带的。 我们只需要做一件事，那就是创建用户。在项目目录下，打开终端，执行以下命令来创建超级管理员。 1python manage.py createsuperuser 打开终端之后，依次输入用户名，邮箱和密码即可创建。成功创建换管理员结果如下： Django 在密码校验这块做的还不错。我使用弱密码 ‘admin’和’1234678’，Django 都不让我通过。 成功创建超级用户之后，使用浏览器访问后台管理系统： 127.0.0.1:8000/admin。 输入用户名和密码并进行登录，会看到以下界面。因为是空项目，所以没有数据，只有显示用户账号的信息。你可以对 admin 进行二次开发，这样你就能在后台对数据库数据进行操作。 常用的 admin 应用我推荐这几个应用都是第三方开源项目，都支持 Django 2.0 版本。具体安装方法可以阅读各个插件的 Github 仓库地址。 1) Xadmin Xadmin 在 Django 后台管理系统应用库中算是名声显赫。它算是一款内置功能比较丰富的框架，提供了基本的CRUD功能，还内置了丰富的插件功能；还包括数据导出、书签、图表、数据添加向导及图片相册等多种扩展功能。 最重要的是，它使用起来非常方便。我们只需要定义数据的字段等信息，即可获取一个功能全面的管理系统。 推荐指数：5 星github 地址: 传送门 2) django-grappelli django-grappelli 可以算是一个功能强大的应用。它使用网格状的形式来呈现数据，这个给人简洁大方的感觉。因此，django-grappelli 更加适合需要对数据频繁操作的场景。 推荐指数：4 星github 地址：传送门 3) django-material django-material 是采用谷歌 Material Design 来设置 UI 。自己比较喜欢 MD 这种风格的界面，所以推荐给大家。 推荐指数：3 星半github 地址：传送门 4) django-admin-bootstrap django-admin-bootstrap 是一个能自动隐藏侧面菜单栏的响应式管理后台。它跟 Xadmin 一样，都是基于 bootstrap 开发的。个人觉得比较适合初学者来学习和研究。 推荐指数：3 星github 地址：传送门","link":"/341.html"},{"title":"Django 学习笔记之表单","text":"本文是自己 Django 学习笔记系列中第 8 篇，算是基础知识篇章中最后一篇笔记。后续的笔记内容会相对比较综合。所以建议大家要把前面的内容，包括本篇笔记掌握。而本篇内容主要是讲解表单。 表单是什么？表单英文单词是 Froms, 它其实属于 HTML 的知识范畴。HTML 表单可以实现用户和 Web 站点之间数据交互。表单允许用户将数据发送到 Web 站点。 但在大多数情况下，Froms 携带的数据发送到 Web 服务器，Web 页面会将其拦截并自己使用它。举个栗子，用户使用浏览器访问一个页面，在页面的搜索框中输入图书的名称，想获取所有销售该图书的商店。Web 站点需要获取图书名称的信息作为数据库查询条件，所以将数据拦截并获取图书的名称。然后通关查询数据库，最后将查询到的所有商店信息返回给浏览器进行渲染显示。另外，博客系统中的评论模块也是这个原理。因此，在一些站点上会爆出 XSS 漏洞。原因可能是编码者没有对用户提交的数据进行过滤或者过滤不严，直接存储到数据库中。 HTML 表单这部分是给不熟悉 HTML 表单同学准备的，如果你已经掌握这部分知识。可以选择直接跳过。 HTML 表单在页面中表现是一个可以填写数据的区域。表单中会根据页面显示需求，采用不同的表单元素来呈现，比如：文本域(textarea)、下拉列表、单选框(radio-buttons)、复选框(checkboxes)等等 它可能长得这个样子 表单使用标签 form 来设置区域范围，它携带常用的属性如下： 123456&lt;form action=\"search.html\" method=\"GET\" target=\"_blank\" &gt; &lt;!-- ... 表单元素 --&gt;&lt;/form&gt; action 属性：指定表单数据提交到哪个页面。例子中是提交到 search.html 页面，这个也会跳转到 search.html 页面。如果你想把数据提交到原来的页面，action 的值为空就行，即 action=”” method 属性：规定提交表单时所用的 HTTP 方法，一般选择** GET 或者 POST**。 target 属性：规定 action 属性中地址的目标（默认：_self）。如果填写值** _blank** ，当点击按钮提交数据时，在新窗口中打开新的页面。 常用表单元素有以下这些： 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt; &lt;form action=&quot;&quot; method=&quot;GET&quot; &gt; &lt;!-- label 标签用于显示内容，不可以输入 --&gt; &lt;label &gt;我是 label 标签&lt;/label&gt;&lt;br&gt; &lt;!-- input 元素 --&gt; &lt;!-- 单行的文本输入框 --&gt; &lt;input type=&quot;text&quot; name=&quot;lastname&quot;&gt;&lt;br&gt; &lt;!-- 单选按钮 --&gt; &lt;input type=&quot;radio&quot; name=&quot;sex&quot; value=&quot;female&quot;&gt;female&lt;br&gt; &lt;!-- 提交按钮, 用于提交表单数据 --&gt; &lt;input type=&quot;submit&quot; value=&quot;submit&quot;&gt; &lt;!-- 还有其他的表单元素, 就不一一列举 --&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 对于每个输入字段 ，必须设置一个 name 属性，数据才会被正确提交。因为页面是通过 name 属性中的值来获取用户输入的内容的。以 GET 方式请求为例，有个单行输入框定义 **name=”q”。当你在输入框中填写值 moneky 然后提交。你会发现浏览器地址发生变化了，从之前的 127.0.0.1:8000 变为 127.0.0.1:8000/?q=monkey Django Form功能Django 的表单针对 HTML 表单实现了一层封装，这使得 Django 的 Form 表单功能更加强大。它具有以下功能：1）自动生成HTML表单元素2）检查表单数据的合法性3）如果验证错误，重新显示表单（数据不会重置）4）数据类型转换（字符类型的数据转换成相应的Python类型） Form 对象Objects Form对象封装了一系列 Field 和验证规则，Form 类都必须直接或间接继承自 django.forms.Form，定义 Form 有两种方式： 方法一：根据 Model 自动生成 Form如果你的需求比较简单，只想将模型的字段全部以表单的形式展示出来，你可以采用这种方法。 123456from django import formsclass AuthorFormOne(forms.Form): name = forms.CharField(max_length=40,label='名称') email = form.EmailField() message = form.CharField(widget=forms.TextInput) Form 表单除了定义属性跟模型差不多，它还具有一些特有的属性。 1) Widget用来渲染成 HTML 元素的工具，如：forms.TextInput 对应 HTML中的 input标签2) Form一系列 Field 对象的集合，负责验证和显示 HTML 元素。3) Form Media用来渲染表单的 CSS 和 JavaScript 资源。 方法二：自定义 Form自定义表单是比较高级用法，有时候通过 Model 自动创建的 Form 无法满足自己需求。譬如：Model 中的某些属性我不需要显示在页面上，或数据处理方式比较复杂，这个时候你就需要自定义 Form。自定义 Form 是直接继承 Form 12345678910# 在 models.py 定义模型 Authorclass Author(models.Model): name = models.CharField(max_length=40) email = models.EmailField()# 新建 forms.py 文件, 用于定义 Formclass AuthorFormTwo(ModelForm): class Meta: model = Author fields = ('name',) # 只显示 model 中指定的字段 视图层的处理在视图文件 view.py 中， 可以获取、过滤到用户提交的数据。 123456789101112131415161718192021222324from django.http import HttpResponseRedirectfrom django.shortcuts import render# Create your views here.from demo_form.form.forms import AuthorFormOnedef formView(request): # 过滤 POST 方法的请求 if request.method == \"POST\": form = AuthorFormOne(request.POST) # 验证表单 if form.is_valid(): # 一般过滤数据 name = form.cleaned_data['name'] email = form.cleaned_data['email'] information = form.cleaned_data['information'] # 处理业务, 如查询数据库信息 return HttpResponseRedirect('/') else: # 不是 GET 请求则显示表单 form = AuthorFormOne() templateView = 'author.html' return render(request, templateView, {'form':form}) form.is_valid() 返回 true 后，表单数据都被存储在 form.cleaned_data 对象中（字典类型，意为经过清洗的数据）。而且数据会被自动转换为 Python 对象，如：在 form 中定义了 DateTimeField ，那么该字段将被转换为 datetime 类型。 而模板文件内容则比较简单，使用几个 HTML 标签以及模板标签就轻松搞定。 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;author 表单&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action='/forms/' method='POST'&gt; {% csrf_token %} {{ form }} &lt;input type=\"submit\" value=\"提交\" /&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 我在 url.py 定义表单的路径是： 123456789from django.contrib import adminfrom django.urls import pathfrom demo_form import viewsurlpatterns = [ path('admin/', admin.site.urls), path('forms/', views.formView),] 所以使用浏览器访问 http://127.0.0.1:8000/forms/， 你会发现页面会自动渲染出表单的信息。 美化模板我们虽然成功把表单内容渲染到页面上，但是页面有点丑陋。你可能会无法忍受，想把页面修改得美观一点，顺便也秀秀自己的 Bootstrap 知识。 Django 默认提供几种显示表单的方式。例如 form.as_p、form.as_table、form.as_ul，在 html 文件中会被渲染成 p 标签，table 标签和 ul 标签。除此之外，还可以 form 还支持自定义。具体实现是你获取到 form 中每个属性，然后逐一渲染指定样式。 所以 author.html 经过调整之后的代码如下： 123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;author 表单&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action='/forms/' method='POST'&gt; &lt;div class=\"\"&gt; {{ form.name.errors }} &lt;label for=\"name\"&gt;姓名：&lt;/label&gt;&lt;br&gt; {{ form.name }} &lt;br&gt; &lt;/div&gt; &lt;div class=\"\"&gt; {{ form.email.errors }} &lt;label for=\"email\"&gt;邮箱：&lt;/label&gt;&lt;br&gt; {{ form.email }} &lt;br&gt; &lt;/div&gt; &lt;div class=\"\"&gt; {{ form.information.errors }} &lt;label for=\"information\"&gt;个人信息：&lt;/label&gt;&lt;br&gt; {{ form.information }} &lt;br&gt; &lt;/div&gt; &lt;p&gt;&lt;input type=\"submit\" value=\"提交\" /&gt;&lt;p&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 源码如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/342.html"},{"title":"Django 实战1：搭建属于自己社工查询系(上)","text":"前面的文章已经把模板、模型、视图、表单等知识点逐一讲解，大家已经熟悉它们具体用法。但如何将其串联起来还一筹莫展。本篇文章分享我之前做过的一个小项目，帮助大家抹开这一层迷雾。 想做什么？我分享的项目是一个社工库查询系统。大家不要一拿到源码，就直接去阅读。建议先思考下，如果让我来设计社工库查询系统，我要如何实现？ 我就先阐述我思考的内容。既然项目是一个查询系统，那么重点在于查询。众所周知，查询数据则是执行 SQL 语句来从数据库中获取数据。SQL 查询语句一般使用表中的字段作为查询条件。哪些能作为查询条件呢？先让我们来看看数据库表是怎么定义的？ 数据库表数据库中只有一个表，名为 Socialusers。它的创建语句是： 12345678910111213141516Create table if not exists `SocialUsers` ( `id` INTEGER PRIMARY KEY, `username` varchar(64), `password` varchar(64), `chineseName` varchar(64) , `email` varchar(64) , `QQ` varchar(15) , `weibo` varchar(300), `identity_number` varchar(25), `cell_phone` varchar(20), `ip_address` varchar(100), `college` varchar(60), `living_place` varchar(200), `source` varchar(50), `remarks` varchar(400) ); 表中一共有 13 个字段，其中有 9 个字段的内容属于敏感字段，涉及到个人账号安全问题。因此，我将这 9 个字段，分别是 username、password、chineseName、email、QQ、identity_number、cell_phone、college、source 作为查询条件。 那么系统最终的效果是根据查询条件和用户输入的内容来查询数据。如果数据库匹配到数据，就通过表格形式打印数据。反之则提醒用户查询不到数据（查询不到数据应该值得庆幸的，说明你的账号数据没有被泄露）。 我将查询条件以下拉框的形式显示，让用户可以自行选择查询条件。查询内容以文章输入框展示，提供用户输入数据的接口。到这里可以回答之前的想要做什么的问题了。我们最终要实现的页面效果如下： 模型建立数据库已经有了表，我直接使用 Django 自带工具生成 Socialusers 模型。这一步就省略不讲，如果你对这步操作不是很熟悉，可以阅读上篇文章的内容。 最终生成的模型如下： 123456789101112131415161718192021class Socialusers(models.Model): id = models.IntegerField(blank=True, primary_key=True) username = models.CharField(max_length=64, blank=True, null=True) password = models.CharField(max_length=64, blank=True, null=True) # Field name made lowercase. chinesename = models.CharField(db_column='chineseName', max_length=64, blank=True, null=True) email = models.CharField(max_length=64, blank=True, null=True) # Field name made lowercase. qq = models.CharField(db_column='QQ', max_length=15, blank=True, null=True) weibo = models.CharField(max_length=300, blank=True, null=True) identity_number = models.CharField(max_length=25, blank=True, null=True) cell_phone = models.CharField(max_length=20, blank=True, null=True) ip_address = models.CharField(max_length=100, blank=True, null=True) college = models.CharField(max_length=60, blank=True, null=True) living_place = models.CharField(max_length=200, blank=True, null=True) source = models.CharField(max_length=50, blank=True, null=True) remarks = models.CharField(max_length=400, blank=True, null=True) class Meta: managed = True db_table = 'SocialUsers' 注意下 chinesename 和 qq 这两个属性。我们在数据库中的定义是 chineseName 和 QQ。而 Django 在生成的模型时，自动将大写转为化小写。因此，当我们在使用 Socialusers 这两个属性时，要注意它们的定义已经改变，属性的定义全是小写的。 表单实现表单的实现由两种方式:一种是根据 model 生成 Form，另一种是自定义 Form。我分别把这两种方式实现。你可以对比下这两种写法的差异，以及如何给表单指定 bootstrap CSS 样式的。 根据 model 生成 Form在 app 目录下新建文件夹 forms 以及文件 forms.py 。forms.py 主要存放表单的定义，实现代码如下： 1234567891011121314151617181920212223242526272829303132# forms.py CONDITION_CHOICES = ( ('username', '用户名'), ('password', '密码'), ('chineseName', '姓名'), ('email', '邮箱'), ('QQ', 'QQ'), ('identity_number', '身份证'), ('cell_phone', '电话'), ('college', '大学'), ('source', '来源'),)class QueryUserForm(forms.Form): condition = forms.CharField( # 也可以定义为 ChoiceField max_length=20, widget=forms.Select(choices=CONDITION_CHOICES, attrs={'class':\"form-control\", 'title':\"query condition\", 'name':'condition', }), localize=('username', '用户名') ) queryContent = forms.CharField( max_length=100, widget=forms.TextInput(attrs={'class': 'form-control is-invalid', 'name': 'queryContent', 'placeholder': '请输入需要要查询的内容...' }), error_messages={'required': '查询内容不能为空 !'} ) 其中， widget 是指定字段呈现样式，如 condition 被指定呈现下拉框 Select；localize 是设置初始化值；error_messages 是错误提示形式。attrs 是为呈现的组件指定一些属性，如 CSS 样式、name 等。这部分内容，后面的文章会做详细讲解。 自定义 Form为了更好区分模型，我在 models.py 中新建一个模型来代替之前的 Socialusers 模型。 1234567891011121314151617181920# models.py # 用于表单查询的 modelCONDITION_CHOICES = ( ('username', '用户名'), ('password', '密码'), ('chineseName', '姓名'), ('email', '邮箱'), ('QQ', 'QQ'), ('identity_number', '身份证'), ('cell_phone', '电话'), ('college', '大学'), ('source', '来源'),)class QueryUser(models.Model): condition = models.CharField(max_length=20, choices=CONDITION_CHOICES) queryContent = models.CharField(max_length=100) def __str__(self): # __unicode__ on Python 2 return self.condition Form 的实现代码如下： 12345678910111213141516171819202122232425262728# forms.py class QueryUserForm(ModelForm): class Meta: model = QueryUser fields = ['condition', 'queryContent', ] # 指定呈现样式字段、指定 CSS 样式 widgets = { 'condition': Select(attrs={'class':\"form-control\", 'title':\"query condition\", 'name':'condition', }), 'queryContent':TextInput(attrs={'class': 'form-control is-invalid', 'name': 'queryContent', 'placeholder': '请输入需要要查询的内容...' }) } localized = { 'condition':('username', '用户名'), 'queryContent':123 } # 自定义错误信息 error_messages = { 'queryContent':{ 'required': '查询内容不能为空 !', } } 各个字段的含义跟第一种实现方式类似，我就不重复说明。 模板创建我创建名为 templates 文件夹来存放 HTML 文件。其中 index.html 是主页面，也是我们查询数据的页面。因为我前端框架使用的是 bootstrap，所以需要加载一些库。我为了满足在电脑无网络的状态也能使用的需求。我将其 bootstrap 所用到的库到打包到 static 目录下。 代码比较多，我只把重点内容贴出来。详细代码可以通过点击【阅读原文】查看完整代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657{% load staticfiles %}&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt; &lt;div class=\"container\" id=\"container\"&gt; &lt;div id=\"page-header\"&gt; &lt;h1 class=\"text-center\"&gt; 社工库查询系统 &lt;/h1&gt; &lt;/div&gt; &lt;div class=\"row\"&gt; &lt;form action=\"\" method=\"GET\" class=\"form-horizontal\" role=\"form\"&gt; &lt;div id=\"checkbox\" class=\"text-center\"&gt; &lt;label class=\"checkbox-inline text-success\"&gt;默认采用完整匹配&lt;/label&gt; &lt;/div&gt; &lt;div class=\"col-md-10 col-md-offset-1\"&gt; &lt;div class=\"col-md-2 col-md-offset-0\"&gt; {{ form.condition }} {% comment %} {{ form.condition }} 在 html 中将被渲染成以下代码 &lt;select name='condition' title=\"query condition\" class=\"form-control\"&gt; &lt;option &gt;用户名&lt;/option&gt; &lt;option&gt;密码&lt;/option&gt; &lt;option&gt;姓名&lt;/option&gt; &lt;option&gt;邮箱&lt;/option&gt; &lt;option&gt;QQ&lt;/option&gt; &lt;option&gt;身份证&lt;/option&gt; &lt;option&gt;电话&lt;/option&gt; &lt;option&gt;大学&lt;/option&gt; &lt;option&gt;来源&lt;/option&gt; &lt;/select&gt; {% endcomment %} &lt;/div&gt; &lt;div class=\"input-group col-md-10 col-md-offset-1\"&gt; {{ form.queryContent.field.errors }} {{ form.queryContent }} {% comment %} {{ form.queryContent }} 在 html 中将被渲染成以下代码 &lt;input type=\"text\" class=\"form-control is-invalid\" name=\"q\" placeholder=\"请输入内容...\" value=\"\"&gt; {% endcomment %} &lt;div class=\"input-group-btn\"&gt; &lt;button type=\"submit\" class=\"btn btn-primary\" required &gt;Search&lt;/button&gt; &lt;div class=\"invalid-feedback\"&gt; Please provide a valid value. &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;br&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 在 html 代码中，我直接将 form 标签直接写出来，里面的 Select 和 Input 标签通过 Django Form 来填充。下拉框使用表单的 condition 属性，即 来填充 ，输入框也是使用 来填充。当它们渲染的时候，会自动被解析为 Select 和 input 控件。 视图我设定是用户提交表单地址不改变。所以表单数据会被提交到原来的页面。因此，在视图的工作是拦截表单，并截取里面的内容。最后将查询结果返回给 HTML 页面。 1234567891011121314151617181920# views.pydef index(request): templateView = 'index.html' if request.method == 'GET': form = QueryUserForm(request.GET) # 验证表单 if form.is_valid(): # 过滤需要的数据, 获取查询条件和查询内容 condition = form.cleaned_data['condition'] keywords = form.cleaned_data['queryContent'] print('condition == ' + condition) print('keywords == ' + keywords) # 查询数据... return render(request, templateView, {'form': form}) # 直接访问主页, 显示的内容 else: return render(request, templateView, {'form': form}) 系统的基本框架已经搭建差不多。因为文章篇幅关系，一部分内容下篇文章讲解。下篇文章主要是如何查询数据、如何根据查询结果显示不同内容、如何将数据呈现出来。 源码如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/444.html"},{"title":"Django 实战1：搭建属于自己社工查询系(下)","text":"上篇文章已经完成框架搭建，本文接着上篇的内容继续讲解。本片主要的说三点内容，分别是：根据条件查询数据、根据查询结果显示不同内容、将查询数据填充到页面上。 逻辑优化在上篇文章，我在原来的 url 地址中处理用户提交的表单数据。Url 值改变了，但是页面没有刷新。同时，表单数据没有进行分类。这会导致用户不管提交什么数据，页面就呈现什么数据。 而正常的逻辑应该是这样。如果用户访问的是首页，那么不需要填充任何数据以及展示提示框。 如果用户提交了表单数据，但是数据库中没有查询到数据，则提醒下用户没有查询到相关数据。 如果用户提交了表单数据，数据库也能查询到数据。页面需要提醒用户查询到数据，并将查询结果展示出来。 为了解决这个问题，我通过定义一个变量 countNum 来区分。 根据查询结果显示不同内容视图改造按照解决方案，我们需要对 V 层进行改造。定义全局变量 countNum，初始化为 -1。如果用户访问的是首页，就直接返回。如果用户提交表单数据，数据库查询不到数据。则将 countNum 赋值为 0 ，然后返回。如果用户提交表单数据，数据库查询不到数据，就返回数据总条。 1234567891011121314151617181920212223242526272829303132333435363738# views.pydef index(request): templateView = 'index.html' countNum = -1 keywords = '' if request.method == 'GET': form = QueryUserForm(request.GET) # 验证表单 if form.is_valid(): # 过滤需要的数据 condition = form.cleaned_data['condition'] keywords = form.cleaned_data['queryContent'] print('condition == ' + condition) print('keywords == ' + keywords) countNum = 0 # 查询结果 # 假设经过查询, 一共获取到 3 条数据 countNum = 3 if countNum != 0: return render(request, templateView, { 'countNum': countNum, 'keywords': keywords, 'form': form, }) # 查询不到数据, 显示没有数据的浮窗 if countNum == 0: return render(request, templateView, { 'countNum': countNum, 'keywords': keywords, 'form': form, }) # 直接访问主页, 显示的内容 else: return render(request, templateView, {'countNum': countNum, 'form': form}) 模板改造T 层（模板）需要根据 V 层（视图）透传过来的 countNum 的值进行判断，然后渲染相应的内容。 123456789101112131415161718192021222324252627282930313233343536373839404142# index.html&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt; &lt;div class=\"container\" id=\"container\"&gt; &lt;!-- 前面代码不变 --&gt; {% if countNum == -1 %} &lt;!-- 显示首页内容, 不需要做处理 --&gt; {% else %} &lt;!-- 查询不到数据, 提醒用户更换 关键词或者类型 --&gt; {% if countNum == 0 %} &lt;div class=\"alert alert-warning alert-dismissible col-md-10 col-md-offset-1\" role=\"alert\"&gt; &lt;button type=\"button\" class=\"close\" data-dismiss=\"alert\"&gt; &lt;span aria-hidden=\"true\"&gt;&amp;times;&lt;/span&gt; &lt;span class=\"sr-only\"&gt;Close&lt;/span&gt;&lt;/button&gt; 找不到与&lt;b&gt;&amp;nbsp{{ keywords }}&amp;nbsp&lt;/b&gt;相关的结果。请更换其他&lt;b&gt;&amp;nbsp关键词或类型&amp;nbsp&lt;/b&gt;试试。&lt;/div&gt;&lt;/div&gt; {% else %} &lt;!-- 显示总数以及查询耗时 打印表格、说明头部行、查询到的数据--&gt; &lt;div class=\"row\"&gt; &lt;div class=\"alert alert-success alert-dismissible col-md-10 col-md-offset-1\" role=\"alert\"&gt; &lt;button type=\"button\" class=\"close\" data-dismiss=\"alert\"&gt; &lt;span aria-hidden=\"true\"&gt;&amp;times;&lt;/span&gt; &lt;span class=\"sr-only\"&gt;Close&lt;/span&gt;&lt;/button&gt; 找到与&lt;b&gt;&amp;nbsp {{ keywords }} &amp;nbsp&lt;/b&gt;相关的结果 {{ countNum }} 个。用时 {{ time }} 秒。&lt;/div&gt; &lt;div class=\"table-responsive col-md-12\"&gt; &lt;table class=\"table table-striped table-hover\"&gt; &lt;tr&gt; &lt;th class=\"text-center\"&gt;用户名&lt;/th&gt; &lt;th class=\"text-center\"&gt;密码&lt;/th&gt; &lt;th class=\"text-center\"&gt;姓名&lt;/th&gt; &lt;th class=\"text-center\"&gt;邮箱&lt;/th&gt; &lt;th class=\"text-center\"&gt;QQ 号码&lt;/th&gt; &lt;th class=\"text-center\"&gt;数据来源&lt;/th&gt; &lt;/tr&gt; &lt;!-- 填充查询到的数据 --&gt; &lt;/table&gt;&lt;/div&gt;&lt;/div&gt; {% endif %} {% endif %} &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 根据条件查询数据模型层主要跟数据库打交道， 数据库存储数据的地方。所以查询数据内容其实是模型知识地运用。 考虑到数据库中可能存在多条关键字（queryContent）相关的数据，所以需要使用 filter() 来匹配。我使用的匹配模式是精确匹配，所以无须使用正则表达式来匹配。直接把关键字作为模型过滤条件就可以了，实现代码如下： 12345# 把 Socialusers 的 username 属性作为 condition 来查询数据。# 查询内容是用户输入的内容 queryContentuser_list = Socialusers.objects.filter(username=keywords)# 获取总条数countNum = user_list.count() condition 记录用户选择下拉框的条目值。该变量的值决定模型 Socialusers 要使用哪个属性来查询。由于 python 没有 switch 语句，只能写多个 elif 来处理多个条件。那么代码可以这么实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# views.pydef index(request): templateView = 'index.html' countNum = -1 keywords = '' time = 0 if request.method == 'GET': form = QueryUserForm(request.GET) # 验证表单 if form.is_valid(): # 过滤需要的数据 condition = form.cleaned_data['condition'] keywords = form.cleaned_data['queryContent'] print('condition == ' + condition) print('keywords == ' + keywords) countNum = 0 # 查询结果 if condition == 'username': user_list = Socialusers.objects.filter(username=keywords) countNum = user_list.count() # 获取查询耗时 time = (connection.queries)[0].get('time') print('user_list size=== ', user_list.count()) print('time === ', time) # 显示分页操作, 每页显示 20 条 paginator = Paginator(user_list, 20) page = request.GET.get('page') try: users = paginator.page(page) except PageNotAnInteger: # 如果请求的页数不是整数，返回第一页。 users = paginator.page(1) except EmptyPage: # 如果请求的页数不在合法的页数范围内，返回结果的最后一页。 users = paginator.page(paginator.num_pages) return render(request, templateView, { 'countNum': countNum, 'condition': condition, 'keywords': keywords, 'form': form, 'users': users, 'time': time, }) elif condition == 'password': # 后面的代码逻辑跟前面类似, 只不过 filer() 的内容改变了。 # 查询不到数据, 显示没有数据的浮窗 if countNum == 0: return render(request, templateView, { 'countNum': countNum, 'keywords': keywords, 'form': form, }) # 直接访问主页, 显示的内容 else: return render(request, templateView, {'countNum': countNum, 'form': form}) 代码中使用到了分页 Paginator，这部分后面会继续讲解。 填充数据最后一步工作就是在模板中填充查询到的数据。因为我们在视图中将查询到 user_list 集合传递给模板。这里要注意的是，user_list 其实是一个查询集 QuerySet，不是真正意义上的列表，只不过命名为列表而已。在模板中，使用一个 for 循环逐一解析每个模型的值，就能完成数据填充工作。 12345678910111213# index.html&lt;!-- 填充查询到的数据 --&gt; # 在下面的注释下面，打印查询到的数据{% for user in users %}&lt;tr&gt;&lt;td class=\"text-center\"&gt;{{ user.username }}&lt;/td&gt;&lt;td class=\"text-center\"&gt;{{ user.password }}&lt;/td&gt;&lt;td class=\"text-center\"&gt;{{ user.chinesename }}&lt;/td&gt;&lt;td class=\"text-center\"&gt;{{ user.email }}&lt;/td&gt;&lt;td class=\"text-center\"&gt;{{ user.qq }}&lt;/td&gt;&lt;td class=\"text-center\"&gt;{{ user.source }}&lt;/td&gt;&lt;/tr&gt;{% endfor %} 第一个实战项目到这里就结束了。主要目的是让大家明白如何将模型、表单、模板、视图串联起来。后面的文章会讲解高级用法以及前面遗漏的细节内容。 源码如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/445.html"},{"title":"该如何学习 Python？","text":"在我的 QQ 学习交流群中，有位读者问我一个很有代表性的问题，在这里和大家分享下。 猴哥，能说下自学的学习经验吗？我都自学 Python 一个月了。 在我看来，这个问题的本质就是找到属于自己的学习方法。我把自己的经验分享给大家，希望对你们有帮助。在这之前，我要先说下学习效率的问题。因为我觉得掌握知识 = 学习方法 + 学习效率。 美国著名学习专家爱德家·戴尔曾提出一种学习方式的理论。它其实是一张图，名为学习金字塔效率图。 在国内，我们最经常接触的三种学习方式分别是听讲，阅读，实践。听讲指的是是上学时听老师讲课或者观看别人的教学视频。由图可知，这是一种学习方式效率最低的。随着时间地流逝，遗忘的内容最多。阅读书籍与听讲相比，则相对好点，但还是不高。而实践以及教授给他人，这两个种方式往往是被大家所推崇。 因此，我们学习新领域的知识时，可以先通过看别人的教学视频或者阅读相关书籍来入门。但一定要去实践，这能确保自己学到大部分知识。例如你正在看别人的 Python 入门课程，在课后自己要手动去敲代码。值得注意的是，不要照着课程的代码来敲打，那是没有效果。如果没有实践机会，也可以将知识讲给其他人。如果很不巧没有观众，那就自问自答 说完学习效率，现在来说下学习方法。观看别人的教学视频这种方式，我就不详细说了。只要你跟着讲师的节奏就可以。我就重点说下自己的阅读方式。因为我比较喜欢阅读纸质书籍，所以推荐都是书籍。如果你喜欢电子书，也可以阅读电子书或者技术博客。 学习新的东西，我首先统计需要学习知识的范围。划出这个范围很简单，每本书籍都有目录，目录的内容就是大致学习范围。然后翻翻几本书籍的目录。如果几本书籍同时出现的内容，这些内容就是重点。接着根据学习内容以及自己时间安排，指定学习计划。最后把学习内容分割为小内容到每天当中，每天坚持学习。 最后，我推荐一些个人觉得不错的 Python 书籍。目前这些书籍都是最新版本的，所以你不用担心过时问题。 { 编程入门 }《笨办法学 Python》《Python 基础教程（第3版·修订版）》《Head First Python (第2版)》《Python 编程：入门到实践》 { 开发进阶 }《Python 项目开发实战（第2版）》《精通 Python 设计模式》《Python 核心编程第3版》《Python 源码剖析》《Python 学习手册（第4版）》《Python Web 开发实战》《Python 3网络爬虫开发实战》","link":"/446.html"},{"title":"Python 编码规范","text":"软件行业现在基本上是协同工作，不再是以前个人单打独斗的年代了。在一个团队中，每个人可能负责一个或者多个模块。如果团队没有统一的编程规范，会增加代码理解难度，从而增加维护成本。所以遵循良好的编码风格，可以有效的提高代码的可读性，降低出错几率和维护难度。另外，使用（尽量）统一的编码风格，还可以降低沟通成本。 网上有许多的编码规范，我介绍分享几个知名编码规范给大家参考学习。 *PEP 8 *PEP 8 可以算是 Python 官方的标准编码规范。它是用于规范 Python 主发行版中的标准库的代码。所以这个编码规范是值得一看。 传送门：文档地址 Google 的 Python 风格指南总所周知，Google 是开源大户。Google 会将项目托管到 Github 上面，任何人都可以 fork、 修改、提交。如果代码贡献者的编程风格与 Google 的不一致, 会给代码阅读者和其他代码提交者造成不小的困扰。Google 因此发布了这份自己的编程风格指南, 使所有提交代码的人都能获知 Google 的编程风格。Google 的 Python 风格也是遵循 PEP8 规范。 传送门：文档地址 Pocoo 风格指南估计大家对 Pocoo 比较陌生，但大家一定对小型 Web 开发框架 Flask 很熟悉。没错，Flask 是 Pocoo 团队开发的项目。 除了 Flask 之外，Pocoo 团队还有开发出很多广受欢迎的项目，例如 Jinja2（模板引擎）、Pygments（语法高亮包）、Sphinx（文档处理器）、Werzeug（WSGI工具集）。Poco o团队编码风格指南适用于所有 Pocoo 团队的项目。总体来说，Pocoo 团队编码风格指南严格遵循了 PEP8 的要求，但略有一些不同之处，并进行了一定的扩展延伸。 传送门：文档地址 PyCharm 目前开发 Python 的主流 IDE 工具，我介绍下如何在 PyCharm 配置 PEP 8 代码提示、将代码格式化符合 PEP 8 规范。 配置 PEP 8 代码提示一般安装 PyCharm 都默认配置了规范提示。直接在右下角调整 Highlighting Level 为 Inspections 就能自动 PEP 8 提示。 在我之前 Django 学习笔记系列的第一个 demo 中，有不符合规范的地方。在代码编辑框的右边会有一个浅黄色的标记，你将鼠标悬停在光标上，PyCharm 会发现有提示。 对于这种提示，只要在第 7 行增加一个回车就搞定了，之后PyCharm 也没有提示。 当然，你也可以修改提示框的配色。 将代码格式化符合 PEP 8 规范这里我们需要使用到一个第三方库 Autopep8。Autopep8 是一个将 Python 代码自动排版为 PEP 8 风格的小工具。它使用 PEP 8 工具来决定代码中的哪部分需要被排版。Autopep8 可以修复大部分PEP 8 工具中报告的排版问题。 打开终端，使用 pip 命令来安装 Autopep8： 1pip install autopep8 autopep8 是一个命令行工具，所以我们在终端下对某个文件进行格式化。命令行如下： 1autopep8 --in-place --aggressive --aggressive &lt;filename&gt; Pycharm 配置 Autopep8 方法如下：1）选择菜单「File」–&gt;「Settings」–&gt;「Tools」–&gt;「External Tools」–&gt;点击加号添加工具 2）填写如下配置项，点击「OK」保存 图片中需要配置信息Name：Autopep8 (可随意填写)Tools settings: Programs：autopep8 Parameters：--in-place --aggressive --ignore=E123,E133,E50 $FilePath$ Working directory：$ProjectFileDir$ 3） 选择菜单「Tool」–&gt;「Extern Tools」–&gt;「Autopep8」或在某个文件中右键选择「Extern Tools」–&gt;「Autopep8」，即可使用autopep8自动格式化你的python代码了。 或","link":"/447.html"},{"title":"计划分享Python Web学习心得","text":"前段时间，有个读者留言跟我说，有空出使用 Python 实现 RESTful API 的教程。我一看，这正合我意。自己很早就想学习 Python web。之前有简单过了解些 Django 框架基础知识。但对于 Python Web，我还是研究不够深入。 因此，打算接下来一段时间。自己学习 Python Web，并将学习心得分享出来。自己在 Web 方面是只菜鸟，所以请老鸟轻喷。 回到刚才话题，RESTful API 是个什么东西呢？不妨我们先看下平常的网页是怎么回事。我们平时浏览的网站，一般分为前端和后端。我们用浏览器观看页面的内容就是前端的工作。前端采用 Html + CSS + JavaScript 技术来呈现页面内容以及页面效果。后端主要负责维护数据库并返回前端请求数据库的数据。如果我们有个需求，不需要那么华丽、炫酷的页面，只需要后端返回的数据。我们把这样的网络请求称为 RESTful API。再者，REST 描述的是在网络中 Client（PC 浏览器、手机 APP 等） 和 Server的一种交互形式；REST本身不实用，实用的是 RESTful API（REST 风格的网络接口）。 后端已经比较成熟的 Web 框架，我们没有必要重复造轮子。Python Web 主流框架有 Flask、Django、Tornado等 FlaskFlask 是一个使用 Python 编写的轻量级 Web 应用框架。它基于 Werkzeug WSGI 工具箱和 Jinja2 模板引擎。Flask 学习成本比较低，花很少的时间成本就能开发出一个简单的博客网站。如果你时间比较充裕，又想学习 Web 开发。可以学习 Flask ，再以 Flask 做跳板学习其他 Web 框架。 DjangoDjango 是以 Python 编写的高级，MVC 风格的开源库。 Django 也被称为“完美主义者的最后框架”。它最初是为新闻网站设计的，并且允许开发人员编写数据库驱动 Web 应用程序。它算是一个全能型框架。它内置了很多模块，能快速解决大量 Web 痛点问题。另外再加上云平台的支持，这使Django 成为 Web 开发者最受欢迎的选择。大名鼎鼎的 Instagram 网站就是基于 Django 开发的。 TornadoTornado 是传说中性能高高的框架。它支持异步处理的功能，这是它的优势。因为其他框架不具备该功能。但 Tornado 也有致命缺点，那就是扩展库资源比较少。Tornado 除了提供了网站基本需要使用的模块外，剩下的则需要开发者自己进行扩展。 所以，综合以上几点，我就决定深入学习 Django。朋友们，敬请期待我的分享吧。","link":"/134.html"},{"title":"Django 学习笔记之使用旧数据库","text":"如果你按照顺序，从第一篇文章读到本文。恭喜你，你已经将 Django 大部分基础知识掌握了。后续的文章是在之前的基础上添砖加瓦或常用的应用。本文将的内容是一个场景应用，新项目使用旧数据库。 可能以前项目是使用其他语言，如 Java 或 PHP 开发的，后面迁移到 Python 上。虽然应用程序改变了，但是数据缺不是丢弃。因此，存在这样的问题。那就是使用 Django 开发的 Web 应用程序如何使用旧的数据库？ 我就使用旧的 SqLite 数据库作为例子进行讲解，MySQL 等其他数据库也是操作类似。 导入数据库旧的数据库名为 MyDataBase.db, 我将其导入到新项目的 db 目录。 然后将 settings.py 文件中的数据库名称修改下。 12345678DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', # 系统自动生成 # 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), 'NAME': os.path.join(BASE_DIR, './db/MyDataBase.db'), }} 创建模型我们知道 Models 层是跟数据库打交道的层次，需要创建数据库表对应的 models。但对应使用旧数据库，我们不需要手动创建，可以使用 Django 提供的 API 反向生成 models。 假设数据有一张表，其创建表的 SQL 语句如下： 123456789101112Create table if not exists `Users` ( `id` INTEGER PRIMARY KEY, `username` varchar(64), `password` varchar(64), `email` varchar(64) , `QQ` varchar(15) , `weibo` varchar(300), `cell_phone` varchar(20), `college` varchar(60), `living_place` varchar(200), `remarks` varchar(400) ); 我们进入到项目的目录下，打开终端，执行以下命令： 1python manage.py inspectdb 目的是针对已有数据库自省生成新的 models。 然后执行导出命令，将模型导出到 models.py 文件中。 1python manage.py inspectdb &gt; models.py 你会看到项目根目录下多了一个名为 models.py 文件。 将其内容迁移到 app 的 models.py 中。默认配置下生成不可修改或删除的 models，所以我们修改 meta class 中的 managed 属性。如果 managed 被设置为 True，则告诉 Django 可以对数据库进行操作。 最后一步，同步 model 的改动到数据库中。在项目目录下，使用终端执行以下命令。 1python manage.py migrate 如果没有报错的话，证明成功导入。有时候可能会报出以下的错误： 原因是表中定义了 id 字段，同时这个字段被设定为主键。具体的解决方式是：修改 model.py 中 id 字段的定义; 将其中的 null=True 修改为 primary_key=True。 12345# 修改前id = models.IntegerField(blank=True, null=True)# 修改后id = models.IntegerField(blank=True, primary_key=True) 保存修改之后，重新执行同步数据库命令即可。","link":"/443.html"},{"title":"想做 Python Web 开发，需要掌握哪些技能？","text":"在 Web 开发领域，Java 凭借企业级支持以及世界丰富的生态环境成为绝对霸主，PHP 紧随其后。有些公司考虑效率问题而采用 C++ 做后台开发语言， 也有人使用 Node.js 开发后台。 另外 Python 也是能够做后台开发的。Python 具有语言简洁、开发效率高等特点。还有成熟且不断更新的开源框架，例如 Flask、Django、Tornado等。所以很多大公司都使用其开发后台。比如人人皆知的 YouTube、Instagram、Reddit、Quora、知乎、豆瓣、果壳等。 因此，市场有 Python Web 开发的岗位需求。那么我想往这方面发展，我需要掌握哪些技能？ Python 语言想使用 Python 做后台开发，第一步就是就是学习 Python 语言。首先要将学习 Python 基本语法，再学习 Python 高级用法（例如闭包，面向对象等），接着再了解各个标准库的用法，最后熟悉下 PEP8 编码规范。 开发框架大部分后台业务逻辑都会使用 Web 框架来开发，目的是提高开发效率。常用的 Python web框架有 Django、Flask、Tornado 等。个人推荐熟练掌握 Django，因为 Django 是一个全能型框架。另外需要了解 REST，学习如何编写 RESTful APIs。 数据库现在网站业务后端用得比较多的有三种类型的数据库，关系型数据库（mysql等），文档型数据库（mongodb等），和内存型数据库（redis等）。因为三种数据库各有优势和其使用场景，所以需要了解下不同类型数据库的使用方法和应用场景，灵活应用到后端代码中。所以要学习如何使用、设计、优化数据库。 前端知识需要了解基本的 HTML、CSS、JavaScript。通常前后端开发是分离的，了解前端知识是有帮助的。能知道自己需要将传递哪些内容给前端，从而提供团队合作效率。如果对前端知识感兴趣，可以了解下 Bootstrap、Vue 等。 Web 服务器Nginx 目前很流行，使用也是很广泛。因为其占用内存少，稳定性高、并发能力强。所以需要掌握 Web 应用部署以及如何使用 Nginx 实现负载均衡。 Linux 操作系统Nginx 通常运行在 Linux 服务器上，所以需要学习 Linux 系统。了解一些常见的 Linux 命令、文件与目录管理、账号与身份管理、程序与资源管理等。推荐阅读 《鸟哥的Linux私房菜基础学习篇》，这本书爽是最具知名度的 Linux 入门书全面。它能详细地介绍了 Linux 操作系统。 计算机网络后台开发经常要跟网络打交道，所以熟悉对网络协议 TCP/IP 和 HTTP。学习 TCP/IP 可以阅读《TCP/IP详解卷1：协议》，学习 HTTP 可以阅读《图解Http》和《Http权威指南》 算法与数据结构我记得读大学时使用的教程是严蔚敏的《数据结构》（C语言版）。其中有一句很经典的话：『程序 = 算法 + 数据结构』。所以需要了解常用的算法和数据结构。推荐阅读《算法图解》、《枕边算法书》 以上信息是自己抓取大量数据，然后分析总结出来的，希望对你有所帮助。","link":"/448.html"},{"title":"Django 实现分页功能","text":"当页面因需要展示的数据条目过多，导致无法在一个页面全部显示。这时，页面经常会采用分页形式进行展示，然后每页显示 20 或者 50 等条数据。分页经常在网站上随处可见， 它大概是这样子： 这样的实现不仅提高了用户体验，还是减轻数据库读取数据的压力。Django 自带名为 Paginator 的分页工具， 方便我们实现分页功能。本文就讲解如何使用 Paginator 实现分页功能。 PaginatorPaginator 类的作用是将我们需要分页的数据分割成若干份。当我们实现化一个 Paginator 类的实例时，需要给 Paginator 传入两个参数。第一个参数是数据源，可以是一个列表、元组、以及查询结果集 QuerySet。第二个参数需要传入一个整数，表示每页显示数据条数。具体写法如下： 12345book_list = []for x in range(1, 26): # 一共 25 本书 book_list.append('Book ' + str(x))# 将数据按照规定每页显示 10 条, 进行分割paginator = Paginator(book_list, 10) 上面代码中，我们传入一个名为 book_list 的列表，该列表中含有 25 本书，然后我们给 Paginator 设定每页显示 10 条数据，最后得到一个 Paginator 实例。 另外 Paginator 类中有三个常用的属性，它们分别是： count：表示所有页面的对象总数。 num_pages： 表示页面总数。 page_range： 下标从 1 开始的页数范围迭代器。 Page 对象Paginator 类提供一个** page(number) **函数，该函数返回就是一个 Page 对象。参数 number 表示第几个分页。如果 number = 1，那么 page() 返回的对象是第一分页的 Page 对象。在前端页面中显示数据，我们主要的操作都是基于 Page 对象。具体用法如下： 12# 使用 paginator 对象返回第 1 页的 page 对象books = paginator.page(1) Page 对象有三个常用的属性： object_list: 表示当前页面上所有对象的列表。 numberv: 表示当前页的序号，从 1 开始计数。 paginator： 当前 Page 对象所属的 Paginator 对象。 除此之外，Page 对象还拥有几个常用的函数： has_next()： 判断是否还有下一页，有的话返回True。 has_previous()：判断是否还有上一页，有的话返回 True。 has_other_pages()：判断是否上一页或下一页，有的话返回True。 next_page_number()： 返回下一页的页码。如果下一页不存在，抛出InvalidPage 异常。 previous_page_number()：返回上一页的页码。如果上一页不存在，抛出InvalidPage 异常。 运用下面是自己编写的 demo 程序，介绍 Paginator 和 Page 如何一起使用。 视图在 views.py 获取需要展示的全部数据，然后使用 Paginator 类对数据进行分页，最后返回第 1 页面的 page 对象。page 对象的作用巨大，一方面展示当前分页数据，还提供获取后续页面数据的接口。 123456789101112131415161718192021222324252627282930313233from django.core.paginator import Paginator, PageNotAnInteger, EmptyPage, InvalidPagefrom django.http import HttpResponsefrom django.shortcuts import renderdef paginator_view(request): book_list = [] ''' 数据通常是从 models 中获取。这里为了方便，直接使用生成器来获取数据。 ''' for x in range(1, 26): # 一共 25 本书 book_list.append('Book ' + str(x)) # 将数据按照规定每页显示 10 条, 进行分割 paginator = Paginator(book_list, 10) if request.method == \"GET\": # 获取 url 后面的 page 参数的值, 首页不显示 page 参数, 默认值是 1 page = request.GET.get('page') try: books = paginator.page(page) # todo: 注意捕获异常 except PageNotAnInteger: # 如果请求的页数不是整数, 返回第一页。 books = paginator.page(1) except InvalidPage: # 如果请求的页数不存在, 重定向页面 return HttpResponse('找不到页面的内容') except EmptyPage: # 如果请求的页数不在合法的页数范围内，返回结果的最后一页。 books = paginator.page(paginator.num_pages) template_view = 'page.html' return render(request, template_view, {'books': books}) 模板模板的工作就是在 HTML 页面中填充数据。当拿到视图传递过来的 books（books 是一个 Page 对象）， 就在 for 循环中打印数据。最后使用 books 根据页面情况展示上一页按钮，当前页数，总页数，下一页按钮。 12345678910111213141516171819202122232425262728293031323334353637{% load staticfiles %}&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns=\"http://www.w3.org/1999/html\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;link href=\"{% static 'css/bootstrap.min.css' %}\" rel=\"stylesheet\"&gt; &lt;script src=\"{% static 'js/jquery.min.js' %}\"&gt;&lt;/script&gt; &lt;script src=\"{% static 'js/bootstrap.min.js' %}\"&gt;&lt;/script&gt; &lt;title&gt;分页&lt;/title&gt;&lt;/head&gt;&lt;br&gt; &lt;div class=\"text-center\" &gt; {% for book in books %} &lt;span&gt;书名： {{ book }} &lt;br /&gt;&lt;/span&gt; {% endfor %} &lt;/div&gt; {# 实现分页标签的代码 #} {# 这里使用 bootstrap 渲染页面 #} &lt;div id=\"pages\" class=\"text-center\" &gt; &lt;nav&gt; &lt;ul class=\"pagination\"&gt; &lt;li class=\"step-links\"&gt; {% if books.has_previous %} &lt;a class='active' href=\"?page={{ books.previous_page_number }}\"&gt;上一页&lt;/a&gt; {% endif %} &lt;span class=\"current\"&gt; Page {{ books.number }} of {{ books.paginator.num_pages }}&lt;/span&gt; {% if books.has_next %} &lt;a class='active' href=\"?page={{ books.next_page_number }}\"&gt;下一页&lt;/a&gt; {% endif %} &lt;/li&gt;&lt;/ul&gt;&lt;/nav&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 运行结果我在 urls.py 中配置当前的 app 访问路径是 paginator/。所以使用浏览器访问地址 http://127.0.0.1:8000/paginator/， 看到访问结果如下： 如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/551.html"},{"title":"Django 使用会话(sessions)功能","text":"淘宝、天猫、京东等电商网站的出现，让我们足不出户就能购物。在这些网站中，都有一个“购物车”的功能。当我们在不同商品页面将商品加入购物车，然后关闭浏览器。等下次浏览该网站时，我们会依然发现购物车的商品还在。这是怎么实现的了？类似这种场景，一般都是采用 Cookie + Session 方式来实现。 Cookie 机制HTTP 协议是无状态的。所以服务器无法根据 HTTP 协议来辨别多个 HTTP 请求来自哪个用户。在实际场景中，服务器经常需要追踪客户端的状态。为了解决这个问题， Cookie 技术应运而生。 cookie 一开始是服务器产生的一段随机字符串。它的主要内容包括：名字，值，过期时间，路径与域等信息。然后服务器将其发送给客户端。在后续的请求中，cookie 会附在请求资源的 HTTP 请求头上，发送给服务器。 Session 机制如果不涉及用户登录等敏感信息时，Cookie 能够满足大部分的场景需求。而客户端（如浏览器）会将 Cookie 是保存在硬盘中。如果用户登录敏感信息保存到 cookie 中，会存在安全性问题。因为当 HTTP 请求被黑客拦截，然后劫持 cookie 信息。黑客就可以凭借该 cookie 登录对应的网站。 Session 的出现很好地解决的这个问题。Session 机制是一个服务器端的机制。它会将信息保存服务器端，跟客户端通信只需要一个随机的字符串 session_id。如果客户端没有禁止 Cookie 功能，session_id 通常是保存在 Cookie中 的。如果 Cookie 被禁用，它则可能通过为 url 加上query string 来添加 session_id。 在 Django 中的应用Django 为我们提供了一个通用的 Session 框架。使用 Django 2.X 版本创建新项目的时，Django 默认会帮我们启用该功能。 Django 默认 Session 数据保存到数据库中，可以在 settings.py 中看到配置信息项目。 123456789INSTALLED_APPS = [ # 启用 sessions 应用 'django.contrib.sessions',]MIDDLEWARE = [ # 启用 Session 中间层 'django.contrib.sessions.middleware.SessionMiddleware',] 然后我们在将自带组件的模型同步到数据库中。如果你还不熟悉这块内容，可以阅读《Django 学习笔记之模型（上）》这篇文章。我们之后会看到数据库中有个 django_session 表： 除了上述的基于数据库的会话，Django 还提供另外三种方法：1）保存到缓存中如果你的场景需要快速存储会话，可以选择该方案。使用之前，需要配置下 Django 的缓存框架。在 settings.py 中增加 SESSION_ENGINE 配置。 这其中也是有两种保存数据的方案，具体配置如下： 方案一 1SESSION_ENGINE = 'django.contrib.sessions.backends.cache' 这种配置方案 Django 只是简单保存会话。同时，这种方案持久性不好。因为当缓存数据存满时将清除部分数据，或者遇到缓存服务器重启时数据将丢失。 方案二 1SESSION_ENGINE = 'django.contrib.sessions.backends.cached_db' 这种方案既保证快速存储会话数据，又保证数据持久性。因为该使用方案， Session 在保存到缓存的同时还会被保存到数据库中，当 Django 在缓存中找不到Session 时，会从数据库中找到。因此，这种方案的性能开销会比方案一大。 如果我们在工程中同时配置了数据库会话和缓存会话，Django 默认优秀选择缓存会话。 2）保存到文件中 这种方案是保存数据到本地磁盘中。因为磁盘的 I/O 瓶颈问题，导致这种方案存储数据效率不是很高。如果要使用这种方案，在 settings.py 中增加 SESSION_ENGINE 配置。具体配置如下： 123SESSION_ENGINE = 'django.contrib.sessions.backends.file'# 可选配置SESSION_FILE_PATH = '/monkey/www/' SESSION_FILE_PATH 默认使用 tempfile.gettempdir() 方法的返回值，就像 /tmp目录。如果你想更新文件的保存路径，可以手动指定。另外需确保你的文件存储目录，以及 Web 服务器对该目录具有读写权限。 3）保存到 cookie 中 这种方案将数据保存到 cookie 中。这种方案适用于对数据保密性不严格的场景。如果要使用这种方案，在 settings.py 中增加 SESSION_ENGINE 配置。具体配置如下： 123SESSION_ENGINE = 'jango.contrib.sessions.backends.signed_cookies'# 建议配置，阻止 javascript 对会话数据的访问，提高安全性。SESSION_COOKIE_HTTPONLY= True","link":"/552.html"},{"title":"Django 实战2：利用 Session 实现自动登录","text":"上篇文章中讲到 Django 如何启动以及配置 sessions 功能。sessions 功能用是跟踪用户的状态，经常结合 Cookie 功能实现自动登录功能。 所谓的“自动登录”指的是：我们登录一些网站，在不关闭浏览器以及距离上次登录时间不是很长的情况下。无论我们在新的标签页打开网站，还是关闭页面重新打开网站，登录状态一直保持着。本文内容有两个：一是利用 Django 实现自动登录功能，二是揭开“自动登录”的神秘面纱。 新建项目我为了将本系列所有文章的示例代码保持集中状态，所以直接在 Django_demo 项目中创建应用。如果第一次看这文章，需要先创建项目(project)，再创建应用(app)。我新建的应用是 demo_session。 然后在 setting.py 中启动请用，并检查 sessions 组件是否启动。 因为需要 Cookie 功能，所以同样需要在 settings.py 增加一些配置。 12345678SESSION_COOKIE_NAME = \"sessionid\" # Session的cookie保存在浏览器上时的keySESSION_COOKIE_PATH = \"/\" # Session的cookie保存的路径(默认)SESSION_COOKIE_DOMAIN = None # Session的cookie保存的域名(默认)SESSION_COOKIE_SECURE = False # 是否Https传输cookieSESSION_COOKIE_HTTPONLY = True # 是否Session的cookie只支持http传输(默认)SESSION_COOKIE_AGE = 1209600 # Session的cookie失效日期(2周)(默认)SESSION_SAVE_EVERY_REQUEST = False # 是否设置关闭浏览器使得Session过期SESSION_COOKIE_AT_BROWSER_CLOSE = False # 是否每次请求都保存Session，默认修改之后才能保存 如果你将 SESSION_SAVE_EVERY_REQUEST 设置为 True， 那么关闭浏览器之后，需要重新登录。 流程应用中会涉及到 3 个页面，所以我绘制流程图帮助理解。 实现新建 model服务器接收到浏览器传送过来登录信息，需要验证账号和密码等信息。所以需要新建 model 保存信息，以便后续跟数据库做校验。这里我只是简单保存信息，登录验证后续讲解。 1234class User(models.Model): username = models.CharField(max_length=20) # 账号 password = models.CharField(max_length=20) # 密码 nickname = models.CharField(max_length=20) # 昵称 新建 form用户将登录信息发送给服务器是用到 POST 请求，所以需要创建表单。在应用目录下新建名为 forms 目录，然后创建 forms.py 文件。 12345678910111213from django.forms import ModelForm, TextInput, PasswordInputfrom demo_session.models import Userclass UserForm(ModelForm): class Meta: model = User fields = ['username', 'password', ] # 只显示 model 中指定的字段 # 指定呈现样式字段、指定 CSS 样式 widgets = { 'username': TextInput(attrs={'class': 'text', 'value': 'monkey'}), 'password': PasswordInput(attrs={'value': '13245678', }) } 新建视图页面一共有三个，分别是登录、首页、登出。具体实现如下： 1234567891011121314151617181920212223242526272829303132333435363738# view.pyfrom django.http import HttpResponseRedirect, HttpResponsefrom django.shortcuts import renderfrom demo_session.form.forms import UserForm# 用户登录def login_view(request): # 过滤 POST 方法的请求 if request.method == 'POST': userfrom = UserForm(request.POST) # 验证表单 if userfrom.is_valid(): username = userfrom.cleaned_data['username'] password = userfrom.cleaned_data['password'] # ... 执行验证登录信息操作 # 将等你信息传递给 Session 对象, 实际应用中不建议这么操作 request.session['username'] = username # 跳转到页面 return HttpResponseRedirect('/index/') else: # 不是 GET 请求则显示表单 userfrom = UserForm() template_view = 'login.html' return render(request, template_view, {'userfrom': userfrom})# 成功登录之后, 跳转首页def index_view(request): username = request.session.get('username', '') # print(username) template_view = 'index.html' return render(request, template_view, {'username': username})# 登出操作def logout_view(request): # 删除 session del request.session['username'] return HttpResponse('登出成功') 对应模板视图 login_view, index_view 对应的模板是 login.html, index.html。其中 login.html 的实现如下： 1234567891011121314151617181920212223242526272829303132{% load staticfiles %}&lt;!DOCTYPE html&gt;&lt;html lang=\"{{ LANGUAGE_CODE|default:\"en-us\" }}\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;登录&lt;/title&gt; &lt;link href=\"{% static 'css/style.css' %}\" rel=\"stylesheet\" type='text/css' &gt; &lt;!--webfonts--&gt; &lt;link href=\"{% static 'css/font.css' %}\" rel='stylesheet' type='text/css'&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt; &lt;script type=\"application/x-javascript\"&gt; addEventListener(\"load\", function() { setTimeout(hideURLbar, 0); }, false); function hideURLbar(){ window.scrollTo(0,1); }&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div class=\"main\"&gt; &lt;div class=\"login-form\"&gt; &lt;h1&gt;Member Login&lt;/h1&gt; &lt;div class=\"head\"&gt; &lt;img src=\"{% static 'images/user.png' %}\" alt=\"\"/&gt; &lt;/div&gt; &lt;form action=\"\" method=\"POST\"&gt; {% csrf_token %} {{ userfrom.username }} {{ userfrom.password }} &lt;div class=\"submit\"&gt;&lt;input type=\"submit\" value=\"login\" &gt;&lt;/div&gt; &lt;p&gt;&lt;a href=\"#\"&gt;Forgot Password ?&lt;/a&gt;&lt;/p&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; index.html 的实现如下： 12345678910111213&lt;!DOCTYPE html&gt;&lt;html lang=\"{{ LANGUAGE_CODE|default:\"en-us\" }}\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;首页&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div &gt; &lt;h3&gt;欢迎 {{username}} 到来~&lt;/h3&gt; &lt;div class=\"submit\"&gt;&lt;a href=\"/logout\"&gt;&lt;input type=\"submit\" value=\"退出登录\" &gt;&lt;/a&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 配置路由最后一步，在 urls.py 中配置访问路径： 12345678from demo_session.views import login_view, index_view, logout_viewurlpatterns = [ # demo_session path('login/', login_view, name='login'), path('index/', index_view, name='index'), path('logout/', logout_view, name='index'),] 实现效果用户访问 http://127.0.0.1/login 进行登录操作。 当点击 login 成功之后，会跳转到首页，首页会显示用户名。同时，Cookie 中多了一个 sessionid 的字段。这字段名就是我们在 setttings.py 定义的。 查询数据库 django_session 表的内容，会多出一条数据。 表中的字段含义如下： session_key: 就是服务器给用户返回的id。在浏览器当中，这个值是保存为sessionid session_data: 这是一个加密后的信息，用来保存用户名和密码等信息 expire_data: 过期时间，Django可以设置过期时间 在新的标签页中打开首页，依然能看到 username 信息。这证明能自动登录。 如果用户退出登录，再访问首页。这时会发现看不到了 username 信息了。 小结实现自动登录功能其实不难，只需要在 Django 的 Sessions 组件。然后根据场景需要，在 settings.py 配置 session 以及 cookie 等信息。 如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/553.html"},{"title":"Django 学习笔记之模板","text":"本文是自己 Django 学习笔记系列的第四篇原创文章。主要接着篇文章的视图内容，讲解模板的用法。另外也说下 Django 学习笔记系列的安排。自己计划大概 15 篇文章的输出自己学习 Django 框架的内容，再用大概 10 篇文章进行实战开发，最后可能用少量的篇幅进行补充。废话不多说，切回主题。 模板是什么 通过之前文章，我们学会使用 render(request, 'content.html') 方法来返回静态页面。但在一些页面中，页面需要根据不同场景（例如时间，角色）显示不同的数据。这就需要使用到模板（Template）。模板通常是 HTML 文件，只不过其中带有特定的语句。这些语句是用来存储并显示数据库中返回的数据。另外，除了 HTML 文件外，Django的模板也能产生任何基于文本格式的文档。 我们就以一个简单的例子来开始学习模板。该模板是一段添加了些变量和模板标签的 html 文件。如果你暂时看不懂其中的内容，没有关系，下面会逐步说明。 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;模板（Template）&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p&gt;Dear {{ person_name }},&lt;/p&gt; &lt;p&gt;It is scheduled to ship on {{ ship_date|date:\"F j, Y\" }}.&lt;/p&gt; &lt;ul&gt; {% for item in item_list %} &lt;li&gt;{{ item }}&lt;/li&gt; {% endfor %} &lt;/ul&gt; {% if ordered_warranty %} &lt;p&gt;Your warranty information will be included in the packaging&lt;/p&gt; {% else %} &lt;p&gt;You did not order a warranty.&lt;/p&gt; {% endif %} &lt;p&gt;Sincerely,&lt;br /&gt;{{ company }}&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 根据上面的代码，让我们逐步分析一下其中的内容： 用两个大括号括起来的文字（例如 ）和 称为 变量 。这里是插入一个变量的值。赋值是在 视图 （views.py）中操作的。 被大括号和百分号包围的文本(例如 {百分号 if ordered_warranty 百分号} )是 模板标签(template tag) 。标签的作用是通知模板系统执行某些操作。 {百分号 for item in item_list 百分号} 是 模板的 for 标签。有点类似 Python 中的 for 语句，能够让你循环遍历序列中的内容。 {百分号 if ordered_warranty 百分号} 则是 if 标签，用于执行逻辑判断。 中用到了 filter 过滤器。这是一种最方便的转换变量输出格式的方式。它的用法跟 Linux 的管道一样，使用管道符 “ | ” 。在这里，我们将变量 ship_date 传递给 date 过滤器，同时指定参数 F j,Y 常用标签从上面的例子中可知，模板中有些常用的标签。让我们来深入了解各个标签的用法。 if/else 标签{百分号 if 百分号} 标签检查一个变量，如果这个变量为真（即，变量存在，非空，不是布尔值假），系统会显示在 {百分号 if 百分号} 和 {百分号 endif 百分号} 之间的任何内容。在每一个 {百分号 if 百分号} 标签后面， 一定要用 {百分号 endif 百分号} 标签来关闭。如： 123{% if is_rain %} &lt;p&gt;外面正在下雨&lt;/p&gt;{% endif %} 如果需要 else 操作， 使用 {百分号 else 百分号} 标签。 12345{% if is_rain %} &lt;p&gt;外面正在下雨。&lt;/p&gt;{% else %} &lt;p&gt;今天是阴天。&lt;/p&gt;{% endif %} {百分号 if 百分号} 标签用法跟 Python 中的 if 语法有些差别。因此需要重点关注下。它不支持用圆括号来组合操作，但支持接受 and ， or 或者 not 关键字来对多个变量做判断。如： 1234567891011{% if sunny and hot %} ...{% endif %}{% if sunny or rainy %} ...{% endif %}{% if not rainy %} ...{% endif %} 另外{百分号 if 百分号} 并没有 {百分号 elif 百分号} 标签， 请使用嵌套的{百分号 if 百分号} 标签来达成同样的效果。 for 标签{百分号 for 百分号} 标签允许我们遍历一个序列上的每一项。在每一次循环中，模板系统会渲染在 {百分号 for 百分号} 和 {百分号 endfor 百分号} 之间的所有内容。 例如，给定一个 图片列表 image_list，我们可以使用下面的代码来显示这个列表： 12345&lt;ul&gt;{% for img in image_list %} &lt;li&gt;{{ img.url }}&lt;/li&gt;{% endfor %}&lt;/ul&gt; 如果你想将列表反方向输出，可以使用 reversed 关键字。 12345&lt;ul&gt;{% for img in image_list reversed %} &lt;li&gt;{{ img.url }}&lt;/li&gt;{% endfor %}&lt;/ul&gt; 在执行循环之前，通常会先检测列表的大小。模板提供了一个标签 {百分号 empty 百分号} 来输出列表为空的提示。 123456&lt;ul&gt;{% for img in image_list %} &lt;li&gt;{{ img.url }}&lt;/li&gt;{% empty %} &lt;p&gt;图片列表不能为空.&lt;/p&gt;&lt;/ul&gt; 除此之外，{百分号 for 百分号} 标签还支持嵌套使用。 1234567{% for page in page_list %}&lt;ul&gt;{% for img in image_list %} &lt;li&gt;{{ img.url }}&lt;/li&gt;{% endfor %}&lt;/ul&gt;{% endfor %} 跟 {百分号 if 百分号} 标签类似，{百分号 for 百分号} 标签的用法不能等同 Python 中的 for 语句。它不支持退出循环操作，即 break 语句；同样，它也不支持 continue 语句。 在每个 {百分号 for 百分号}循环中有一个被称为 ** forloop ** 的模板变量。这变量提供一些带有循环进度信息的属性。 forloop.counter 表示当前循环的执行次数的总数。这个计数器是从 1 开始记录，所以在第一次循环操作是，forloop.counter 会被设置为 1。 forloop.counter0 类似于 forloop.counter ，但是它是从0计数的。 第一次执行循环时这个变量会被设置为0。 forloop.revcounter 是记录循环中还没有被遍历项的总数。循环初次执行时 forloop.revcounter 将被设置为序列的长度。 最后一次循环执行中，这个变量将被置1。 forloop.revcounter0 类似于 forloop.revcounter ，但它以0做为结束索引。因此，第一次循环执行的时候，该变量的值为 序列的长度减 1。 forloop.first 是一个布尔值。如果你需要在第一次循环时，执行一些操作。可以利用该属性。 forloop.last也是布尔类型。用法跟 forloop.first 类似。它的运行场景是最后一个循环。 ifequal 标签比较两个变量的值是在是太常见了，所以 Django 模板提供了 {百分号 ifequal 百分号} 标签提供我们使用。 {百分号 ifequal 百分号} 标签比较两个值，当它们相等时，显示在 {百分号 ifequal 百分号} 和 {百分号 endifequal 百分号} 之中所有的值。如下面的例子是比较两个模板变量 name 和 currentname： 12345{% ifequal name curentname %}&lt;p&gt;名字是一样。&lt;/p&gt;{% else %}&lt;p&gt;名字是不一样。&lt;/p&gt;{% endifequal %} 除了判断两个变量的值，该标签还支持字符串，整数和小数做为参数，但是不支持 Python 的列表类型、布尔类型和字典类型。 模板还有一个比较不相等的 ifnotequal 标签，它的用法跟 ifequal 标签类似。 注释标签如果是需要对单行进行注释操作，使用 # # 标签： 1{# 单行注释 #} 如果要实现多行注释，需用到 {百分号 comment 百分号} 模板标签，就像这样： 123{% comment %}多行注释{% endcomment %} 上下文(context)对象context 对象视图和模板文件的承接桥梁。 context 对象携带视图中需要填充的数据，然后在模版渲染的时候，将数据赋值给模板的变量。模板进而可以渲染显示。 让我们通过下面的例子来了解 context 的用法。在 views.py 中，我们创建一个 current_time 视图，然后用 Django 模板系统修改视图。其内容如下： 12345678from django.template import Template, Contextfrom django.http import HttpResponsedef current_time(request): now = '2018-2-27 20:01:56' t = Template(\"&lt;html&gt;&lt;body&gt;It is now {{ time }}.&lt;/body&gt;&lt;/html&gt;\") html = t.render(Context({'time': now})) return HttpResponse(html) context 和 Python 字典很类似，它以键值对的形式传递参数。context 不仅能传递字符穿和 datetime.date 这样的简单参数值，还能处理更加复杂的数据结构，例如列表、字典和类的对象。模板遍历复制数据结构是用到句点符号(.)。 下面是向模板传递一个 Python 字典的例子。 12345678from django.template import Template, Contextfrom django.http import HttpResponsedef case_dir(request): person = {'name': '极客猴', 'age': '18'} t = Template('{{ person.name }} is {{ person.age }} years old.') html = t.render(Context({'person': person})) return HttpResponse(html) 向模板传递一个类的对象的列子： 12345678910111213# 在其它目录有一个实体类class Person(object):def __init__(self, name, age):self.name, self.age = name, age# 在 views.py 文件中from django.template import Template, Contextfrom django.http import HttpResponsedef case_class(request): t = Template('{{ person.name }} is {{ person.age }} years old.') html = t.render(Context({'person': Person('极客猴','18')})) return HttpResponse(html) 默认情况下，如果一个变量不存在，模板系统会把它展示为空字符串，不会引发一个异常。 加载模板Django 提供模板功能目的是为了让视图和前端页面内容隔开来。同时，前端设计师可能对 HTML 编码比较熟悉，但完全不懂 Python。Python 工程是不一定都熟悉前端的知识。因此，不提倡直接在 视图中混入模板内容。 views.py 中的视图函数只负责加载模板文件，模板一般存放到 templates 文件夹中。 Django 提供了一种使用方便且功能强大的 API，用于从本地中加载模板。当你新建一个新的 Django 项目时，在 setting.py 配置文件中有个 TEMPLATES 选项。TEMPLATES 的 DIRS 属性是记录存放模板文件的绝对路径。 123456789101112131415TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },] 我们无需手动去配置 DIRS，当我们创建目录时，Django 会自动帮我们填充好路径。如果你在 application 目录中创建名为 templates 目录，你会发现 setting.py 中的 TEMPLATES 选项发生变化。 12345678910111213141516TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates')] , 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },] 了解 Django 的机制之后，我们就可以加载模板文件了。因为 Django 已经帮我们搞定模板文件搜索工作。加载模板，我们使用函数 django.template.loader.get_template()，而不是手动从文件系统中加载。调用 get_template() 函数，需要传入模板文件名称的参数。get_template() 函数帮我们实现了打开模板文件，关闭模板文件，异常处理等工作。这大大减轻了我们重复的工作量。 123456789from django.template.loader import get_templatefrom django.template import Contextfrom django.http import HttpResponsedef current_time(request): now = '2018-2-27 20:01:56' t = get_template('current_time.html') html = t.render(Context({'time': now})) return HttpResponse(html)","link":"/238.html"},{"title":"我爬取豆瓣影评，告诉你《复仇者联盟3》在讲什么？","text":"我爬取豆瓣影评，告诉你《复仇者联盟3》在讲什么？ 《复仇者联盟3：无限战争》于 2018 年 5 月 11 日在中国大陆上映。截止 5 月 16 日，它累计票房达到 15.25 亿。这票房纪录已经超过了漫威系列单部电影的票房纪录。不得不说，漫威电影已经成为一种文化潮流。 复联 3 作为漫威 10 年一剑的收官之作。漫威确认下了很多功夫， 给我们奉献一部精彩绝伦的电影。自己也利用周末时间去电影院观看。看完之后，个人觉得无论在打斗特效方面还是故事情节，都是给人愉悦的享受。同时，电影还保持以往幽默搞笑的风格，经常能把观众逗得捧腹大笑。 如果还没有去观看的朋友，可以去电影院看看，确实值得一看。 本文通过 Python 制作网络爬虫，爬取豆瓣电影评论，并分析然后制作豆瓣影评的云图。 分析先通过影评网页确定爬取的内容。我要爬取的是用户名，是否看过，五星评论值，评论时间，有用数以及评论内容。 然后确定每页评论的 url 结构。第二页 url 地址： 第三页 url 地址： 最后发现其中的规律：除了首页，后面的每页 url 地址中只有 start= 的值逐页递增，其他都是不变的。 数据爬取本文爬取数据，采用的主要是 requests 库和 lxml 库中 Xpath。豆瓣网站虽然对网络爬虫算是很友好，但是还是有反爬虫机制。如果你没有设置延迟，一下子发起大量请求，会被封 IP 的。另外，如果没有登录豆瓣，只能访问前 10 页的影片。因此，发起爬取数据的 HTTP 请求要带上自己账号的 cookie。搞到 cookie 也不是难事，可以通过浏览器登录豆瓣，然后在开发者模式中获取。 我想从影评首页开始爬取，爬取入口是：https://movie.douban.com/subject/24773958/comments?status=P，然后依次获取页面中下一页的 url 地址以及需要爬取的内容，接着继续访问下一个页面的地址。 12345678910111213141516171819202122232425262728293031323334353637383940import jiebaimport requestsimport pandas as pdimport timeimport randomfrom lxml import etreedef start_spider(): base_url = 'https://movie.douban.com/subject/24773958/comments' start_url = base_url + '?start=0' number = 1 html = request_get(start_url) while html.status_code == 200: # 获取下一页的 url selector = etree.HTML(html.text) nextpage = selector.xpath(\"//div[@id='paginator']/a[@class='next']/@href\") nextpage = nextpage[0] next_url = base_url + nextpage # 获取评论 comments = selector.xpath(\"//div[@class='comment']\") marvelthree = [] for each in comments: marvelthree.append(get_comments(each)) data = pd.DataFrame(marvelthree) # 写入csv文件,'a+'是追加模式 try: if number == 1: csv_headers = ['用户', '是否看过', '五星评分', '评论时间', '有用数', '评论内容'] data.to_csv('./Marvel3_yingpping.csv', header=csv_headers, index=False, mode='a+', encoding='utf-8') else: data.to_csv('./Marvel3_yingpping.csv', header=False, index=False, mode='a+', encoding='utf-8') except UnicodeEncodeError: print(\"编码错误, 该数据无法写到文件中, 直接忽略该数据\") data = [] html = request_get(next_url) 我在请求头中增加随机变化的 User-agent, 增加 cookie。最后增加请求的随机等待时间，防止请求过猛被封 IP。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def request_get(url): ''' 使用 Session 能够跨请求保持某些参数。 它也会在同一个 Session 实例发出的所有请求之间保持 cookie ''' timeout = 3 UserAgent_List = [ \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.4; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2225.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2224.3 Safari/537.36\", \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\", \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 4.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36\", \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.67 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.3319.102 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2309.372 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2117.157 Safari/537.36\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36\", \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1866.237 Safari/537.36\", ] header = { 'User-agent': random.choice(UserAgent_List), 'Host': 'movie.douban.com', 'Referer': 'https://movie.douban.com/subject/24773958/?from=showing', } session = requests.Session() cookie = { 'cookie': \"你的 cookie 值\", } time.sleep(random.randint(5, 15)) response = requests.get(url, headers=header, cookies=cookie_nologin, timeout = 3) if response.status_code != 200: print(response.status_code) return response 最后一步就是数据获取： 123456789101112131415161718192021222324252627def get_comments(eachComment): commentlist = [] user = eachComment.xpath(\"./h3/span[@class='comment-info']/a/text()\")[0] # 用户 watched = eachComment.xpath(\"./h3/span[@class='comment-info']/span[1]/text()\")[0] # 是否看过 rating = eachComment.xpath(\"./h3/span[@class='comment-info']/span[2]/@title\") # 五星评分 if len(rating) &gt; 0: rating = rating[0] comment_time = eachComment.xpath(\"./h3/span[@class='comment-info']/span[3]/@title\") # 评论时间 if len(comment_time) &gt; 0: comment_time = comment_time[0] else: # 有些评论是没有五星评分, 需赋空值 comment_time = rating rating = '' votes = eachComment.xpath(\"./h3/span[@class='comment-vote']/span/text()\")[0] # \"有用\"数 content = eachComment.xpath(\"./p/text()\")[0] # 评论内容 commentlist.append(user) commentlist.append(watched) commentlist.append(rating) commentlist.append(comment_time) commentlist.append(votes) commentlist.append(content.strip()) # print(list) return commentlist 制作云图因为爬取出来评论数据都是一大串字符串，所以需要对每个句子进行分词，然后统计每个词语出现的评论。我采用 jieba 库来进行分词，制作云图，我则是将分词后的数据丢给网站 worditout 处理。 123456789101112131415def split_word(): with codecs.open('Marvel3_yingpping.csv', 'r', 'utf-8') as csvfile: reader = csv.reader(csvfile) content_list = [] for row in reader: try: content_list.append(row[5]) except IndexError: pass content = ''.join(content_list) seg_list = jieba.cut(content, cut_all=False) result = '\\n'.join(seg_list) print(result) 最后制作出来的云图效果是： “灭霸”词语出现频率最高，其实这一点不意外。因为复联 3 整部电影的故事情节大概是，灭霸在宇宙各个星球上收集 6 颗无限宝石，然后每个超级英雄为了防止灭霸毁灭整个宇宙，组队来阻止灭霸。","link":"/554.html"},{"title":"Django 学习笔记之模型高级用法(上)","text":"前面有两篇文章简单介绍 Django 的模型，这一部分算是基础知识。我自己近期也总做了下总结，将花大概两篇的篇幅来分享下模型的一些高级用法。 如果想熟悉 Django 的用法，我认为应该一开始要熟悉一些细节用法，后面再了解 Django 的实现原理。而细节用法往往体现在一些差别用法，难以理解的知识点上。 复杂的字段类型 经过前面的学习，我们知道模型的字段类型一方面是指定数据库表的列名称和数据类型，另一方面决定 HTML 中的表单标签类型。 整数类型的区别Django 的整数类型有三个，分别是 IntegerField、BigIntegerField 和 SmallIntegerField。这三个字段区别在于取值范围。IntegerField 在 Django 所有支持的数据库中，合法取值范围是 -2147483648 到 2147483647。而 BigIntegerField 是一个 64 位整数，它允许的值范围是 -9223372036854775808 到 9223372036854775807。所以在数据库迁移的时候，特别数据库中有 Sqlite 时，要更加注意数字的取值范围。SmallIntegerField 取值范围是 -32768 到 32767。 自增类型的区别AutoFiled 和 BigAutoFiled 都是自增类型，它们都是由整数类型演化而来。AutoFiled 是一个根据实际 ID 自动增长的 IntegerField。通常不需要直接使用它,如果表中没有设置主键时，Django 将会自动添加一个自增主键。BigAutoField 其实也是一个 BigIntegerField，但它支持 ID 自动增长。所以它的取值范围不能为负数和零了。 时间类型DateField 和 DateTimeField 中的两个重要属性 auto_now 和 auto_now_add 默认值都是 Flase。 设置 auto_now 或者 auto_now_add 的值为 True，间接给该字段设置了 editable=False 和 blank=True 。给参数赋值需要传递一个 datetime.date 对象。如果时间是一串字符串，则转化为 date 对象。 DateField 支持输入值的形式如下： 123['%Y-%m-%d', # '2006-10-25' '%m/%d/%Y', # '10/25/2006' '%m/%d/%y'] # '10/25/06' DateTimeField 支持输入值的形式如下： 12345678['%b %d %Y', # 'Oct 25 2006' '%b %d, %Y', # 'Oct 25, 2006' '%d %b %Y', # '25 Oct 2006' '%d %b, %Y', # '25 Oct, 2006' '%B %d %Y', # 'October 25 2006' '%B %d, %Y', # 'October 25, 2006' '%d %B %Y', # '25 October 2006' '%d %B, %Y'] # '25 October, 2006' FilePathField该字段是用于保存文件路径信息的。默认最大长度为 100，当可通过 max_length 参数自定义。它包含几个重要的参数： path：必传参数。记录目录或者文件的绝对路径。例如：/home/monkeymatch：可选参数，它是一个正则表达式，主要用于匹配过滤出文件名。recursive：可选参数，表示是否包含子目录。默认值为 Flase。allow_files：可选参数，表示是否将文件名包括在内，默认值为 True。allow_folders：可选参数，表示是否将目录名包括在内默认值为 Flase。 Django 规定 allow_files 和 allow_folders 两者之间必须有一个值为 True。 FileField上传文件字段，常见于表单中。一般而言，文件都是保存在服务器的硬盘中。因此，该字段在数据库中其实是一个字符串类型，默认最大长度100，可以通过max_length参数自定义。 FileField 有两个重要的可选参数：upload_to 和 storage upload_toupload_to 是指定文件上传的目录。用法如下： 123456class MyModel(models.Model): # 文件上传到 MEDIA_ROOT/uploads upload = models.FileField(upload_to='uploads/') # 或者 # 文件上传到 MEDIA_ROOT/uploads/2015/01/30 upload = models.FileField(upload_to='uploads/%Y/%m/%d/') 其中 MEDIA_ROOT 是在 settings.py 中设置，表示上传文件的根目录。另外还需要设置 MEDIA_URL, 它表示上传文件对外能访问的 url 地址。 StorageStorage 是一个文件操作对象。它提供 size(path)、open(path).read()、delete(path)、exists(path)等方法来操作文件。 ImageField保存图像文件的字段。ImageField 用法跟 FileField 类似。除了需要在 seeting.py 中增加相关配置，还都拥有共同的 upload_to 字段选项。 它还有额外的可选参数：一个是 height_field，表示保存图片的高度。 另一个是 width_field，表示保存图片的宽度。 关系字段之前文章讲了三种关系字段的类型、定义、作用。今天讲下其中的一些字段选项。 ForeignKey1） on_delete在 Django 2.0 中，设置外键时需要添加一个 on_delete 选项。外键本身涉及到两个表的数据，况且外键在数据库中是有约束行为。所以 on_delete 参数是 Django 模拟 SQL 约束的行为。 on_delete 有几个可选值： CASCADE：这就是默认的选项，级联删除，你无需显性指定它。 PROTECT: 保护模式，如果采用该选项，删除的时候，会抛出 ProtectedError 错误。 SET_NULL: 置空模式，删除的时候，外键字段被设置为空，前提就是blank=True, null=True,定义该字段的时候，允许为空。 SET_DEFAULT: 置默认值，删除的时候，外键字段设置为默认值，所以定义外键的时候注意加上一个默认值。 SET(): 自定义对应的实体的值。 2）limit_choices_to该参数用于限制外键所能关联的对象，只能用于 Django 的 ModelForm（Django的表单模块）和 admin 后台，对其它场合无限制功能。该值接受是一个字典、返回一个字典的函数 3) db_constraint默认情况下，这个参数被设为 True，表示遵循数据库约束。如果设为 False，那么将无法保证数据的完整性和合法性。 4） related_name用于关联对象反向引用模型的名称。主要用于反向查询，即外键源模型实例通过管理器返回第一个模型的所有实例。 默认情况下，这个管理器的名字为 foo_set，其中 foo 是源模型名字的小写。例如： 123456# 在终端下使用 Django&gt;&gt;&gt;b = Book.objects.get(id=1)# 其中 entry_set 为默认的 related_name&gt;&gt;&gt;b.entry_set.all() &gt;&gt;&gt;b.entry_set.filter(headline__contains='天龙八部')&gt;&gt;&gt;b.entry_set.count() 如果我们设置 related_name=’novels’，那么上面的代码将变为： 123456# 在终端下使用 Django&gt;&gt;&gt;b = Book.objects.get(id=1)# 其中 entry_set 为默认的 related_name&gt;&gt;&gt;b.novels.all() &gt;&gt;&gt;b.novels.filter(headline__contains='天龙八部')&gt;&gt;&gt;b.novels.count() 5）related_query_name反向查询的关系查询集名称。用于从目标模型反向过滤模型对象的名称。具体用法如下： 1234567891011class Tag(models.Model): article = models.ForeignKey( Article, on_delete=models.CASCADE, related_name=\"tags\", related_query_name=\"tag\", ) name = models.CharField(max_length=255)# 现在可以使用 tag作为查询名Article.objects.filter(tag__name=\"important\") 字段选项字段选项是给每个 Field 指定一些属性。 db_column： 指定当前数据库表中该字段的列名。如果没有指定，Django 默认将 Field 名作为字段名。 db_index： 如果赋值为 True, 将会为这个字段创建数据库索引。 db_tablespace：如果该字段已经设置了索引，db_tablespace 用于指定字段索引的数据库表空间的名字。另外还需要看使用的数据库支不支持表空间。如果不支持，该参数设置没有效果。 editable：设置该字段是否能被编辑，默认是 True。如果设为 False , 这个字段将不会出现在 admin 或者其他 ModelForm 中。 同时也会跳过 模型验证 。 error_messages：用于自定义错误提示信息。参数接受的是字典类型的值。字典的 key 可以是 null, blank, invalid, invalid_choice, unique, 和 unique_for_dat 其中的一个。 help_text：用于前端页面上显示提示信息。要确保页面不存在 XXS 漏洞，需要使用django.utils.html.escape() 对内容进行转义。 unique_for_date：设置为 DateField 或者 DateTimeField 字段的名字，表示要求该字段对于相应的日期字段值是唯一的。例如，字段 title 设置了 unique_for_date=”pub_date” ，那么Django将不会允许在同一 pub_date 的两条记录的 title 相同。 unique_for_month：用法跟 unique_for_date 类似。 unique_for_year：用法跟 unique_for_date 类似。 verbose_name：为字段设置别名。对于每一个字段类型，除了 ForeignKey、ManyToManyField和 OneToOneField 这三个特殊的关系类型，其第一可选位置参数都是 verbose_name。如果用户没有定义该选项， Django会自动将自动创建，内容是该字段属性名中的下划线转换为空格的结果。 比如这个例子中描述名是 person's first name: 1first_name = models.CharField(\"person's first name\", max_length=30) 而没有主动设置时，则是 first name: 1first_name = models.CharField(max_length=30) 对于外键、多对多和一对一字字段，由于第一个参数需要用来指定关联的模型。因此必须用关键字参数 verbose_name 来明确指定。如下： 1234567891011poll = models.ForeignKey( Poll, on_delete=models.CASCADE, verbose_name=\"the related poll\",)sites = models.ManyToManyField(Site, verbose_name=\"list of sites\")place = models.OneToOneField( Place, on_delete=models.CASCADE, verbose_name=\"related place\",) 另外 verbose_name 不用大写首字母，在必要的时候 Django 会自动大写首字母。 validators：该字段将要运行的一个验证器的列表。例如 RegexValidator、EmailValidator。","link":"/449.html"},{"title":"数据库种类那么多，该如何选择？","text":"技术真的是日新月异，Web 网站已经脱离之前的静态网站的体系，转而使用动态语言搭建的动态网站。这也衍生出一个问题：该如何存储数据了？数据库就应运而生，它的作用是提供存储数据的容器。方便 web 网站进行存储、查询、更新等。数据库种类也很多，有成熟且稳定的 MySql 数据库，有后起之秀的 MangoDB 数据库，也有新时代宠儿 Redis 数据库。除此之外，还有其他一些数据库，例如 Sqlite、Oracle 等。 那么问题来了，面对多种类型的数据库，自己该如何选择？ 或许你因个人比较喜欢 MySql 数据库，所以选择它。也许你在网络上查了一下资料，发现别人都推荐使用 MangoDB，所以就选择它。 这两种想法都是不能正确地选择。任何脱离业务来谈架构都是在瞎扯。因此，要根据项目业务的场景需求来决定选择哪种数据库。每种数据库都各有优缺点，而选取标准是选择最优，最适合。 我个人的理解是结合以下几个方面来考虑： 读写速度这存储数据方式往往决定读写的速度。 Mysql 无论数据还是索引都存放在硬盘中。到要使用的时候才交换到内存中。能够处理远超过内存总量的数据。 MongoDB 的所有数据实际上是存放在硬盘的，所有要操作的数据通过 mmap 的方式映射到内存某个区域内。然后，MongoDB 就在这块区域里面进行数据修改，避免了零碎的硬盘操作。 Redis 所有数据都是放在内存中的。但是它也支持数据持久化到硬盘中。 我们都知道磁盘读取数据的效率远远低于内存。所以在一般情况下，这三者的读写数据的速度排序是：Redis &gt; MongoDB &gt; Mysql 是否支持事务以及复杂查询MySql 是关系型数据库，支持事务操作以及 join 方式的复结构化查询。而 MangoDB 是非关系型数据库, 既不支持事务操作，也不支持 join 操作。Redis 同样不支持。 因此，针对以下场景应考虑使用 MySql：1）业务数据中有大量结构化数据，如用户账号、地址等。因为这些数据通常需要做结构化查询。2）业务存在许多事务性操作，需要保证事务的强一致性。 业务数据量增长速度在一到两年内，业务数据的增长量不在预测范围内，优先考虑使用 MangoDB。 因为 MangoDB 内建了sharding、很多数据分片的特性，容易水平扩展，比较好的适应大数据量增长的需求。而 MySql 在这方面表现要逊色些，MySql 单表数据量达到 5-10 G 时会出现明细的性能降级，需要做数据的水平和垂直拆分、库的拆分完成扩展。 Redis 由于内存容量限制，不会用来存储大量数据。一般拿它做缓存。 表结构是否明确如果在业务场景中，数据库表接口不明确，数据还在不断增加。例如以下场景，内容管理平台（如 BBS 论坛中帖子场景），用户社交平台（如贴吧中的帖子以及用户评论），优先考虑使用 MangDB 。 因为 MongoDB 是非结构化文档数据库，扩展字段很容易且不会影响原有数据。 写在最后，数据库作为存储数据的容器， 在架构选择上，应多花点时间考虑。","link":"/555.html"},{"title":"Linux，越折腾越喜欢","text":"今天这波分享可以说是自己热血来潮。起因是自己收到某个问答社区小秘的问题邀请。问题是《谁能给我推荐几本linux的书？从基础到进阶提高的linux书？》。自己挺怀念大学那段折腾 Linux 的时光，所以就忍不住评论一波。可曾没有想到，这一评论的阅读量有 1.5 W，有些网友想要鸟哥 Linux 的学习视频，私信我以及到公众号后台留言。 因此，就有今天这篇文章。一是我分享下自己学习 Linux 的过程，希望各位小伙伴不要踩坑。二是分享学习资料。 大学二年级，我课程中有一门专业课《操作系统》。当时授课老师是我最喜欢的老师。他原先在北京理工大学讲课，后来到我们学校教书。他不仅自己学识淹博，而且有丰富的教学经验，讲课方式生动。所以同学们很喜欢上他的课，当然我也是不例外。另外他姓龚，同学们觉得他和蔼可亲，就经常称呼他“老龚”。 我自己预习《操作系统》课程，真是看不下去。书本内容真的是深奥难懂又枯燥无味。后来上课，老师用以先实践后理论的方式来讲课，我们都听得入神。 我记得龚老师那时说过一句很经典的话：《操作系统》主要是讲解 Linux 系统。而我们现在使用的 Windows 系统是傻瓜式操作。很多有趣东西被系统给封装，都看不到。推荐同学们去学习 Linux 系统，哪怕一开始在 Linux 上玩游戏也好。从那之后，我开始折腾 Linux，后来就一发不可收拾。 当时自己在网上搜索一番，我发现 Linux 发行版本确实很多。当时自己选择 Ubuntu 系统操作，因为 Ubuntu 确实适合入门。如果小伙伴不知道怎么选择发行版，可以看以下内容。 我列举几个发行版：1、有适合新手的 Ubuntu 系统 Ubuntu 是国内乃至全球热门的Linux发行版。也是各种推荐入门Linux爱好者安装的一个Linux发行版。它的桌面版本提供了图形化，方便新手从图形化界面过渡到命令行操作。同时，它还具有很棒很强大的软件库。使用 apt-get 命令就能安装各种所需的应用。 2、Debian 几大基础发行版之一，Ubuntu就是基于Debian的。这个发行版本的特别是不求新，但求稳。如果你想选择一款 Linux 系统作为你的桌面系统，又想运行各种服务器应用。推荐使用 Debian，另外它安装应用也是很方便。跟 Ubuntu 一样，使用 apt-get 命令。毕竟 Ubuntu 是 Debian 的“孩子”。 3、Fedora 经常被拿来跟 Ubuntu 比较的发行版。这个发行版由红帽（Red Hat）赞助，提供了非常炫酷的操作界面 KDE。另外它也有跟 Ubuntu 一样的包管理工具 YUM，使用起来也是很方便。不够还是要看个人习惯。 4、Arch Linux 如果你想在开发板或者老旧的电脑折腾 Linux，Arch 是你不二之选。Arch 系统是轻量级，安装什么组件都由你决定。这也充分满足“极客”的需求，想怎么折腾就怎么折腾，先怎么定制化就怎么定制化。最后，它还具备完善的 WIKI 文档，特别是 WIKI 中文化程度很高。 除此之外，还有很多发行版本，我就不一一列举了。 自己当时在 WMware 虚拟机安装 Ubuntu，然后开始自己的折腾之路。当时想 QQ 聊天工具是必备，自己就在 Ubuntu 上折腾安装 QQ。想想应该有点音乐才够味，于是乎折腾怎么安装深度音乐播放器。看到 KDE 桌面很炫酷，折腾安装 KDE。一开始因为好奇，各种折腾。再到后来竟然折腾编译 Linux 内核。 自己一开始就乱折腾这些，当看了鸟叔的 Linux 视频后才算是真正“上道”。我看视频主要学到了一些常见的 Linux 命令、文件与目录管理、账号与身份管理、进程管理、Vim 编辑器等内容。后面我就开始使用 vim 编写代码，折腾 gcc 编译，调用系统 api 创建进程等 因此，如果小伙伴想入门 Linux，无论后续是否要往 Linux 方向走。建议大家选择自己喜欢的发行版，然后跟着《鸟哥的Linux私房菜基础学习篇》这书的节奏学习。最好是一边看书，一边实践，加深自己的印象。当然，看视频也是不错的选择。 如果你想要鸟哥的私房菜 linux 视频教程以及我整理的一些笔记资料。可以在公众号后台回复【Linux】，即可获取。","link":"/557.html"},{"title":"Django 学习笔记之模型高级用法(下)","text":"接着上篇文章内容，本文分享自己对模型一些用法的总结。 模型的元数据Meta除了抽象模型，在模型中定义的字段都会成为表中的列。如果我们需要给模型指定其他一些信息，例如排序方式、数据库表名等，就需要用到 Meta。Meta 是一个可选的类，具体用法如下： 1234567class Author(models.Model): name = models.CharField(max_length=40) email = models.EmailField() class Meta: managed = True db_table = 'author' 不知你是否对上述代码有影响。通过 Django 将数据库表反向生成模型时，Django 会默认带上 managed 和 db_table 信息。 我主要说下 Meta 一些重要的属性，其他属性你可以通过文档信息进行学习。 abstract: 如果 abstract = True，模型会指定为抽象模型。它相当于面向对象编程中的抽象基类。 proxy：如果设置了proxy = True，表示使用代理模式的模型继承方式。 db_table：指定当前模型在数据库的表名。 managed：该属性默认值为 True，表示能创建模型和操作数据库表。 ordering：指定该模型生成的所有对象的排序方式。默认按升序排列，如果在字段名前加上字符 “-” 则表示按降序排列，如果使用字符问号 “？” 表示随机排列。 123ordering = ['pub_date'] # 表示按'pub_date'字段进行升序排列ordering = ['-pub_date'] # 表示按'pub_date'字段进行降序排列ordering = ['-pub_date', 'author'] # 表示先按'pub_date'字段进行降序排列，再按`author`字段进行升序排列。 verbose_name：给模型设置别名。如果不指定它，Django 会使用小写的模型名作为默认值。 12verbose_name = \"book\"verbose_name = \"图书\" verbose_name_plural：因为英语单词有单数和复数两种形式，这个属性是模型对象的复数名。中文则跟 verbose_name 值一致。如果不指定该选项，那么默认的复数名字是 verbose_name 加上 ‘s’ 。 12verbose_name_plural = \"books\"verbose_name_plural = \"图书\" indexes：为当前模型建立索引列表。用法如下： 1234567891011from django.db import modelsclass Customer(models.Model): first_name = models.CharField(max_length=100) last_name = models.CharField(max_length=100) class Meta: indexes = [ models.Index(fields=['last_name', 'first_name']), models.Index(fields=['first_name'], name='first_name_idx'), ] 模型的继承根据模型的 Meta 信息设置，模型继承方式可以分为三种：1）抽象模型模型的 Meta 类中含有 abstract = True 属性。抽象模型一般被当作基类，它持有子类共有的字段。值得注意的是，抽象模型在数据库中不会生成表。 12345678910111213from django.db import models# 抽象模型class Person(models.Model): name = models.CharField(max_length=500) age = models.PositiveIntegerField() class Meta: abstract = True# 子模型class Student(Person): school_name = models.CharField(max_length=20) 子模型如果没有定义 Meta 类，那么会继承抽象模型的 Meta 类。但是 abstract 属性不会被继承。 2）多表继承这种方式继承方式，子模型的父模型可以一个或者多个。 当父类模型是正常的模型，即不是抽象模型，在数据库中有对应表。 虽然在 Model 层不推荐使用多重继承，但 Django 的 ORM 还是支持这样的使用方式。如果使用多表继承，子模型跟每个父模型都会添加一个一对一的关系。 12345678910111213from django.db import models# 父模型 oneclass Model_One(models.Model): attr1 = models.CharField(max_length=10)# 父模型 twoclass Model_Two(models.Model): attr2 = models.CharField(max_length=10)# 子模型class Multiple(Model_One, Model_Two): attr3 = models.CharField(max_length=10) 多重继承的时候，子类的 ORM 映射会选择第一个父类作为主键管理，其他的父类作为一般的外键管理。 3）代理模型使用多表继承时，父类的每个子类都会创建一张新数据表。但是我们只是想扩展一些方法，而不想改变模型的数据存储结构。我们可以将在 Meta 类中增加约束proxy=True 来实现。此时子模型称为父模型的代理类，子类中只能增加方法，而不能增加属性。 1234567891011121314151617from django.db import modelsfrom django.contrib.auth.models import Userclass Person(User): name = models.CharField(max_length=10) class Meta: proxy = True def do_something(self): passclass Man(Person): job = models.CharField(max_length=20)class Woman(Person): makeup = models.CharField(max_length=20)","link":"/450.html"},{"title":"使用 Python 将数据写到 CSV 文件","text":"我们从网上爬取数据，最后一步会考虑如何存储数据。如果数据量不大，往往不会选择存储到数据库，而是选择存储到文件中，例如文本文件、CSV 文件、xls 文件等。因为文件具备携带方便、查阅直观。 Python 作为胶水语言，搞定这些当然不在话下。但在写数据过程中，经常因数据源中带有中文汉字而报错。最让人头皮发麻的编码问题。 我先说下编码相关的知识。编码方式有很多种：UTF-8, GBK, ASCII 等。 ASCII 码是美国在上个世纪 60 年代制定的一套字符编码。主要是规范英语字符和二进制位之间的关系。英语词汇组成简单，由 26 个字母构成。使用一个字节就能表示一个字母符号。外加各种符号，使用 128 个字符就满足编码要求。 不同国家有不同语言文字。同时，文字组成部分的数量相比英语字母要多很多。根据不完全统计，汉字的数量大约将近 10 万个，日常所使用的汉字有 3000 个。显然，ASCII 编码无法满足需求。所以汉字采用 GBK 编码，使用两个字节表示一个汉字。简体中文的编码方式是 GBK2312。 那 UTF-8 又是什么编码？这要先说 Unicode 了。Unicode 目的是为了统一各种编码。因为各国都各自的编码方式。如果使用一种编码编码，使用另一种编码解码。这会造成出现乱码的情况。但 Unicode 只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。UTF-8 就是在互联网上使用最广的一种 Unicode 的实现方式。 因此，如果我们要写数据到文件中，最好指定编码形式为 UTF-8。 Python 标准库中，有个名为 csv 的库，专门处理 csv 的读写操作。具体使用实例如下： 12345678910111213141516171819202122232425import csvimport codecs# codecs 是自然语言编码转换模块fileName = 'PythonBook.csv'# 指定编码为 utf-8, 避免写 csv 文件出现中文乱码with codecs.open(fileName, 'w', 'utf-8') as csvfile: # 指定 csv 文件的头部显示项 filednames = ['书名', '作者'] writer = csv.DictWriter(csvfile, fieldnames=filednames) books = [] book = { 'title': '笑傲江湖', 'author': '金庸', } books.append(book) writer.writeheader() for book in books: try: writer.writerow({'书名':book['title'], '作者':book['author']}) except UnicodeEncodeError: print(\"编码错误, 该数据无法写到文件中, 直接忽略该数据\") 这种方式是逐行往 CSV 文件中写数据， 所以效率会比较低。如果想批量将数据写到 CSV 文件中，需要用到 pandas 库。 pandas 是第三方库，所以使用之前需要安装。通过 pip 方式安装是最简单、最方便的。 1pip install pandas 使用 pandas 批量写数据的用法如下： 123456789101112131415161718192021222324import pandas as pdfileName = 'PythonBook.csv'number = 1books = []book = { 'title': '笑傲江湖', 'author': '金庸',}# 如果 book 条数足够多的话，pandas 会每次往文件中写 50 条数据。books.append(book)data = pd.DataFrame(books)# 写入csv文件,'a+'是追加模式try: if number == 1: csv_headers = ['书名', '作者'] data.to_csv(fileName, header=csv_headers, index=False, mode='a+', encoding='utf-8') else: data.to_csv('fileName, header=False, index=False, mode='a+', encoding='utf-8') number = number + 1except UnicodeEncodeError: print(\"编码错误, 该数据无法写到文件中, 直接忽略该数据\")","link":"/556.html"},{"title":"爬取《Five Hundred Miles》在网易云音乐的所有评论","text":"在使用 Ajax 技术加载数据的网站中， JavaScript 发起的 HTTP 请求通常需要带上参数，而且参数的值都是经过加密的。如果我们想利用网站的 REST API 来爬取数据，就必须知道其使用的加密方式。破解过程需要抓包，阅读并分析网站的 js 代码。这整个过程可能会花费一天甚至更长的时间。 问：那么是否有办法绕过这机制，直接获取网站数据？答：有的。使用 Selenium 库模拟浏览器行为来抓取网站数据，达到事半功倍的效果。 本文内容是利用 Selenium 爬取网易云音乐中的歌曲 《Five Hundred Miles》 的所有评论，然后存储到 Mongo 数据库。 前期准备本文中所用到的工具比较多，所以我将其列举出来。 Selenium Selenium 是一个 Web 应用程序自动化测试的工具。它能够模拟浏览器进行网页加载。所以使用其来帮助我们解决 JavaScript 渲染问题。 接下来就是安装 selenium, 使用 pip 安装是最方便的。 1pip install selenium Chrome 浏览器 在爬取数据过程中, 需要启动浏览器来显示页面。因此，电脑中需要一款浏览器。这里推荐使用 Chrome 浏览器。推荐使用 59 版本以上的 Chrome，当然能使用最新版本那最好不过，目前最新版本是 68。 Webdriver Webdriver 是浏览器驱动。selenium 通过 Webdriver 来操作浏览器。因为我们使用的浏览器是 Chrome，所以需要下载 Chrome 浏览器对应的驱动。 下载地址：http://chromedriver.chromium.org/downloads webdriver 下载解压完成之后，将其放到 Python 目录下的 Script 文件夹中。 MongoDB 网易云音乐的评论数据总数都很大，十几万条数据比比皆是，甚至还有上百万条数据。所以需要将数据存储到数据库中，我选用的是 MongoDB。 pymongo pymongo 是 Python 操作 MongoDB 的库。同样使用 pip 进行安装。 1pip install pymongo 爬取思路1）使用 Selenium 驱动 Chrome 浏览器打开需要爬取的页面。2）获取页面中 最新评论 标签后面的评论总数，计算出一共有多少个分页， 方便统计。利用总评论数除以 20（每个页面显示 20 条评论），然后对结果进行向上取整。3）爬取第一页面的评论的数据，然后存储到数据库中。4）利用 Selenium 模拟点击下一页按钮，再继续爬取该页面的评论数据，并存储到数据库中。5）一直循环点击，直到所有分页的数据都被爬取完成。 代码实现我们要爬取的歌曲是 《Five Hundred Miles》，先找到其 url 地址，然后调用爬取函数。 123if __name__ == '__main__': url = 'http://music.163.com/#/song?id=27759600' # Five Hundred Miles start_spider(url) 使用 selenium 启动 Chrome 浏览器。 12345678910111213141516171819from selenium import webdriverdef start_spider(url): \"\"\" 启动 Chrome 浏览器访问页面 \"\"\" \"\"\" # 从 Chrome 59 版本, 支持 Headless 模式(无界面模式), 即不会弹出浏览器 chrome_options = webdriver.ChromeOptions() chrome_options.add_argument('--headless') brower = webdriver.Chrome(chrome_options=chrome_options) \"\"\" brower = webdriver.Chrome() brower.get(url) # 等待 5 秒, 让评论数据加载完成 time.sleep(5) # 页面嵌套一层 iframe, 必须切换到 iframe, 才能定位的到 iframe 里面的元素 iframe = brower.find_element_by_class_name('g-iframe') brower.switch_to.frame(iframe) # 获取【最新评论】总数 new_comments = brower.find_elements(By.XPATH, \"//h3[@class='u-hd4']\")[1] 根据评论总数计算出总分页数。 12345678910111213# start_spider(url)max_page = get_max_page(new_comments.text)def get_max_page(new_comments): \"\"\" 根据评论总数, 计算出总分页数 \"\"\" print('=== ' + new_comments + ' ===') max_page = new_comments.split('(')[1].split(')')[0] # 每页显示 20 条最新评论 offset = 20 max_page = ceil(int(max_page) / offset) print('一共有', max_page, '个分页') return max_page 接着循环抓取评论数据，首先抓取第 1 页的评论数据。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# start_spider(url)current = 1is_first = Truewhile current &lt;= max_page: print('正在爬取第', current, '页的数据') if current == 1: is_first = True else: is_first = False data_list = get_comments(is_first, brower)def get_comments(is_first, brower): \"\"\" 获取评论数据 \"\"\" items = brower.find_elements(By.XPATH, \"//div[@class='cmmts j-flag']/div[@class='itm']\") # 首页的数据中包含 15 条精彩评论, 20 条最新评论, 只保留最新评论 if is_first: items = items[15: len(items)] data_list = [] data = {} for each in items: # 用户 id userId = each.find_elements_by_xpath(\"./div[@class='head']/a\")[0] userId = userId.get_attribute('href').split('=')[1] # 用户昵称 nickname = each.find_elements_by_xpath(\"./div[@class='cntwrap']/div[1]/div[1]/a\")[0] nickname = nickname.text # 评论内容 content = each.find_elements_by_xpath(\"./div[@class='cntwrap']/div[1]/div[1]\")[0] content = content.text.split('：')[1] # 中文冒号 # 点赞数 like = each.find_elements_by_xpath(\"./div[@class='cntwrap']/div[@class='rp']/a[1]\")[0] like = like.text if like: like = like.strip().split('(')[1].split(')')[0] else: like = '0' # 头像地址 avatar = each.find_elements_by_xpath(\"./div[@class='head']/a/img\")[0] avatar = avatar.get_attribute('src') data['userId'] = userId data['nickname'] = nickname data['content'] = content data['like'] = like data['avatar'] = avatar print(data) data_list.append(data) data = {} return data_list 将第 1 页评论数据存储到 Mongo 数据库中。 1234567891011121314# start_spider(url)save_data_to_mongo(data_list)def save_data_to_mongo(data_list): \"\"\" 一次性插入 20 条评论。 插入效率高, 降低数据丢失风险 \"\"\" collection = db_manager[MONGO_COLLECTION] try: if collection.insert_many(data_list): print('成功插入', len(data_list), '条数据') except Exception: print('插入数据出现异常') 模拟点击“下一页”按钮。 12345678910111213# start_spider(url)time.sleep(1)go_nextpage(brower)# 模拟人为浏览time.sleep(random.randint(8, 12))current += 1def go_nextpage(brower): \"\"\" 模拟人为操作, 点击【下一页】 \"\"\" next_button = brower.find_elements(By.XPATH, \"//div[@class='m-cmmt']/div[3]/div[1]/a\")[-1] if next_button.text == '下一页': next_button.click() 最后就一直循环爬取评论。 爬取结果评论总数大概有 23W 条, 我又在代码中增加延时操作。所以爬取所有评论大概需要 69 个小时。目前我只跑了 9 个小时，我贴下暂时爬取的结果。 扩展知识这部分内容跟上述内容联系不大, 属于服务器技术范畴。如果你不感兴趣的话，可以直接跳过。另外，这部分内容是自己的理解。如果有讲错的地方，还请多多指出。 我们访问普通网站的整个过程： 我们访问使用 Ajax 加载数据的网站的整个过程：","link":"/660.html"},{"title":"安利两个 MongoDB 可视化工具","text":"MongoDB 是一种 NoSQL 数据库。NoSQL(Not Only SQL )，意即”不仅仅是SQL”, 泛指非关系型的数据库。这两种类型差别之一是存储方式。关系数据库以键值对存储，它的结构不固定。而关系型数据库以行和列的二维表格形式来存储数据。所以非关系型数据库（如 MongoDB）不支持标准的 SQL 的语法。 如果我们刚接触 Mongo 数据库，对语法还不熟悉，想通过可视化方式来操作。那么，今天我就给大家推荐两个 Mongo 可视化操作工具。 Mongo Plugin大多数 Python 程序员使用的编辑器是 Pycharm。因为 Pycharm 是一个非常人性化的 IDE 工具。Pycharm 工具自带支持 MySQL、SQLite 等数据库的可视化工具。但是还不支持 Mongo 数据库。国外一位大神 dboissier 专门为 Pycharm 开发了 一款插件。它的就是 Mongo Plugin。这让我们可以在 Pycharm 上对 mongo 数据库进行管理。 安装该插件也很方便，直接在 Pycharm 的【settings】-【Plugin】选项中搜索 mongo 即可安装。 然后在【settings】中找到【Mongo Servers】，在配置下 MongoDB 的安装路径。然后填写连接的数据库配置信息，就可以操作数据库了。 成功连接数据库之后，就可以对 mongo 的 Collections（相当于 MySQL 的 表）进行操作。 通过下图，我们了解到数据是以 Key-Value 形式展示。我们也可以直接对数据进行修改或者删除。 推荐原因： 它是 Pycharm 的插件。所以我们可以直接在 Pycharm 操作，方便快捷。 可以图形化显示数据。 支持 增删改查 基本操作。 NosqlclientNosqlclient 是一个跨平台的免费的 MongoDB 管理工具。因为它是由有 Node.js 编写的，相当于一个 web 应用程序，所以我们可以直接将其部署到服务器上。如果我们使用 Windows 系统或者 Mac 系统，我们不需要安装 Node.js 环境，再运行 Nosqlclient。官方团队已经为我们提供安装包，我们只需下载安装，然后即可直接运行。 传送门：下载地址 界面采用“简约”的风格，给人一种清爽，舒服的感觉。 数据库监控界面，可以监控当前内存使用情况，数据读写情况 数据库管理界面，支持管理用户，导出/导入数据等 工具界面，直接命令行操作数据库，分析 Collections（相当于 MySQL 的 表）的情况等。我最喜欢这个功能。 数据管理，对数据进行增删改查操作。 推荐原因： 界面友好、直观，同时操作方便 有对内存和数据库数据的实时监控功能 以图表形式即时展示数据库读/写情况 支持数据库数据导入导出 支持 SSH 远程连接 模式分析","link":"/661.html"},{"title":"Python 中“is”和“==”的区别","text":"相比 C/C++ 、Java 等强类型语言, Python 定义变量的方式就简单多了。我们只需要给变量起个变量名，而不需要给变量指定类型。 正因为 Python 弱化类型这一概念，所以我们能随意给变量赋值。值可以是整数，浮点数，字符串，列表等。 1234a = 1a = 3.33a = 'monkey'a = [1, 2, 3] 凡事都有正反两面。使用弱类型语言编程，我们可以不需要定义变量类型，可以随意转换类型，代码看起来很简介。但是在变量判断是否相等时候，会给我们造成一定困惑。 判断值相等，是选择 “is” 还是 “==” 了？ 当你了解 Python 的语言特性之后，这个问题就会迎刃而解。在 Python 中，万物皆为对象。 每个对象有 3 个属性。分别是：id，type，value。 id 就是对象的内存地址，可以通过内置函数 id() 查看对象引用的地址。 type 表示对象的类型。Python 也是有类型的概念。对于编译器或者解释器而言，类型可以协助确保上面那些电荷、字节在程序的运行中始终如一地被理解。我们可以通过内置函数 type() 查看对象的类型。 value 就是对象的值。 综上所述：如果我们要判断两个变量的值是否相等，需要使用 “==”。一般运用在判断数值和字符串是否相等。 123456789a = '123'b = '123'print(a=b)&gt;&gt; Truec = 1d = 2print(c==d)&gt;&gt; False is 则是用来判断两个变量的 id 是否相等，当两个变量的 id 相等时，说明这两个变量指向的地址是相同的，那么这两个变量的一切属性(包括：类型、值)都相同。同时，Python 也规定 None, False, 空字符串””, 0, 空列表[], 空字典{}, 空元组()都相当于 False。所以我们可以使用 is 来判断变量是否为空。 12345str = \"\"print(str is None)&gt;&gt; Falseprint(str is not None)&gt;&gt; True","link":"/664.html"},{"title":"多线程爬取 unsplash 图库","text":"我公众号文章的封面配图都在 Unsplash 上找的。因为 Unsplash 是一个完全免费的、无版权的高清图片资源网站。 所谓的「无版权」是指这个网站上的图片由创作者自愿分享出来，完全免费提供给任何人作为任何用途使用。Unsplash 的原话是「do whatever you want」，进一步说明是「你可以免费对图片进行复制、修改、分发，包括用作商业目的，无需经过允许即可使用」。 自己发现之前在寻找图片上还是挺花费时间的。先在 Unsplash 上浏览图片，当发现觉得还不错的图片就会下载下来。另外，下载图片还需要自己点击下载按钮。这确实挺花费时间。现在自己学会了网络爬虫，是时候改善下情况。 分析Unsplash 网站采用瀑布流样式来呈现图片。首页以开始只会呈现一部分图片，当我们滑动滚动条到底部时，网页才会继续加载部分图片。这网站经常使用 Ajax 技术来加载图片。针对动态渲染网页，我会选择 Selenium 来爬取。但是，我这次为了追求高效率下载图片，势必要使用多线程。因此，只能放弃使用 Selenium，转而通过抓包方式来分析网站。 我使用浏览器的开发者工具来查看网络请求。查看首页的数据包，只能得到知首页是经过重定向的信息。 接着, 自己满怀期待查看 main.js 文件。因为名字的原因，所以自己怀疑这个 js 文件的作用是发起请求网络。自己只要在代码中搜索下 http 字样，说不定还有意外的收获。 结果希望又落空。只能接着分析。 经过一番漫长的分析之后，最后发现两个很有价值的信息。 这个 url 地址十有八九是图片的请求地址。 自己使用浏览器访问这个地址，证实自己的猜想是正确的。 根据英语单词，可以推断出各个参数的意思。page 表示页数, 从前面的信息得知目前一共有 71131 个页面；per_page 表示每页拉去的图片数, order_by 表示按时间从现在到以前的顺序来拉取图片。 我自己使用 Requests 库编写一段简单的请求代码。目的是验证网站是否有反爬虫机制，结果发现没有。 爬取思路因为多线程需要考虑线程安全的问题，所以我决定使用 Queue 队列模块来存储所有的的 url 地址。Queue 模块中提供了同步的、线程安全的队列类，其中就有 FIFO（先入先出)队列 Queue。Queue 内部实现了锁原语，帮我们实现加锁和释放锁的操作。因此，我们能够在多线程中直接使用。 最终的思路是： 1) 计算出所有图片的 url 地址，然后使用 Queue 存储起来2) 创建并启动多个线程，然后每个线程要完成以下工作：使用 requests 库请求 url 地址、使用 JSON 库解析的 JSON 形式的响应体，获取图片的下载地址、使用 urllib 库下载图片到本地。 代码实现先定义一些常量, 方便后续操作。 123456# 使用队列保存存放图片 url 地址, 确保线程同步url_queue = Queue()# 线程总数THREAD_SUM = 5# 存储图片的位置IMAGE_SRC = 'D://Unsplash/' 计算出所有的 url 地址, 然后存放到 url_queue 队列中。 12345678910def get_all_url(): \"\"\" 循环计算出所有的 url 地址, 存放到队列中 \"\"\" base_url = 'https://unsplash.com/napi/photos?page={}&amp;per_page=1&amp;order_by=latest' page = 1 max_page = 71131 while page &lt;= max_page: url = base_url.format(page) url_queue.put(url) page += 1 print('计划下载', url_queue.qsize(), '张图片') 创建多个线程，然后逐个启动。 12345if __name__ == '__main__': get_all_url() for i in range(THREAD_SUM): unsplash = Unsplash(i+1) unsplash.start() 原生的 Thread 类无法满足场景的需求。我新建一个名为 Unsplash 的类，该类继承threading.Thread 来自定义线程类，接着重写 run 方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Unsplash(threading.Thread): NOT_EXIST = 0 def __init__(self, thread_id): threading.Thread.__init__(self) self.thread_id = thread_id def run(self): while not self.NOT_EXIST: # 队列为空, 结束线程 if url_queue.empty(): NOT_EXIST = 1 break url = url_queue.get() self.get_data(url) time.sleep(random.randint(3, 5)) def get_data(self, url): \"\"\" 根据 url 获取 JSON 格式的图片数据\"\"\" headers = { 'User-agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.3964.2 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8', 'referer': 'https://unsplash.com/', 'path': url.split('com')[1], 'authority': 'unsplash.com', 'viewport-width': '1920', } response = requests.get(url, headers=headers) print('请求第[ ' + url + ' ], 状态码为 ', response.status_code) self.get_image_url(response.text) def get_image_url(self, response): \"\"\" 使用 json.loads(response) 将其转化为字典类型, 以便采用 key-value 形式获取值 raw：包含Exif信息的全尺寸原图，此类图片的容量很大 full：全尺寸分辨率的图片，去除了Exif信息并且对内容进行了压缩，图片容量适中 normal：普通尺寸的图片，去除了Exif信息，并且对分辨率和内容进行了压缩，图片容量较小； \"\"\" image_url = json.loads(response)[0]['urls']['full'] self.save_img(image_url) def save_img(self, image_url): print('线程', self.thread_id, ' | 正在下载', image_url) try: if not os.path.exists(IMAGE_SRC): os.mkdir(IMAGE_SRC) filename = IMAGE_SRC + image_url.split('com')[1].split('?')[0] + '.jpg' # 下载图片，并保存到文件夹中 urllib.request.urlretrieve(image_url, filename=filename) except IOError as e: print('保存图片出现异常失败', e) 爬取结果按照计划，执行完一次程序代码。硬盘的 D 盘目录中会有一个 Unsplash 的文件夹，里面会有 71131 张图片。我是将爬虫程序运行在云主机上，所以就只显示本地爬取的 100 多张图片。 源码如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/662.html"},{"title":"os.path 模块用法详解","text":"总所周知，Windows 操作系统和 Linux 系统存在很多不兼容的地方。文件路径就是一个明显的例子。在 Linux 中，路径的分割采用正斜杠 “/“，比如 “/home/monkey”；而在 Windows 中，路径分隔采用反斜杠 “&quot;，比如 “C:\\Windows\\System”。 我们在编码过程中，往往需要和文件或文件夹打交道。如果程序中涉及到到路径操作，我们最好使&lt;用 Python 标准库中的 os.path 模块来实现。这样能避免出现程序无法多平台运行的问题。 os.path 全称是Common pathname manipulations, 译为通用路径名操作。其作用是提供操作各种处理文件名以及路径名的函数。因此，本文的内容是讲述os.path模块中几个常见函数的用法。 获取文件(夹)绝对路径abspath(path) 函数返回的是 path 经过规范化的绝对路径。假设在 D 盘中有 Downloads 的文件夹，该文件夹中有叫 cat.jpg 的图片。我们要获取这张图片的经过转椅的绝对路径，可以在 Downloads 文件夹中打开 DOS 窗口，然后进入 Python 环境。 1234# 先引用 os 标准库import osos.path.abspath('cat.jpg')&gt;&gt; 'D:\\\\Downloads\\\\cat.jpg' 从路径中切割出文件名split(path) 函数的作用是将传入的 path 分割成目录和文件名，然后以元组形式返回。 123456789101112import os# Downloads 文件夹下有个 cat.jpg 的图片os.path.split('D:\\\\Downloads\\\\cat.jpg')&gt;&gt; ('D:\\\\Downloads', 'cat.jpg')# Downloads 文件夹下有个 animal 的文件夹os.path.split('D:\\\\Downloads\\\\animal')&gt;&gt; ('D:\\\\Downloads', 'animal')# 如果 path 是以两个反斜杠 \\\\ 结尾，则返回空os.path.split('D:\\\\Downloads\\\\')&gt;&gt; ('D:\\\\Downloads', '') 将多个文件路径合并多个路径组合合并需要用到函数 join(path1, path2, ...)。join 函数合并路径有个特点：在所有 path 参数中，只会从第一个绝对路径的参数开始合并。这也意味着第一个绝对路径之前的参数将被忽略。 1234567import osos.path.join('Downloads', 'D:\\\\', 'animal')&gt;&gt; 'D:\\\\animal'# 第一个参数是绝对路径，则按顺序合并os.path.join('D:\\\\', 'Downloads', 'animal')&gt;&gt; 'D:\\\\Downloads\\\\animal' 判断路径是否存在exists(path) 函数是判断一个文件或者文件夹的路径是否存在。如果 path 存在，返回 True；如果 path 不存在，则返回 False； 12345678910111213import os# D 盘确实有个 Downloads 的文件夹os.path.exists('D:\\\\Downloads\\\\')&gt;&gt; True# Downloads 还有个 animal 的文件夹os.path.exists('D:\\\\Downloads\\\\animal\\\\')&gt;&gt; True# D 盘没有名为 cat 的文件夹os.path.exists('D:\\\\cat\\\\')&gt;&gt; False 判断路径是否是一个存在的文件判断 path 是否是一个存在文件，函数 isfile(path) 就能轻松搞定。如果 path 是一个存在的文件，返回True。否则返回 False。 1234567import osos.path.isfile('D:\\\\Downloads\\\\cat.jpg')&gt;&gt; Trueos.path.isfile('D:\\\\Downloads\\\\animal')&gt;&gt; False 将路径规范化normcase(path) 的作用是解决操作系统间路径分割符的兼容性。在 Linux 和 Mac 平台上，该函数会原样返回path，在 Windows 平台上会将路径中所有字符转换为小写，并将所有斜杠转换为反斜杠。 123456789import # Linux 系统下os.path.normcase('/home/monkey')&gt;&gt; '/home/monkey'# Window 系统下os.path.normcase('D:/download\\\\animal')&gt;&gt; 'd:\\\\download\\\\animal' 除了上述的常用函数之外，os.path 还有其他函数。感兴趣的同学浏览 Python 官网文档。","link":"/766.html"},{"title":"Python 读取文本文件的内容","text":"数据存储方式有很多种。如果数据的数据量比较大、数据类型繁多且要求便于搜索，我们一般会选择存储到数据库中。如果数据内容只是一些的文本信息，我们可以将数据存储到 TXT 、JSON、CSV 等文本文件中。类似存储小说、日志内容等场景，一般是将内容存储到文本文件中。数据已经存储到 txt 文件中，那该如何读取了？本文的主要内容是讲解如何读取文本文件的内容。 打开文件文本操作可以想象成对水池进行加水和排水。文本文件就好比一个存储水的水池，数据就类似水。从文本文件中读取数据好比让水池排水。在这过程中，我们需要一条“管道”才能从读取到数据。在 Python 语言中，open() 函数就是这样的“管道”。当 open() 函数成功打开文件后，我们会得到一个 file 对象。 1file = open('One Day.txt', 'r') 但是操作文件经常会出现各种异常，例如文件不存在，文件不具备可读属性等。因此，我们需要做异常处理工作。这里推荐使用 with 语句，其内部已经实现异常处理相关的逻辑。另外还有一个好处，我们还可以不用调用 close() 函数来关闭文件。 12with open('One Day.txt', 'r') as file: pass open() 函数的第二个参数是打开模式。可以是只读r，写入w，追加a、以二进制形式读取rb等。 read()read() 函数读取数据方式有点暴力。它是一次性将文件的全部内容读取到内存中。如果文件太多的话，会把内存给撑爆。为了保险起见，我们通常每次只读取一小段区间内容，然后反复调用。 1234# -*- coding:utf-8 -*-size = 1024with open('one day.txt', 'r') as file: print(file.read(size)) readline()如果我们需要每次只读取一行内容，则需要用到readline() 函数。这种读取方式虽然效率不高，但是占用内存小，能做到即读即用。 12345678# -*- coding:utf-8 -*-import timewith open('one day.txt', 'r') as file: for line in file: # realine() 读取整行内容，包括 \"\\n\" 字符 print(file.readline().strip()) time.sleep(1) readlines()realines() 函数跟read()类似，会一次性读取所有内容，然后按行返回一个 list 对象。这种读取方式速度会比较快。但随着文本的增大，占用内存会越来越多。一般读取配置文件，可以使用这种方法。 1234567# -*- coding:utf-8 -*-import timewith open('one day.txt', 'r') as file: for line in file.readlines(): print(line.strip()) time.sleep(1)","link":"/767.html"},{"title":"爬取网易云音乐精彩评论","text":"(一)故事的小黄花从出生那年就飘着童年的荡秋千随记忆一直晃到现在 Re So So Si Do Si LaSo La Si Si Si Si La Si La So吹着前奏望着天空我想起花瓣试着掉落…… 小编猴哥有个爱好，喜欢一边听着熟悉的旋律，一边看着网易云音乐歌曲中的评论，特别是精彩评论。 评论内容，让人泫然流涕的故事，就是让人深思的段子。 （二）某天，猴哥突发奇想，想将自己平时喜欢听的歌曲的精彩评论爬取下来。以后就可以直接阅读这些评论，无须打开网页。 说干就干。猴哥打开浏览器访问网易云音乐，随便点击某个歌曲页面。现在大多数网站都采用 Ajax 技术来获取数据。所以需要先判断网页是否采用该技术。 有个谷歌浏览器插件名为 Toggle JavaScript，它能控制页面中 javascript 启用或者禁用。 正常的页面长这样： 当禁用页面 JavaScript 脚本之后，正常显示数据页面会变成一个空白页面。 因此，可以断定网易云音乐加载数据方式采用 Ajax。 Ajax 技术可以在不刷新页面的情况下，利用嵌在 HTML 文档中的 JavaScript 脚本向服务器请求数据，然后更新到页面。想进一步确认数据来源，需要知道请求域名以及请求参数。 这就需要借助浏览器的开发者工具(一般按 F12 键会显示)，根据抓取数据包进行分析。因为我们已经确定网站采用 Ajax ，所以直接在选择 XHR 过滤器过滤出所有请求。 然后依次对每个 url 链接的 HTTP 请求进行分析，着重观察 Headers，Preview 选项。最后，猴哥发现 R_SO_4_186001?csrf_token= 请求中有我们需要的信息。Preview 中有字段跟精彩评论中用户名一致。 继续切换到 Headers 确认请求域名以及请求需要携带的参数。 那么爬取思路是：使用 POST 方式携带参数 params 和 encSecKey 向该地址 http://music.163.com/weapi/v1/resource/comments/R_SO_4_186001?csrf_token= 发起HTTP 请求。返回结果中的 Json 数据就是用户评论数据。 （三）既然思路明确，编写代码就是容易多了。 这里，猴哥使用列表来保存想爬取精彩评论的歌曲。 1234567891011121314songs_url_list = [ 'http://music.163.com/#/song?id=186016', # 晴天 'http://music.163.com/#/song?id=186001', # 七里香 'http://music.163.com/#/song?id=27876900', # Here We Are Again 《喜剧之王》电影插曲 'http://music.163.com/#/song?id=439915614', # 刚好遇见你 'http://music.163.com/#/song?id=139774', # The truth that you leave 'http://music.163.com/#/song?id=29567189', # 理想 'http://music.163.com/#/song?id=308353', # 钟无艳 'http://music.163.com/#/song?id=31445772', # 理想三旬 'http://music.163.com/#/song?id=439915614', # 刚好遇见你 'http://music.163.com/#/song?id=28815250', # 平凡之路 'http://music.163.com/#/song?id=25706282', # 夜空中最亮的星 'http://music.163.com/#/song?id=436514312', # 成都] 然后截取每个链接中 id 字段的值。 1234def get_song_id(url): \"\"\" 从 url 中截取歌曲的 id \"\"\" song_id = url.split('=')[1] return song_id 接着根据 id 拼接处请求的 url 地址，再使用 requests 发起 HTTP 请求。 12345678910111213141516171819202122232425for each in songs_url_list: start_spider(get_song_id(each)) time.sleep(random.randint(5, 8))def start_spider(song_id): \"\"\" 评论数据采用 AJAX 技术获得, 下面才是获取评论的请求地址 \"\"\" url = 'http://music.163.com/weapi/v1/resource/comments/R_SO_4_{}?csrf_token='.format(song_id) headers = { 'User-agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.3964.2 Safari/537.36', 'Origin': 'http://music.163.com', 'Referer': 'http://music.163.com/song?id={}'.format(song_id), } formdata = { 'params': '57Wh2mgebLOOPQVBc+B2wz4sCCH/nXZFEoTc/XNySiqT0V7ZxUADzDNgTXXhYgAJ5BNMryMgxhdwNzF1GyxDZo3iR9/YYbWgCAQHC5DCDuObqvxNcOcnQDaRqJCrqQcrEABW1SwKitfbD3wMEyB4tJu+rU8goSwg2FP/PBBLs9DVs1iWdWGjV6CdrocA36Rs', 'encSecKey': '63774137ba4f5cc60d1b6a3bc14985a9563a7bfdec4f3e74297ffc07514adf18f90620933a01c2db4ca989cc4e1dfc49789981424c294a34e48c2cbe7aa51533a5cc5b5776a9e499cd08770bc596655dbe8e001d1ed5fd47a27dd195128480820cc67a799d341f95d447e3522851f2b64ad1cb8350e2015b265b9e684179351c', } response = requests.post(url, headers=headers, data=formdata) print('请求 [ ' + url + ' ], 状态码为 ') print(response.status_code) # get_hot_comments(response.text) # 将数据写到 CSV 文件中 write_to_file(get_hot_comments(response.text)) 因为请求返回结果是 Json 数据，我们只需要精彩评论(HotComments)内容，所以需要对数据进行处理下。 12345678910111213141516def get_hot_comments(response): \"\"\" 获取精彩评论 请求返回结果是 Json 数据格式, 使用 json.loads(response) 将其转化为字典类型, 就可以使用 key-value 形式获取值 \"\"\" data_list = [] data = {} for comment in json.loads(response)['hotComments']: data['userId'] = comment['user']['userId'] data['nickname'] = comment['user']['nickname'] data['content'] = comment['content'] data['likedCount'] = comment['likedCount'] data_list.append(data) data = {} # print(data_list) return data_list 最后将数据保存到 CSV 文件中。 123456789101112131415161718def write_to_file(datalist): print('开始将数据持久化……') file_name = '网易云音乐精彩评论.csv' with codecs.open(file_name, 'a+', 'GBK') as csvfile: filednames = ['用户Id', '昵称', '评论内容', '点赞数'] writer = csv.DictWriter(csvfile, fieldnames=filednames) writer.writeheader() for data in datalist: print(data) try: writer.writerow({filednames[0]: data['userId'], filednames[1]: data['nickname'], filednames[2]: data['content'], filednames[3]: data['likedCount']}) except UnicodeEncodeError: print(\"编码错误, 该数据无法写到文件中, 直接忽略该数据\") 写到这里，小伙伴们应该了解如何爬取运用 Ajax 技术加载数据的网站了。可能某些网站的请求携带的参数只能使用一次，那就进一步数据包中 js 代码。推断出加密方式，自己再用代码还原。 哈哈，这里请允许我贴下爬取结果。","link":"/659.html"},{"title":"分享一个爬取网站的小技巧","text":"有时候，我们很想爬取一个网站的数据。如果 PC 端的网页的反爬机制太强，我们可以换个思路。现在很多网站为了满足手机浏览器能正常访问的需求，都会推出手机版的网页。PC 端抓取数据有难度，我们可以从手机端入手。 你也许听说过，抓取手机 App 端数据就需要搭建手机抓包环境。那么我们就要屁颠屁颠去抓包搭建？哈哈，显然不用。我给大家分享一个小技巧，可以节省搭建环境的时间。 我们的抓取目标是 Web 手机端页面数据，而不是 App 端内的数据。因此，我们只要使用 PC 浏览器访问手机 Web 页面，就能继续使用 PC 浏览器进行抓包分析。 举个栗子，假如我要抓取淘宝首页的数据。我先用手机浏览器访问淘宝网站。 然后获取到手机端淘宝首页的 url 地址。 从图中，我们可知淘宝 web 手机端首页地址是：https://h5.m.taobao.com/。接着我们再用 PC 浏览器访问。 PC 端浏览器能正常访问，说明我们能使用浏览器自带的开发者工具来进行抓包分析。","link":"/768.html"},{"title":"搭建手机抓包环境","text":"智能手机的普及，这使得移动互联网成为另一个流量巨头。越来越多的公司也会转而直接提供 App 来展示内容。例如微信以及其生态（微信公众号、小程序）、抖音等。这也说明人们对于手机的依赖心更强。那抓取 App 的数据显得更加有意义。本文的主要内容是讲述如何搭建手机抓包环境。 工具准备 1）一台电脑最好是笔记本电脑，因为笔记本电脑内置了无限网卡。如果你使用的台式电脑，你需要自己购买无线网卡。可以不需要购买专业的抓包网卡， 购买那些 USB 迷你无线网卡就满足使用要求。如果你不清楚买什么网卡，可以选择小米随身 WIFI。 2）一台手机 3）抓包软件常用的抓包软件有 WireShark、Fiddler、Charles、miniproxy 等。我这里使用 Fiddler 作为实例进行讲解，他们抓包的原理基本相同。看个人喜欢选择抓包软件即可。 4）软 AP 软件现在市面上有很多结合网卡能创建出软AP(Soft-AP)软件。常见的软件有：猎豹免费 WIFI、360 免费 WIFI、WIFI 共享精灵。 原理分析1）对手机 App 进行抓包，究竟抓取什么内容？手机 App 离不开跟服务器交互。它们之间通过 HTTP/HTTPS 协议建立链接，然后大多数以 JSON 格式传输数据。对 App 抓包，抓的也是 HTTP/HTTPS 的包。 2）抓包软件原理是什么？抓包软件可以看成一个代理服务器，所有数据包都经过这个代理服务器转发。在转发过程中，代理服务器会把这些数据拦截下来。这就是我们看到的“抓包”。 3）是否需要用到软 AP 软件？这个跟你的网络有点关系。如果你的电脑和手机处于同一网段，那么就不需要使用软 AP 创建个 WIFI 网络。如果不在同一网段，就需要用到软 AP 软件。最常见的场景应该是：电脑使用的有线网络，手机使用的是无线网络。这两个设备很有可能不在同一网段。 4) 如果判断两个设备是否位于同一网段？网段是 IP 地址分类的概念。我们现在使用的 IP 地址还是 IPv4。一个 IP 地址大概长这个样子：192.168.1.110。一般情况下，如果两个设备的 IP 地址前三位都是一样的话，就算是位于同一网段。我的电脑的 IP 地址是： 我的手机的 IP 地址是： 上面两张图说明位于同一网段。如果两个设备不在同一网段，使用电脑开启个软 AP，然后将手机连接到这个 WIFI 网络。这样他们就处于同一网段了。 搭建环境接下来，我按照两个设备不在同一网段的情况进行讲解。因为这种情况遇到的机率非常高。 首先，先下载猎豹免费 WIFI。安装成功之后，开启 WIFI， 并让手机连接到刚才创建的 WIFI。 然后到 Fiddler 的官网下载安装包。Fiddler 官网毕竟是国外网站，访问有时候很慢。如果下载不下来，可以到国内的软件商城下载。安装成功，打开 Fiddler 软件，我们会看到以下界面。左边是抓取到的数据包，右边是数据包分析界面。 接着将“是否允许远程设备连”选项开启。Tools -&gt; Fiddler Options -&gt; Connections，勾选”Allow remote computers to connect”。 另外，我们可以看到 Fildder 监听着的 8888 端口号。这个是手机设置代理的端口号。在手机的连接的 WIFI 的设置中配置代理。代理 IP 地址是电脑的 IP 地址，端口就是刚才设定的 8888 端口。这里要注意的是，要先确保电脑和手机处于同一网段。 我们使用软 AP 创建 WIFI，这保证电脑和手机处于同一网络。IP 地址要重新查看一遍。 再将刚才的获取的 IP 地址和端口填写到 WIFI 的代理配置中。 现在谷歌和苹果都强推使用 HTTPS 协议，所以我们也要配置下抓取 HTTPS 的包。 最后，用手机浏览器访问代理服务器地址安装证书。根据上图，我访问的是 192.168.192.2:8888, 浏览器会提示是否安装证书。安装成功之后，即可对手机 App 进行抓包。","link":"/770.html"},{"title":"这些抓包工具，你值得拥有","text":"如今的时代是互联网时代，互联网已经在我们的生活如影随形。可以说我们无时无刻在跟互联网打交道。而在工作，我们可能会因开发调试、测试、排查网络故障等原因，需要对网路数据包进行抓取、拦截以解析。因此，本文主要内容是推荐几款不错的抓包工具。 FiddlerFiddler 是一个使用 C# 编写的 http 抓包工具。它使用灵活，功能强大，支持众多的 http 调试任务，是 web、移动应用的开发调试利器。所以 Fiddler 经常被运用在网络爬虫抓包、HTTP API 测试、手机抓包等场景。 传输门: 下载地址 推荐原因： 操作简单，上手容易，学习成本低。 能够抓取 HTTP/HTTPS 协议的数据包。 支持伪造 CA 证书来欺骗浏览器和服务器，从而实现解密 HTTPS 数据包。 不仅支持抓取 PC 浏览器的数据包，而且支持抓取手机数据包。 支持设置“断点”，从而能够修改 HTTP 的请求头信息以及请求体的数据。 CharlesFiddler 虽然强大且好用，但是不支持 Mac OS 系统。Charles 是 Fiddler 在 Mac 系统上的代替品。目前 Charles 算是 Mac 系统上最好用的抓包工具。它使用 Java 语言开发的，所以安装以及使用之前，要事先安装好 Java 环境。另外，它还支持 Windows、Linux 等操作系统。 不过 Charles 是一款收费的软件，当 30 天的免费体验期过后，就需要花费 30 刀购买一个 License。 值得庆幸的是国内有个名为 zzzmode 的大神提供了破解方法。破解版下载地址：传输门: 下载地址 推荐原因： 同样操作简单，使用方便。 支持捕获 HTTP/HTTPS 的数据包 支持修改网络请求参数 支持截获网络请求并动态修改 支持流量控制。可以模拟慢速网络以及等待时间（latency）较长的请求。 支持AJAX调试。可以自动将json或xml数据格式化，方便查看。 AppiumAppium 是移动端自动化测试框架。它跟 Selenium 有点类似，使用驱动程序在 Android、iOS 设备上执行模拟点击、滑动等操作。 Appium 的核心是一个公开 REST API 的 Web Server。它负责监听来自 Client 的连线与指令，并且把执行结果以 HTTP 状态的方式回应。目前安装 Appium 可以通过 Appium Desktop来安装。 传输门: 下载地址 推荐原因：可以使用 Appium 模拟 App 的操作，从而到达爬取 App 加密数据的目的。 WiresharkWireshark 是一款非常优秀、支持 Unix 和 Windows 平台的网络协议分析工具。它可以监听电脑的网卡所有的数据包，实现实时检测网络通讯数据以及获取详细的封包指令。它是运维工程师、网络安全工程师的必备工具。运维工程师可以用其排查网络问题。网络安全工程师可以利用其监控 TCP 网络动态、分析 DDos 数据等。 推荐原因：Wireshark 能抓取网卡所有的数据包。这也说明其能抓取 ISO 模型中除了物理层之外的协议数据包。因此，它是我们学习计算机网络知识的好帮手。不过，它功能比较多，需要一定时间成本。","link":"/765.html"},{"title":"618 购物节买什么？当然是书","text":"时光荏苒，2018年已经过半。又到一年一度的的”618”购物节，是时候来一波”买买买”。如果你目前想改变下自己，为自己的未来奋斗。可以少买一两件衣服，少买一两箱零食，腾出一两百块钱来买几本书。 当当，京东正好有图书满减活动。当当网的优惠还是相当大的，满200减100，满400减200，等同打 5 折。如果书籍满足满减要求，就能优惠。 我个人也是买了一些书籍，坚持持续学习。 我推荐一些个人觉得不错的书籍。这些书籍基本上要么已经购买了，要么已经囤在购物车中。另外，大部分书籍都是图灵图书推荐过。 Python 编程入门比较适合零基础，想入门的小伙伴。书籍用到的都是 Python 3.x 版本 《笨办法学 Python》 《Python 基础教程（第3版·修订版）》 《Head First Python (第2版)》 《Python 编程：入门到实践》 Python 开发进阶如果小伙伴已经把基础知识学得差不多了，想进一步提高自己的能力。可以阅读进阶书籍。同样，书籍用到的都是 Python 3.x 版本。 《Python 项目开发实战（第2版）》 《精通 Python 设计模式》 《Python 核心编程第3版》 《Python 源码剖析》 《Python 学习手册（第4版）》 《Python 3 网络爬虫开发实战》 数据分析单独把数据分析拎出来说，是因为数据分析挺有分量的。 《Python 数据科学手册》 算法通过阅读代码方式来学习算法是一个枯燥，痛苦的过程。因为需要很强的想象能力。我们换一种方式来，通过图片形式来学习算法或许更有有趣些。 《算法图解》 数据库后台开发基本上跟数据库打交道，所以数据库知识挺重要的。同时，MySQL 数据库是使用最广泛的数据库。推荐一本 MySQL 基础的书籍。 《MySQL必知必会》 计算机网络这本书很详细，很系统地阐述网络基础知识。可以算是一本很棒的计算机网络科普读物。 《网络是怎样连接的》 另外还有感谢业内人称“良心张”，“帅张”的 stormzhang提供了当当的优惠券。张哥的优惠券可以和官方叠加使用。两种优惠码： 在官方满 200-100 的基础上再减 20，简单来说就是满 200-120，算下来就是 80/200 = 4 折。 优惠码： Q8GP50 在满 400-200 的基础上，再减 50，简单来说就是满 400-250，算下来就是 150/400 = 3.75 折。 优惠码： Q3MG53 注意：1）以上优惠码一个用户只能用一个。2）优惠码只支持『计算机类书籍』，其他书籍不支持。如果你出现『没有使用此优惠码的商品』，那书单你一定是有不合规的书。3）具体使用是在当当页面结算之后，可以输入优惠码兑换。","link":"/558.html"},{"title":"想提高爬虫效率？aiohttp 了解下","text":"对于爬虫程序，我们往往会很关注其爬虫效率。影响爬虫效率有几个因素有，是否使用多线程，I/O 操作，是否同步执行等。其中 I/O 操作、同步执行是最影响爬虫效率的。 众所周知，Requests 库一个优秀的 HTTP 库，通过它可以非常简单地发起 HTTP 请求。不过，这个库所执行的网络请求都是同步。当爬虫程序进程获得 CPU 的时间片时，如果程序在进行 I/O 操作（例下载图片），在这段 IO 执行的时间里，CPU 处于空闲中，这样会造成 CPU 的计算能力就被浪费了。 如果 CPU 能将等待时间利用起来，那么爬虫效率就提高了。那就需要对程序进行改造，将 I/O 同步操作变成异步操作。本文内容是介绍一个强大的异步 I/O 操作的库 —— aiohttp。 aiohttp 介绍说到 aiohttp ，不得不说下 asyncio 。asyncio 是 Python 3.4 版本引入的标准库。它工作模式是单线程并发，使用协同执行 I/O 操作。asyncio 的编程模型就是一个消息循环。我们从 asyncio 模块中直接获取一个 EventLoop 的引用，然后把需要执行的协程扔到 EventLoop 中执行，就实现了异步 IO。 使用 asyncio 实现一个异步函数 hello() 的例子: 1234567891011121314import asyncio@asyncio.coroutine # 修饰符，等同于 asyncio.coroutine(hello())def hello(): print(\"Hello world!\") # 异步调用asyncio.sleep(1): r = yield from asyncio.sleep(1) print(\"Hello again!\")# 获取EventLoop:loop = asyncio.get_event_loop()# 执行coroutineloop.run_until_complete(hello())loop.close() 而 aiohttp 则是基于 asyncio 实现的 HTTP 框架。 aiohttp 全称是 Async http client/server framework。翻译成中文是异步 HTTP 的客户端/服务器框架。从名字中，我们可知 aiohttp 是分为服务器端和客户端，专门异步处理 HTTP 的请求。 aiohttp 安装安装 aiohttp 可以通过 pip 方式安装，在终端中执行安装命令即可。 1pip install aiohttp async/await 语法前面我们讲到异步 I/O 的用法，但是声明异步函数比较繁琐，还需要依赖 yield 语法。在 Python 3.5 中，引入了 async/await 关键字，使得异步回调的写法更加直观和人性化。 在函数 def 之前增加关键字async,表示这个函数是异步函数。相当于替代语法@asyncio.coroutine。具体例子例如： 12async def hello(): print(\"Hello World!\") 另外使用 await 替换了 yield from, 表示这部分操作为异步操作。 1234async def hello(): print(\"Hello World!\") r = await asyncio.sleep(1) print(\"Hello again!\") 最后执行异步函数，还是需要用到 EventLoop 引用，然后利用协程执行异步函数。最终的代码如下： 123456789101112import asyncioasync def hello(): print(\"Hello world!\") r = await asyncio.sleep(1) print(\"Hello again!\")if __name__ == '__main__': loop = asyncio.get_event_loop() tasks = [hello(), ] loop.run_until_complete(asyncio.wait(tasks)) loop.close() 运行结果如下： 123Hello world!&gt;&gt; 会暂停一秒钟Hello again! aiohttp 基本用法我们使用 aiohttp 以 GET 方式向httpbin.org网站发起一个 HTTP 请求。因为是 aiohttp 是异步处理 HTTP 请求。所以还必须遵循 Python 的异步函数语法，即需使用 async/await 语法。 使用 aiohttp 发起一个 HTTP 请求，具体编写可以分为以下几步：1）使用 async 定义异步函数2）通过 aiohttp.ClientSession 获取一个 session 对象3）用该 session 对象以 GET、POST、PUT 等方式去请求网页4）最后获取 EventLoop 引用，执行异步函数。 12345678910111213import asyncioimport aiohttp# 定义异步函数 main()async def main(): # 获取 session 对象 async with aiohttp.ClientSession() as session: # get 方式请求 httbin async with session.get('http://httpbin.org/get') as response: print(response.status) print(await response.text())loop = asyncio.get_event_loop()loop.run_until_complete(main()) aiohttp 支持自定义 headers、设置超时时间、设置代理、自定义 cookie 等。 1234567891011121314151617181920212223import asyncioimport aiohttpurl = 'http://httpbin.org/post'headers = { 'User-agent': \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\",}data = { 'data': 'person data',}# 定义异步函数 main()async def main(): # 获取 session 对象 async with aiohttp.ClientSession() as session: # post 方式请求 httbin async with session.post(url=url, headers=headers, data=data) as response: print(response.status) print(await response.text())loop = asyncio.get_event_loop()loop.run_until_complete(main()) 关于 aiohttp 更多用法，可以执行阅读官网文档。说句实话，aiohttp 跟 Requests 的用法大同小异。如果你已经学会了 Requests 库，很快就能掌握 aiohttp 的用法。","link":"/871.html"},{"title":"Django 2.1 版本已经来袭","text":"一个备受关注、很多人都在使用的软件，软件开发团队会定期发布新版本。其主要目的是修复之前一些 Bug 以及新增一些新特性。Django 作为主流的 Python Web 开发框架，当然也不例外。在几天前，即 8 月 1 号，Django 官方团队发布 Django 2.1 版本。 新版本发布，我们可以不用急于更新替换，但要关注新版本的发布内容。我看了下官方的发布公告，总结几个重要的点。 Python 版本支持Django 2.1 将不再支持 Python 3.4，最低要求 Python 版本是 3.5。这也意味着 Django 2.0 是最后一个支持 Python 3.4 的版本。 模型新增查看权限这个功能，很多开发者都期待了很久。盼星星盼月亮，终于盼来了。 在以前的版本中，模型（Model）只有“增删改”权限。新版本增加查看（View）权限。这也意味着在 Model 的 Meta 配置选项中， default_permissions 属性默认值变为 (‘add’, ‘change’, ‘delete’, ‘view’)。 此外，官方团队考虑到向前兼容性。在旧版本中，如果用户取得修改（change）权限。更新到新版本，用户会自动获得查看（View）权限。 View 权限同样也适用于 admin，它可以给用户指定只读权限。这里是通过 ModelAdmin.has_view_permission(request, obj=None) 来设置。如果希望一个 obj 被设置成只读属性，那么要返回 True。 模型新版本的 Model 类也一些特性，我也列举出比较重要的点。 Model 模型类支持 init_subclass 重写 查询表达式终于支持负号查询了。 在模型的表单中，BinaryField 可以被设置为 editable=True。 表单表单（Forms）中的 ImageField 组件新增 accept=”image/*” 属性。 缓存基于内存缓存（local-memory cache backend）采用最近最少使用（LRU）筛选策略，以替换之前随机数的策略。 数据库版本支持 MySQL 数据库Django 2.1 不再支持 MySQL 5.5 以及以下版本，只能选择 5.6 或者更高版本。 PostgreSQL 数据库Django 2.1 支持 PostgreSQL 最低版本为 9.4. SQLite 数据库新版本不再支持 &lt; 3.7.15 版本的 SQLite。","link":"/872.html"},{"title":"使用 Python 生成二维码","text":"新时代，人们有人信新的追求，自然而然会有新发明的诞生。去年，在“一带一路”国际合作高峰论坛举行期间， 20 国青年投票选出中国的“新四大发明”：高铁、扫码支付、共享单车和网购。其中扫码支付指手机通过扫描二维码跳转到支付页面，再进行付款。这种新的支付方式，造就二维码满天飞的现象。那么让我们来扒一扒如何使用 Python 来生成二维码图片。 二维码二维码（2-dimensional bar code），是用某种特定的几何图形按一定规律在平面（二维方向上）分布的黑白相间的图形记录数据符号信息的。它能将数字、英文字母、汉字、日文字母、特殊符号(如空格，%，/ 等)、二进制等信息记录到一个正方形的图片中。 因此，在转换的过程中，离不开编码压缩方式。在许多种类的二维条码中，常用的码制有：Data Matrix, Maxi Code, Aztec, QR Code, Vericode, PDF417, Ultracode, Code 49, Code 16K等。 二维码在现实生活中的应用越来与普遍，归于功于 QR code 码制的流行。我们常说的二维码就是它。所以，二维码又被称为 QR code。 QR code 是一种矩阵式二维条码（又称棋盘式二维条码）。它是在一个矩形空间通过黑、白像素在矩阵中的不同分布进行编码。在矩阵相应元素位置上，用点（方点、圆点或其他形状）的出现表示二进制“1”，点的不出现表示二进制的“0”，点的排列组合确定了矩阵式二维条码所代表的意义。 二维码结构我们的目的是要使用 Python 生成 QR 码，那我们需要先了解二维码(QR 码)的结构。根据标准（ISO/IEC 18004），我们可以了解到 QR 码结构如下： 1) 功能图形功能图形是不参与编码数据的区域。它包含空白区、位置探测图形、位置探测图形分隔符、定位图形、校正图形五大模块。 空白区：空白区顾名思义就是要留空白。因此，这里不能有任何图样或标记。这样才能保证 QR 能被识别。 位置探测图形：这个有点类似中文的“回”字。在 QR 码中有个这样的标识，它分别的左上、右上和左下角。作用是协助扫描软件定位 QR 码并转换坐标系。我们在扫描二维码的时候，不管是竖着扫、横着扫、斜着扫都能识别出内容，主要是它的功劳。 位置探测图形分隔符：主要作用是区分功能图形和编码区域。 定位图形：它由黑白间隔的各自各自组成的线条。主要用于指示标识密度和确定坐标系。原因是 QR 码一种有 40 个版本，也就是说有 40 种尺寸。每种二维码的尺寸越大，扫描的距离就越远。 校正图形：只有 Version 2 及以上的QR码有校正标识。校正标识用于进一步校正坐标系。 2) 编码区域编码区域是数据进行编码存储的区域。它由格式信息、版本信息、数据和纠错码字三部分构成。 格式信息： 所有尺寸的二维码都有该信息。它存放一些格式化数据的信息，例如容错级别、数据掩码，和额外的自身 BCH 容错码。 版本信息：版本信息是规定二维码的规格。前面讲到 QR 码一共有 40 种规格的矩阵（一般为黑白色），从21x21（版本1），到177x177（版本40），每一版本符号比前一版本 每边增加4个模块。 数据和纠错码：主要是存储实际数据以及用于纠错码字。 二维码的绘制过程二维码已经是有一套国际标准，绘制二维码过程的严格按照标准来执行。这个过程是比较复杂，我自己也是看了大概，然后总结出大致绘制过程。如果你想深入了解绘制细节，可以阅读标准。 二维码的绘制大概过程如下：1）在二维码的左上角、左下角、右上角绘制位置探测图形。位置探测图形一定是一个 7x7 的矩阵。2）绘制校正图形。校正图形一定是一个 5x5 的矩阵。3）绘制两条连接三个位置探测图形的定位图形。4）在上述图片的基础上，继续绘制格式信息。5）接着绘制版本信息。6）填充数据码和纠错码到二维码图中。7）最后是绘制蒙版图案。因为按照上述方式填充内容，可能会出现大面积的空白或黑块的情况，导致扫描识别会十分困难。所以需要对整个图像与蒙版进行蒙版操作(Masking)，蒙版操作即为异或 XOR 操作。在这一步，我们可以将数据排列成各种图片。 二维码的生成我们既然已经了解二维码原理，那么可以利用 Python 生成二维码。然而网络上高人比比皆是。已经有大神编写了 Python 生成二维码的第三方库，所以我们不需要重复造轮子, 使用现成的库即可。 我就推荐两个库：qrcode 和 python-qrcode。 qrcode qrcode 运行在 Python 3 版本上，它可以玩出很多花样。例如能生成以下三种二维码图片：普通二维码、带图片的艺术二维码（黑白与彩色）、动态二维码（黑白与彩色）。它比较适合直接用于生成二维码图片的场景。 安装 qrcode 库可以使用 pip 方式。但是该库依赖 pillow、numpy 和 imageio。因此，我们需要先安装依赖库，再安装 qrcode。最后的安装命令如下： 12345# 逐一安装pip install pillowpip install numpypip install imageiopip install myqr 该库生成带图片的艺术二维码算是一大亮点，具体用法如下: 1myqr https://github.com -p github.jpg -c 上述命令作用是将 github 主页写到彩色二维码中。 该库还支持生成 gif 的彩色二维码图片，具体用法如下： 1myqr https://github.com -p github.gif -c -con 1.5 -bri 1.6 效果图如下： 最后补上该库的 Github 仓库地址 python-qrcode python-qrcode 相比 qrcode 要稍微逊色一点。不过它也有自己的特色。它支持生成矢量图，而且比较适合在代码中生成二维码的场景。 安装 python-qrcode 同样建议使用 pip 方式，安装命令如下： 1pip install qrcode 在 Python 代码中，最简单的用法是这样。 12import qrcodeimg = qrcode.make('https://github.com') 它也支持自定义二维码的信息，具体用法如下： 1234567891011import qrcodeqr = qrcode.QRCode( version=1, error_correction=qrcode.constants.ERROR_CORRECT_L, box_size=10, border=4,)qr.add_data('https://github.com')qr.make(fit=True)img = qr.make_image(fill_color=\"black\", back_color=\"white\") 如果你想深入了解该库，可以到 Github 仓库阅读相关的文档。","link":"/874.html"},{"title":"程序员该如何过七夕？","text":"说的七夕节，我们都会不由自主地想起牛郎织女的传说。这是一个美丽且千古流传的爱情故事。它也是我们四大民间爱情传说之一。 传说是牛郎和织女之间情深意重，后来被王母娘娘强行拆散。所以牛郎和织女只等到每年农历七月初七，走上由成千上万的喜鹊组成的桥才得以相会。 但不知从哪时起，七夕被人们普遍认为是情人节。但在中国历史上是没有这回事。中国的传统情人节是元宵节(也称为上元节)。在古代，平常女子特别是大家闺秀或小家碧玉，都是”三步不出闺门”。只有到了元宵节，才能走出家门，才有机会和男生幽会谈情。而七夕节又称乞巧节，即女子绣针线活的节日。目的是向织女乞求智慧和巧艺。 另外，古代很多文人都是在七夕抒发哀怨之情。例如李清照的《行香子·草际鸣蛩》中写得:星桥鹊驾，经年才见，想离情、别恨难穷。牵牛织女，莫是离中。甚霎儿晴，霎儿雨，霎儿风。 又如白居易的《七夕·烟霄微月澹长空》:烟霄微月澹长空，银汉秋期万古同。几许欢情与离恨，年年并在此宵中。 我们中国人向来很注重仪式感。既然七夕节是情人节这一说法已经深入人心。那我们也随着大流过，不然会显得跟别人格格不入。 那么重点来了，七夕如何过呢？猴哥在这里给一个良心建议。 如果你是有女朋友或者有追求目标的男生，今天你需要改变自己的形象，把打扮成男神模样。可以把之前的格子衬衫，牛仔裤和运动鞋甩到一边。然后上半身换上一件纯色衬衫或 V 领 POLO 衫，再搭配一条西裤或者休闲裤，双脚穿上一双布洛克鞋或者懒人鞋。最后妹子约出来，和她一起度过一个浪费的七夕。 如果你跟我一样是单身，还是带上耳机一边听《单身情歌》，一边默默地加班。","link":"/875.html"},{"title":"Python 面试宝典","text":"步入 9 月，徐徐的秋风给酷热的天气带来丝丝凉意。同时，也吹来一股招聘高潮。俗话说“金九银十”，每年的 9、10 月都是招聘高潮。有些小伙伴会参加秋招，有些小伙伴会选择跳槽。猴哥特意给大家送上一份 Python 面试的资料，资料都来源于 github。 interview_python该仓库提供的面试题目还是挺全面的，涉及到 Python 语言特性、操作系统、数据库、网络协议、场景面试算法题目。同时，该仓库收集的面试题目也是很有深度。举个栗子，在 Python 语言特性中，提到 Python 的闭包、迭代器和生成器、垃圾回收机制、面向切面编程 AOP 和装饰器等高级用法。如果有小伙伴的算法知识比较薄弱，可以通过做题方式来理解。 点击☞☞☞Github 仓库地址 Algorithm_Interview_Notes-Chinese近两年来，机器学习算是 IT 行业的热点。有热点就有人才需求。如果有小伙伴要面试机器学习的岗位。可以关注下这个仓库。这个仓库主要收集算法、NLP、深度学习、机器学习面试笔记。 点击☞☞☞Github 仓库地址 system-design-primer俗话说的好，人往高处走水往低处流。我们不能每天只埋头工作工作。平时也要注重学习新的知识。如果你想成为一名高级工程师（大牛），那么要熟悉系统架构的知识，能够参与系统架构的设计和开发。该仓库的主要讲述一些常见的系统架构设计以及解决方案，内容偏向于后台开发的知识。 点击☞☞☞Github 仓库地址","link":"/877.html"},{"title":"盘点一些网站的反爬虫机制","text":"因为 Python 语法简介以及强大的第三方库，所以我们使用它来制作网络爬虫程序。网络爬虫的用途是进行数据采集，也就是将互联网中的数据采集过来。 网络爬虫的难点其实并不在于爬虫本身。而是网站方为了避免数据被爬取，增加了各种各样的反爬虫措施。如果想要继续从网站爬取数据就必须绕过这些措施。因此，网络爬虫的难点在于反爬的攻克和处理。那么本文主要介绍一些网站的反爬虫措施。 妹子图这个网站的反爬虫机制比较简单。当我们使用网络请求库下载图片时，该网站会对检查每个 HTTP 请求的 headers 头部中 Referer 字段。它判断该字段是否为空，如果字段为空，那么不会返回正常显示的图片，而是返回一张带有“图片来自妹子网，请勿盗链”字样的图片。 遇到这种机制，突破也是比较简单。对每个 HTTP 请求，将页面的 url 地址填充到 Referer 字段中。 豆瓣几乎所有的爬虫新手都会爬取豆瓣练练手。但是豆瓣还是保持开放的态度，反爬虫机制做得还是很人性化。它的反爬虫机制大概如下： 1、在没有携带 cookie 的情况下，如果某个 IP 短时间高并发请求网站，该 IP 会立马被封。当 IP 被封，登录豆瓣网站会解封。2、在携带 cookie 的情况下，某个 IP 请求网站过于频繁。豆瓣的反爬虫机制变为只封 cookie 不封 IP。也就说退出登录或者换个账号还能继续访问网站。 面对这么体谅新手的网站，我们要下手不能那么猛。我们只要在代码中登录账号，同时降低并发数，再随机延迟等待一段时间。我们的爬虫程序就不会被封杀了。 拉勾网拉勾网站刚出来的时候，反爬虫机制还没有现在这么严格。估计爬取网站的人多了起来，网站管理员为了保护服务器增加一些手段。该网站的反爬虫机制大概是这样子。 1、在没有登录的情况下，程序只能连续访问 3 个 Url。如果再继续访问，网站会将链接重定向，然后提示我们登录。2、如果在登录情况下，连续请求部分 url 之后，我们的 IP 会被封。 针对这样的爬虫机制，我们只能使用 IP 代理池来突破。 汽车之家汽车之家论坛的反爬虫机制就比较高级。它利用前端页面自定义字体的方式来实现反爬的技术手段。具体使用到是 CSS3 中的自定义字体(@font-face)模块，自定义字体主要是实现将自定义的 Web 字体嵌入到指定网页中去。这就导致我们去爬取论坛帖子的口碑时，获取到的返回文本中每隔几个字就出现一个乱码符号。 每次访问论坛页面，其中字体是不变的，但字符编码是变化的。因此，我们需要根据每次访问动态解析字体文件。 具体可以先访问需要爬取的页面，获取字体文件的动态访问地址并下载字体，读取 js 渲染后的文本内容，替换其中的自定义字体编码为实际文本编码，就可复原网页为页面所见内容了。 最后说句良心话，我们爬取别人网站的数据，要在不损害别人网站的情况下进行。所以建议大家不要在网站访问高峰的时候爬取数据，尽量选择在晚上进行爬取。同时设置延时操作降低并发数。","link":"/876.html"},{"title":"带你了解代理 IP 那些事","text":"因为 Python 语法简介以及强大的第三方库，所以我们使用它来制作网络爬虫程序。网络爬虫的用途是进行数据采集，也就是将互联网中的数据采集过来。 在爬取某些网站时，我们经常会设置代理 IP 来避免爬虫程序被封。我们获取代理 IP 地址方式通常提取国内的知名 IP 代理商（如西刺代理，快代理，无忧代理等）的免费代理。这些代理商一般都会提供透明代理，匿名代理，高匿代理。那么这几种代理的区别是什么？我们该如何选择呢？本文的主要内容是讲解各种代理 IP 背后的原理。 代理类型代理类型一共能分为四种。除了前面提到的透明代理，匿名代理，高匿代理，还有混淆代理。从安全程度来说，这四种代理类型的排序是高匿 &gt; 混淆 &gt; 匿名 &gt; 透明。 代理原理代理类型主要取决于代理服务器端的配置。不同配置会形成不同的代理类型。在配置中，这三个变量 REMOTE_ADDR，HTTP_VIA，HTTP_X_FORWARDED_FOR 是决定性因素。 1) REMOTE_ADDRREMOTE_ADDR 表示客户端的 IP，但是它的值不是由客户端提供的，而是服务器根据客户端的 IP 指定的。 如果使用浏览器直接访问某个网站，那么网站的 web 服务器（Nginx、Apache等）就会把 REMOTE_ADDR 设为客户端的 IP 地址。 如果我们给浏览器设置代理，我们访问目标网站的请求会先经过代理服务器，然后由代理服务器将请求转化到目标网站。那么网站的 web 服务器就会把 REMOTE_ADDR 设为代理服务器的 IP。 2）X-Forwarded-For（XFF）X-Forwarded-For 是一个 HTTP 扩展头部，用来表示 HTTP 请求端真实 IP。当客户端使用了代理时，web 服务器就不知道客户端的真实 IP 地址。为了避免这个情况，代理服务器通常会增加一个 X-Forwarded-For 的头信息，把客户端的 IP 添加到头信息里面。 X-Forwarded-For 请求头格式如下： 1X-Forwarded-For: client, proxy1, proxy2 client 表示客户端的 IP 地址；proxy1 是离服务端最远的设备 IP; proxy2 是次级代理设备的 IP；从格式中，可以看出从 client 到 server 是可以有多层代理的。 如果一个 HTTP 请求到达服务器之前，经过了三个代理 Proxy1、Proxy2、Proxy3，IP 分别为 IP1、IP2、IP3，用户真实 IP 为 IP0，那么按照 XFF 标准，服务端最终会收到以下信息： 1X-Forwarded-For: IP0, IP1, IP2 Proxy3 直连服务器，它会给 XFF 追加 IP2，表示它是在帮 Proxy2 转发请求。列表中并没有 IP3，IP3 可以在服务端通过 Remote Address 字段获得。我们知道 HTTP 连接基于 TCP 连接，HTTP 协议中没有 IP 的概念，Remote Address 来自 TCP 连接，表示与服务端建立 TCP 连接的设备 IP，在这个例子里就是 IP3。 3）HTTP_VIAvia 是 HTTP 协议里面的一个header,记录了一次 HTTP 请求所经过的代理和网关，经过1个代理服务器，就添加一个代理服务器的信息，经过2个就添加2个。 代理类型区别1) 透明代理(Transparent Proxy)代理服务器的配置如下： 123REMOTE_ADDR = Proxy IPHTTP_VIA = Proxy IPHTTP_X_FORWARDED_FOR = Your IP 透明代理虽然可以直接“隐藏”客户端的 IP 地址，但是还是可以从HTTP_X_FORWARDED_FOR来查到客户端的 IP 地址。 2） 匿名代理(Anonymous Proxy)代理服务器的配置如下： 123REMOTE_ADDR = proxy IPHTTP_VIA = proxy IPHTTP_X_FORWARDED_FOR = proxy IP 匿名代理能提供隐藏客户端 IP 地址的功能。使用匿名代理，服务器能知道客户端使用用了代理，当无法知道客户端真实 IP 地址。 3） 混淆代理(Distorting Proxy)代理服务器的配置如下： 123REMOTE_ADDR = Proxy IPHTTP_VIA = Proxy IPHTTP_X_FORWARDED_FOR = Random IP address 与匿名代理的原理相似，但是会伪装得更逼真。如果客户端使用了混淆代理，服务器还是能知道客户端在使用代理，但是会得到一个假的客户端 IP 地址。 2） 高匿代理(Elite Proxy 或 High Anonymity Proxy)代理服务器的配置如下： 123REMOTE_ADDR = Proxy IPHTTP_VIA = not determinedHTTP_X_FORWARDED_FOR = not determined 高匿代理既能让服务器不清楚客户端是否在使用代理，也能保证服务器获取不到客户端的真实 IP 地址。 代理的选择普通匿名代理能隐藏客户机的真实 IP，但会改变我们的请求信息，服务器端有可能会认为我们使用了代理。不过使用此种代理时，虽然被访问的网站不能知道客户端的 IP 地址，但仍然可以知道你在使用代理，当然某些能够侦测 IP 的网页仍然可以查到客户端的 IP。 而高度匿名代理不改变客户机的请求，这样在服务器看来就像有个真正的客户浏览器在访问它，这时客户的真实IP是隐藏的，服务器端不会认为我们使用了代理。 因此，爬虫程序需要使用到代理 IP 时，尽量选择普通匿名代理和高匿名代理。另外，如果要保证数据不被代理服务器知道，推荐使用 HTTPS 协议的代理。 文章参考：HTTP 请求头中的 X-Forwarded-Forproxy代理类型:透明代理 匿名代理 混淆代理和高匿代理","link":"/978.html"},{"title":"中秋佳节，不妨读一本好书","text":"时光荏苒，又值中秋佳节。因为中秋节是我国重要的传统节日，又是非常古老的节日。所以它包括许多民俗文化活动。例如：祭祀月亮以怀念嫦娥, 合家团聚吃月饼，赏月等。 我的故乡——潮汕还是能很好传承古代的文化。这得益于两方面：一方面，潮汕人的祖先是北方的大家族，他们为了躲避常年的战乱而来这里栖息；另一方面，潮汕地区的地理位置三面环山，南面朝海，交通不发达。韩愈被贬潮州，曾总结过：“潮之州，大海在其南，群山拥其北”。 因此，我们会在这中秋月圆之夜，吃月饼，赏月等，当然还有重要的节日：听老一辈的讲古。我母亲年轻时喜欢读书，加上平时也喜欢听电台的文学节目。所以我母亲经常是主讲人。她会给我们讲成语典故、唐宋时期的民间故事、明朝的史学等。在其中，最让我难忘的还是红楼梦的为人处事之道。因为这两句诗世事洞明皆学问，人情练达即文章。教会我很多。现在我已经毕业工作，深深体会到会做人做事的重要性。这真的能事半功倍。所以，我到现在还是会去阅读这方面的书籍。在这里，我也分享几本好书。 《唐浩明评点曾国藩家书（上下册）》曾国藩是一个资质平平的人，然后通过自我意志的“刻意训练”，成就阔大的事业。曾国藩是晚清儒学的集大成者，用尽一生精力来实践儒学所提倡的道德理想。他留给我们的宝贵资料是他的家书系列，这其中体验出他的为人处世之道。 学诚法师的《好好说话》无论生活还是工作，沟通是与人交往的重要途径。一句简单的话可能伤害别人，甚至造成很大的后果。一句简单的话也可能给人带来一时、一天乃至一声的帮助。这体现出说话技巧的重要性。这本书能让我们在知道在什么时候该说什么话，以及怎么说。","link":"/979.html"},{"title":"高并发的那些事","text":"“高并发”对后台开发同学来说，既熟悉又陌生。熟悉是因为面试和工作经常会提及它。陌生的原由是服务器因高并发导致出现各位问题的情况少之又少。同时，想收获这方面的经验也是”摸着石头过河”， 需要大量学习理论知识，再去探索。 如果是客户端开发的同学，字典中是没有“高并发”这个名词。这验证一句老话，”隔行如隔山”。客户端开发，特别是手机应用开发，更多地是考虑如何优化应用的性能，降低 App 的卡顿率等。 本文是一篇科普文，分享自己近来学到的知识。 什么是高并发？由于分布式系统的问世，高并发（High Concurrency）通常是指通过设计保证系统能够同时并行处理很多请求。通俗来讲，高并发是指在同一个时间点，有很多用户同时的访问同一 API 接口或者 Url 地址。它经常会发生在有大活跃用户量，用户高聚集的业务场景中。 其实，高并发也离我们的生活并不遥远，例如大学学校的选课系统。一到选课的时候，一大批学生同时选课，导致系统出现“不良反应”；再如淘宝的 618 和 双 11 的购物活动；遇到节假日，12306 上演的“抢票大战”。另外，DDos 攻击也能算高并发的场景。 高并发会来带的后果 服务端： 高并发会导致站点服务器/DB服务器资源被占满崩溃，甚至出现服务器宕机的情况；数据的存储不完整，数据更新异常问题。 用户端： 服务端的问题是高并发的直接反馈，而客户端是间隔反馈。它反馈给用户情况是糟糕的体验。 提高系统并发能力的方式在这个“云”的时代，提高分布式系统并发能力的方式，方法论上主要有两种：垂直扩展（Scale Up）与水平扩展（Scale Out）。 1) 垂直扩展提升单机处理能力。垂直扩展的方式又有两种： 增强单机硬件性能，例如：增加 CPU 核数如 32 核，升级更好的网卡如万兆，升级更好的硬盘如 SSD，扩充硬盘容量如 2T，扩充系统内存如 128G； 提升单机架构性能，例如：使用 Cache 来减少 I/O 次数，使用异步来增加单服务吞吐量，使用无锁数据结构来减少响应时间； 2) 水平扩展只要增加服务器数量，就能线性扩充系统性能。虚拟化技术的出现，让水平扩展变得轻松且简单。现在的云主机几乎是虚拟主机，而不是物理主机。这样的话，线性扩充也就是分分钟的事，前提是要有足够的物理主机支撑。 高并发的三个经典问题 单台服务器最大并发 单台服务器最大并发问题，一般是指一台服务器能够支持多少TCP并发连接. 一种理论说法是受到端口号范围限制。操作系统上端口号 1024 以下是系统保留的，从 1024-65535 是用户使用的。由于每个TCP连接都要占一个端口号，所以我们最多可以有 60000 多个并发连接。 但实际上单机并发连接数肯定要受硬件资源（内存、网卡）、网络资源（带宽）的限制。特别是网卡处理数据的能力，它是最大并发的瓶颈。 C10K并发连接问题 C10K并发连接问题是指单机 1 万个并发连接问题。如何突破单机性能局限，是高性能网络编程所必须要直面的问题。这些局限和问题最早被 Dan Kegel 进行了归纳和总结，并首次成系统地分析和提出解决方案，后来这种普遍的网络现象和技术局限都被大家称为 C10K 问题 。 C10K问题本质上是操作系统的问题。对于 Web1.0/2.0 时代的操作系统而言， 传统的同步阻塞 I/O 模型都是一样的，处理的方式都是 requests per second，并发 10K 和 100 的区别关键在于CPU。 创建的进程线程多了，数据拷贝频繁（缓存I/O、内核将数据拷贝到用户进程空间、阻塞）， 进程/线程上下文切换消耗大， 导致操作系统崩溃，这就是C10K问题的本质！ C10M并发连接问题 回顾了过去的10年里，我们面临高性能网络编程领域著名的C10K问题，最终也成功提出解决方案。下一个10年，是时候考虑C10M并发问题了。 C10M 并发连接问题指的是单机服务器实现 C10M（即单机千万并发连接）。 Django 与高并发的联系想弄清楚这个问题，首先要了解下 Django 在服务器中所处的位置。 上图中讲到 Django 应用服务器可以分为三层： Web 框架层 Web框架层就是我们开发出来的 Django Web 应用程序。它负责处理 HTTP 请求的动态数据。 WSGI 层 WSGI 不是用于与程序交互的API，也不是真实的代码，WSGI 只是一种接口。它只适用于 Python 语言，其全称为 Web Server Gateway Interface。其定义了 web服务器和 web应用之间的接口规范。 Web 服务器层 Web 服务层作用是主要是接收 HTTP 请求并返回响应。常见的 web服务器有 Nginx，Apache，IIS等。 特别是 Nginx, 它的出现是为了解决 C10K 问题。Nginx 依靠异步事件驱动架构来帮助其处理大量的并发会话，由于其对资源的轻量利用和伸缩自如的特性，它成为了广受欢迎的 web 服务器。 Django 框架注重的数据交互。所以考虑的问题是 Django 适不适合于高并发的场景。它是一个经过大型网站规模验证的框架。Instagram 支撑上亿日活，所以 Django 能适用于高并发场景。所以不是想着 Django 框架能支撑到多大的并发量，而是我们想要抗住很大的并发量，怎么优化现有框架。","link":"/1080.html"},{"title":"10 张图带你了解后台服务架构演变","text":"上篇文章讲了一些高并发相关的知识，相信大家对高并发有些简单的认识。说到高并发，往往离不开分布式系统。人们经常将两者拿来一起讨论，因为高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一。可以这么说，目前应用商城上很多社交应用，网络游戏的后台服务都是分布式服务。那具体什么催生出今天的分布式系统呢？文章的主要内容是讲讲大型网站的服务架构演变。 初始阶段的网站架构在互联网展露出萌芽的网络时代，网站基本都是小型网站。网站的访客也不是很多，通常会将应用程序、数据库、文件等所有资源都在一台服务器上。这里为 Java Web 服务为例。网站开发者可以使用 Tomcat 等 Web 容器直接运行 JSP 程序，然后将数据存储到数据库，文件直接存放到服务器的磁盘中。就像这样子： 应用服务和数据服务分离随着网站业务的发展和用户量的增加，一台服务器就无法再满足需求了。大量用户访问导致访问速度越来越慢，而逐渐增加的数据也会导致存储空间不足。这时需要将 Web 应用和数据分离，分别将存放到不同的服务器：应用服务器、文件服务器和数据库服务器。这样不仅提高了单台机器的负载能力，也提高了容灾能力。 使用缓存改善网站性能随着用户再增加，网站又会一次面临挑战：数据库压力太大导致整站访问效率再此下降，用户体验受到影响。 一个网站往往 80% 的业务访问集中在 20% 的数据上。那么将这一小部分频繁读取的数据先提前缓存在内存中，而不是每次都去数据库读取。这样就可以减少数据库的访问压力，从而提高整个网站的访问速度。 缓存分为本地缓存和分布式缓存服务器，前者更快但容量有限，后者理论上容量可以无限伸缩。 使用集群改善并发处理能力使用缓存后，数据访问压力得到了缓解.但是单一应用服务器能够处理的请求连接有限，在网站访问高峰期，应用服务器就成了整个网站的效率瓶颈。因此使用负载均衡处理器势在必然。通过负载均衡调度服务器，可将来自浏览器的访问请求分发到应用的集群中的任何一台服务器上。使用服务器集群也有个好处，Web 应用程序更新可以做到用户无感知。 大部分应用使用软件来实现负载均衡。常见的软件有 Nginx 等。 数据库读写分离当用户达到一定规模后，数据库因为负载压力过高而成为网站的瓶颈。虽然前面使用缓存能满足查询的需求，但是大部分数据操作还是需要通过数据库来完成。而目前主流的数据库都提供主从热备功能，通过配置两台数据库主从关系，可以将一台数据库的数据更新同步到另一台服务器上。网站利用数据库这一功能实现数据库读写分离，从而改善数据库负载压力。 应用服务器在写数据的时候，访问主数据库，主数据库通过主从复制机制将数据更新同步到从数据库，这样当应用服务器读数据的时候，就可以通过从数据库获得数据。为了便于应用程序访问读写分离后的数据库，通常在应用服务器端使用专门的数据访问模块，使数据库读写分离对应用透明。 反向代理和CDN加速随着网站名气越多越大，用户规模越来越大，网站业务也随着继续壮大。为了满足不同地区的用户快速访问网站的需求，需要提高网站的访问速度。主要手段有使用 CDN 和反向代理。 同时 Ajax 技术的出现，Web 应用会将数据（内容和图片）和页面框架（指 HTML 文件以及其中的标签）。页面框架内容存放到 CDN 服务器上，数据存放到数据库服务器上。当用户使用浏览器访问网站，会显示页面框架，然后页面框架发起 HTTP 请求加载数据。 而反向代理是部署在网站的中心机房，当用户请求到达中心机房后，首先访问的反向代理，如果反向代理缓存着用户请求的资源，则直接返回给用户。 因此，CDN 和反向代理的基本原理都是缓存。 使用分布式文件系统和分布式数据库系统任何强大的单一服务器都满足不了大型网站持续增长的业务需求。 分布式数据库时网站数据库拆分的最后手段，只用在单表数据规模非常大的时候才使用。不到不得已时，网站更常用的数据库拆分手段是业务拆分，将不同业务的数据部署在不同的物理服务器上。 使用NoSQL和搜索引擎随着网站业务越来越复杂，对数据存储和检索的需求也越来越复杂。网站需要采用一些非关系数据库技术如 NoSQL 数据库和非数据库查询技术如搜索引擎。而常见的 NoSQL 数据库有 Mongodb、HBase等。 业务拆分大型网站为了应对日益复杂的业务场景，通过使用分而治之的手段将真个网站业务拆分成不同的产品线。如大型购物交易网站都会将首页、商铺、订单、买家、卖家等拆分成不同的产品线，分归不同的业务团队负责。 分布式服务随着业务拆分越来越小，存储系统越来越庞大，应用系统的整体复杂度呈指数级增加，部署维护越来越困难。 既然每一个应用系统都需要执行许多相同的业务操作，比如用户管理、商品管理等，那么可以将这些共用的业务提取出来，独立部署。由这些可复用的业务连接数据库，提供共用业务服务，而应用系统只需要管理用户界面，通过分布式服务调用共用业务服务完成具体业务操作。 大型网站的架构演化到这里，基本上大多数的技术问题都可以得以解决了。 本文本分内容以及图片参考书籍《大型网站技术架构：核心原理与案例分析》作者: 李智慧。","link":"/1081.html"},{"title":"详解 Scrapy 中间键的用法","text":"Scrapy 爬虫框架的出现，确实能让我们更加专注于数据抓取。同时，我们借助 Scrapy 框架来爬取整个站点数据也显得更加容易。虽然 Scarpy 负责 url 调度、网络请求、页面数据下载等工作，但是它的扩展性很高，其中就支持自定义中间件(Middleware)。本文主要讲解中间件(Middleware)的用法。 什么是中间件中间件的运用比较广泛，如果直接从定义的角度去理解中间件会有点乱，我以分布式系统为例子进行说明。在上篇文章，我讲到目前后台服务架构基本都是往分布式发展。其实分布式系统也算是一个中间件。 那什么是分布式系统？分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。我们从下图中可得知，分布式系统是介于操作系统和用户应用之间的软件。 那么不妨我们进一步拓展下思维去理解中间件。可知，中间件(middleware)是基础软件的一大类，属于可复用软件的范畴。顾名思义，中间件处于操作系统软件与用户的应用软件的中间。 中间件在 Scrapy 框架中的作用我们先通过一张图了解下 Scrapy 架构。 我们可以看到 Scrapy 框架是有两个中间件。一个是 Downloader 中间件，它是 Engine 和 Downloader 的枢纽。主要负责处理 Downloader 传递给 Engine 的 responses； 另一个是 Spider 中间件，它Spider 中间件是 Engine 和 Spider 的连接桥梁；它主要是处理 Spider 的输入(responses) 以及输出 item 和 requests 给 Engine； 实现自己的中间件在 Scrapy 框架中，Downloader 中间件和 Spider 中间件都是支持自定义扩展。在实际应用中，我们经常需要对 Downloader 中间件进行制定化。例如实现一个 User-Agent 中间件给每个 HTTP 请求的头部增加随机筛选的 User-Agent 属性；或者实现一个代理中间件给每个 HTTP 请求设置随机选择的代理地址。 接下来，让我们学习如何实现 Scrapy 的 Downloader 中间件。1) 定义中间件在 Scrapy 项目中，找到 middlewares.py 文件，在文件中创建自己的中间件类。例如，我创建一个代理中间件： 1class ProxyMiddleware(object): 每个中间件一共有三个方法，分别是： process_request(request,spider) 当每个 request 通过下载中间件时，该方法被调用。该方法必须返回以下三种中的任意一种：None，返回一个 Response 对象，返回一个 Request 对象或 raise IgnoreRequest。每种返回值的作用是不同的。 None: Scrapy 将会继续处理该 request，执行其他的中间件的相应方法，直到合适的下载器处理函数( download handler )被调用,该 request 被执行(其 response被下载)。如果有多个中间件，其他的中间件可以通过返回 Null，然后指定对应的中间件去处理 request Request 对象：Scrapy 则停止调用 process_request 方法并重新调度返回的 request。简单来说是拒绝该 Request 的 HTTP 请求。 Response 对象：直接返回结果。 raise IgnoreRequest 异常：抛出异常，然后会被中间件的 process_exception() 方法会被调用。 process_response(request, response, spider) process_response 的返回值也是有三种：Response 对象，Request对象，或者 raise 一个 IgnoreRequest 异常。 如果返回的结果是 Response， 该 response 会被在链中的其他中间件的 process_response() 方法处理。 如果的结果是 Request 对象，则中间件链停止，request 会被重新调度下载。 raise IgnoreRequest 异常: 抛出异常，然后会被中间件的 process_exception() 方法会被调用。。 process_exception(request, exception, spider) 当下载处理器(download handler)或 process_request() (下载中间件)抛出异常(包括 IgnoreRequest 异常)时，Scrapy 调用 process_exception()。 process_exception() 的返回结果同样也是有三个: None、Response 对象、Request 对象。 如果其返回 None ，Scrapy 将会继续处理该异常，接着调用已安装的其他中间件的 process_exception() 方法，直到所有中间件都被调用完毕。 如果其返回一个 Response 对象，则其他中间件链的 process_response() 方法被调用，之后 Scrap y将不会调用其他中间件的 process_exception() 方法。 如果其返回一个 Request 对象， 则返回的request将会被重新调用下载。这将停止中间件的 process_exception() 方法执行，就如返回一个 response 的那样。如果 HTTP 请求失败，我们可以在这里对 HTTP 请求进行重试。例如我们频繁爬取访问一个网站导致被封 IP，就可以在这里设置增加代理继续访问。 我们可以不用实现全部方法，只需要根据需求实现对应的方法即可。例如，我想给每个 HTTP 请求都添加代理地址， 我实现 process_request() 即可。 12345678class ProxyMiddleware(object): # overwrite process request def process_request(self, request, spider): # 从数据库中随机读取一个代理地址 proxy_address = proxy_pool.random_select_proxy() logging.debug(\"===== ProxyMiddleware get a random_proxy:【 {} 】 =====\".format(proxy_address)) request.meta['proxy'] = proxy_address return None 2) 在 setting.py 启用中间件我们已经实现了中间件，最后一步需要启用该中间件。我们将定义的中间件添加到 settings.py 文件。如果你是重载系统中间件，还需要将系统的中间件的值设置为 None。我前面定义的代理中间件，是需要对 HTTP 请求做操作。所以重载了 HttpProxyMiddleware 中间件。 123456# 中间件填写规则# yourproject.myMiddlewares(文件名).middleware类# 设置代理'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': None,'scrapydemo.middlewares.ProxyMiddleware': 100,","link":"/1082.html"},{"title":"总结是成长的秘方","text":"时光荏苒，2018 年已经过去半年。你年初制定新年计划，不知现在完成的进度是多少了？不管怎样，应该好好总结下。 学习，生活，工作就像一场航行。我们这艘孤零零的小船在茫茫大海中飘荡着。或许我们知道此次航行的目的地。但是航程会有各种突发状况。这会让我们不知不觉偏离航线，进而影响到达目的地的时间。这时就需要导航工具，而总结就是这一工具。 善于总结是一件好事。在总结过程中，我们能对自己有个清楚的认识。知道自己哪里做的不好，需要改进。知道自己哪方面值得继续保持。 曾子善于总结，在这方面可以算是宗师。他的名言早传遍中华大江南北。吾日三省吾身——为人谋而不忠乎？与朋友交而不信乎？传不习乎？ 工作中善于总结，能保证我们的努力和付出不会白费。从而避免成为“伪工作者”。每天下班之前，我们可以以日报形式总结下今天的工作。看下今天的工作内容是否有意义？看下今天的工作是否完成了？明天要做什么内容？ 学习也是如此。阶段性总结能检查自己的学习成果。及时发现自己未知的领域，然后继续填坑。这一个月来，我也是一直在总结。自己梳理之前的爬虫代码，后续会将分享出来。特别提醒下，看别人的爬虫文章，重点阅读别人的爬虫思路以及分析的过程。 写在最后，我给各位小伙伴送上一份福利 —— Python 爬虫知识的框架图","link":"/663.html"},{"title":"今天， IG 帮我们圆梦","text":"对于英雄联盟(俗称 LOL 或撸阿撸)游戏玩家来说，今天无疑是值得庆祝的一天。IG (全称是 Invictus Gaming)零封对手 FNC，豪取我们盼望了 8 年的世界冠军。 看完比赛，我内心一直激动不已，只想说一句。 这只队伍确确实实是牛逼! 因为一开始它不被看好，队员都是东拼西凑的。上单是一个没人要的上单，打野是借过来的打野，中单是付赠品，直播出道的下路，还有一个双排来的辅助。他们依然能克服各种客观因素，在赛场上打出自己的风采。拿我最喜欢的队员 theshy 来说。 第一把，theshy 来一手“千里走单骑”，强行开团，团队打出一波“2 换 5”，为第一盘奠定胜局。 第二把的刀妹更是秀，中路塔下花式补刀；然后在下波团战，theshy 化身“铁头娃”千里追击对手到敌方下路二塔，然后为小兵为踏板，强顶防御塔斩获双杀；最后一把团战，theshy 更是走单骑，秀一波一打五的操作，还全身而退。 由于前两把打出气势，theshy 第三把更加势不可挡。他拿出一手剑魔，人挡杀人，好像在他的眼中就没有防御塔这说法。 现在的我，已经不玩英雄联盟了。但是自己还是会关注游戏比赛。或许这就是青春，或许这能让我们回味那些年游戏给我们带来的欢乐。","link":"/1184.html"},{"title":"程序员应该懂点产品知识——竞品分析","text":"最近产品经理和程序员打架的事件闹得满城风雨。网传事情大概是这样: 一个产品经理给研发提出一个产品需求，要求 app 的主题颜色可以随着用户手机壳颜色改变而变化。 然后，两人直接干架。 出现这样的新闻，我们不能以看热闹的眼光来看待，应该积极思考背后的原由。总所周知，产品经理和程序员的矛盾一直很存在。既然矛盾一直存在，那我们程序员应如何优雅跟产品经理沟通？沟通的内容往往离不开这些问题:这个需求是否要做？如果做，有什么意义？如果不做，有什么损失？显然，我们要想一一回答这些问题，不能以技术的思维来回答。要懂一些产品知识，从产品的角度切入来解答问题。 另外，产品经理有些时候提出这样的需求。竞争对手的产品最近新增某些功能或者接口，但自家产品没有，需要补齐。产品经理一定是将自己产品和竞品做对比之后才提出来的。如果这部分工作由我们来做，我们要如何做对比？从哪些方面做对比呢？针对竞品分析，我自己说下自己的心得。 为什么要做竞品分析？除了前面讲到的几点原因，还有一个重要原因。如果我们做了一个竞品分析报告，然后递交给自己团队的 Leader，甚至是 CTO 或者老板。可能会在一定程度上帮助到他们。团队管理者会在年初和年中制定团队未来半年的发展规划。先是确定产品的发展大方向，然后再对产品细分出各项指标。假设团队的计划是追赶竞争对手，那我们这份报告可以称得上是及时雨。 怎么做竞品分析？竞品分析可以从以下三个方面进行分析：业务层面、产品层面、市场层面。 业务层面 业务一般是指团队产品定位方向。例如团队产品的面向用户群体（也可以说业务是 toC 还是 toB ？)、产品的主攻行业、产品的销售方式。关于自家的产品，我们在刚入职的时候，都会花一定时间来熟悉这些内容。而我们对竞争对手的产品或多或少有了解，但是没有全面分析过。我们可以通过其官网，搜索微信公众号、CSDN 博客、知乎专栏等公关稿来了解对手的主攻行业和目标客户。其中，目标客户还可以通过做市场调研途径来收集信息。我们把这些内容都收集完整，这个才算完整迈出第一步。接下来，我们还需要迈出更大一步。因为我们是在做竞品分析，既然有竞争，那么需要有个对比结论。自家产品在哪方面做得比对手好，那么方面需要进一步改进。然后再根据上述内容进行提炼，找出对手产品占领市场的策略以及关键阵地。 产品层面 每一件产品有其自身的特点、产品功能。另外，toB 类型的产品往往具备提供解决方案的能力。 首先说下产品特点如何做对比。产品特点也可以说是标签。每家公司在推广产品的时候，一定会给产品打上标签。这标签可以理解为产品特点。可能有些公司会为产品打个口号，这也能大体表示产品的特点。例如阿里云的口号是“为了无法计算的价值”，腾讯云的口号是“连接，智能，未来”。 而产品功能方面，要区分 toC 产品 和 toB 产品。如果团队是做 toC 产品，我们需要体验各自应用，从功能以及用户体验两个方面做对比。如果团队是做 toB 产品，那么我们可以查阅官网产品文档、API 文档。然后将功能、API 接口做下对比。 toB 产品一般会给客户做定制化需求，然后将通用的东西抽取出来，然后打包成解决方案。这些内容官网文档一定会有详细的介绍。 市场层面 市场主要对比营销方式、品牌口碑、市场评价这三方面的内容。 市场营销是一本艺术，它能将一个刚出厂的产品变得妇孺皆知。我们可以通过收集公关稿、广告投放渠道等方面了解相关信息。如果 toB 产品，有个重要的指标是客户案例。因为 toB 产品是面对企业，所有官网一定会晒出经典客户案例。 借助“客户案例”，我们可以进一步深挖竞品所在行业的市场占有，然后结合市场调研。我们还可以得出品牌曝光率、口碑等结论。 市场评价方面，可以收集各种新闻稿。例如去 36Kr 等知名的网站进行搜索。 以上内容是自己对产品的一些理解。如果有哪些地方讲错或者您有更好的看法，可以跟我联系。","link":"/873.html"},{"title":"为什么说 TCP 协议是可靠的？","text":"为什么说 TCP 协议是可靠的？几张图带你彻底了解 TCP 协议。 TCP 协议是 TCP/IP 协议栈中的传输层的协议，TCP协议又叫传输控制协议(Transport Control Protocal)。众所周知，它是一个可靠协议。因为它能保证接收端完整地接受到发送端发送的数据包，即保证不丢包。 那 TCP 协议如何保证不丢包呢？这个是本文重点讲述的内容。 TCP 协议的作用ISO（国际标准化组织）曾提出一个 OSI 七层模型。将网络的协议划分为 7 个层，从低到高排序是：物理层、数据链路层、网络层、传输层、会话层、表示层和应用层。但是这个模型仅停留在理论阶段。因为该模型过于庞大、复杂，以至于无法被广泛应用。 后来技术人员在 TCP/IP 等协议集问世之后，提出 TCP/IP 协议栈。该模型很贴近实际场景，所以被广泛的应用。TCP/IP 协议栈一共分为 4 个层次。从低到高依次排序是：数据链路层（有书籍称之为网络接口层)、网络层、传输层、应用层。 网络接口层：针对不同物理网络的连接形式的协议：以太网、FDDI 光纤分布式数据接口。其中协议有 ARP 协议(地址解析协议)、RARP 协议(反向地址转换协议) 网际层：负责数据的传输，路由以及地址选择。最主要的协议是 IP 协议。 传输层：确认数据传输以及进行纠错处理。传输层中中有两个非常重要的协议，即 TCP 协议和 UDP 协议。 应用层：各种服务以及应用程序。常见的应用层协议有 HTTP 协议、FTP 协议(文件传输协议)、SMTP 协议(简单邮件传输协议)等 TCP 协议通信特点TCP 协议是实现端口到端口的通信。它虚拟了本文流(byte stream)的通信。我们知道，计算机数据的本质是有序的 0/1 序列(如果以byte为单位，就叫做文本流)。计算机的功能就是储存和处理文本流。所以TCP 是采用“流”通信。 但是传输层的下一层是网络层。即 TCP 协议的下一层协议是 IP 层。这就意味着 TCP 协议最终还是由 IP 协议规定的形式传输数据。而IP 协议是以数据包方式传送。同时，IP 数据包的 MTU 也有长度限制。所以 TCP 协议会将数据切割为一个个片段，然后丢给网络层，接着打包成一个个数据包进行传输。 但是这样，流数据变成了一个个片段数据，这会无法保证数据到达的次序。因为 IP 协议在传输过程中，不会按顺序进行发送和接受数据包。针对这问题，TCP 协议为了确保数据到达的顺序与文本流顺序相同。TCP 协议将每个 TCP 片段中分为头部(header)和数据(payload)两部分。每个头部中带有一个序号。这相当于给每个片段增加一个序号标记，方便后续排序。 TCP 实现可靠通信的两种方式我们都知道 IP 协议是“不太靠谱”。因为 IP 协议是不可靠的，所以 IP 数据包可能在传输过程中发生错误或者丢失。这就意味着，TCP 协议不得不面对以下三个问题。1）每个数据包有可能发送不成功 2）数据包在传输过程中有可能被丢弃 3）接收端有可能接受不到数据包 TCP 为了解决这丢包问题，提出两个补救措施。 1 ACK 回复 在每收到一个正确的、符合次序的片段之后，就向发送方(也就是连接的另一段)发送一个特殊的 TCP 片段，用来知会(ACK，acknowledge)发送方：我已经收到那个片段了。这个特殊的 TCP片段 叫做 ACK 回复。如果一个片段序号为 L，对应ACK 回复有回复号 L+1，也就是接收方期待接收的下一个发送片段的序号。 2 重新发送机制 如果发送方在一定时间等待之后，还是没有收到 ACK 回复，那么它推断之前发送的片段一定发生了异常。发送方会重复发送(retransmit)那个出现异常的片段，等待 ACK 回复，如果还没有收到，那么再重复发送原片段… 直到收到该片段对应的 ACK 回复(回复号为 L+1 的 ACK)。 TCP 的滑动窗口虽然采用 “ACK 回复” + “重新发送机制” 方式能实现不丢包，但是会存在两个问题。1.效率低的问题。发送方保持发送 -&gt; 等待ACK -&gt; 发送 -&gt; 等待ACK…的单线工作方式，这样的工作方式叫做 stop-and-wait。stop-and-wait 虽然实现了 TCP 通信的可靠性，但同时牺牲了网络通信的效率。同时，在等待ACK的时间段内，我们的网络都处于闲置(idle)状态 2.有点小缺陷如果片段一直没有被确认，会导致后续的片段无法发送出去。 TCP 为了进一步优化解决这两个问题，提出滑动窗口(sliding window)的概念。滑动窗口被同时应用于接收方和发送方, 发送方和接收方各有一个滑窗。当片段位于滑窗中时，表示 TCP 正在处理该片段。此外，如果滑窗中可以有多个片段，也就是可以同时处理多个片段。 我们借助一些图片来进一步了解下滑动窗口内部机制。 黄色框框表示可以容纳三个片段的固定大小的滑窗。在图中，并假设片段从左向右排列。实际运用中，滑动窗口是可变的，窗口大小是字节(byte)来计算的。 对于发送方来说，滑窗的左侧为已发送并已 ACK 过的片段序列，滑窗右侧是尚未发送的片段序列。如果滑动窗口第一个片段一直没有收到 ACK 回复，窗口不会向右滑动。但是发送方还是可以继续发送后面两个片段数据包。 对于接受方来说，滑窗的左侧是已经正确收到并 ACK 回复过的片段，也就是正确接收到的文本流。滑窗中的片段是期望接收的片段。如果滑窗中第一个片段先收到， 滑窗会向右移动。如果滑窗中后面两个片段先收到，但是第一个片段没有收到。窗口不会向右滑动。 发送端已经发送三个数据包（1、2、3），在等待每个数据包的 ACK 回复 接收端成功收到两个数据包，回复两个 ACK。还有一个数据包没有收到。当收到 数据包 1 时，接收端会回复一个 ACK 1，然后将窗口向有滑动一个位置。 发送端成功接收到 ACK 1 回复 发送端的窗口向右滑动一个位置 在没有收到 ACK 2 和 3 的回复，还能继续发送数据包 4 之前数据包 4 已经发送了。在之后成功收到 ACK 2 和 3 的回复，窗口向右滑动两个位置，现在又能继续发送数据包 5，6 通过上面一系列图片，我们可以大致知道滑动窗口的机制。我们来做下小总结： 对于发送端 如果滑动窗口第一个片段一直没有收到 ACK 回复，窗口不会向右滑动。但是发送方还是可以继续发送后面两个片段数据包。 对于接受端 如果滑窗中第一个片段先收到，滑窗会向右移动。如果滑窗中后面两个片段先收到，但是第一个片段没有收到。窗口不会向右滑动 那么实际应用中确实是这样吗？如果接收方每接受一个片段，就回复一个 ACK。这种效率有点低。所以实际应用中， TCP 协议为了减少了 ACK 回复所消耗的流量，采用的是累计 ACK 回复。 接收方往往利用一个 ACK 回复来知会连续多个片段的成功接收。通过累计 ACK，所需要的 ACK 回复通常可以降到 50%。 我们同样通过图片的形式来了解累计 ACK 回复的原理。 在图中，橙色为已经接收的片段。方框为滑窗，滑窗可容纳3个片段。 情况1：滑窗还没接收到片段 7 时，已接收到片段 8，9。这样就在滑窗中制造了一个“空穴”(hole)。 情况2：当滑窗最终接收到片段7时，滑窗送出一个回复号为 10 的 ACK 回复。发送方收到该回复，会意识到，片段 10 之前的片段已经按照次序被成功接收。整个过程中节约了片段 7 和片段 8 所需的两个 ACK 回复。 文章参考内容： Vamei 大神的《协议森林08 不放弃 (TCP协议与流通信)》 Vamei 大神的《协议森林10 魔鬼细节 (TCP滑窗管理)》","link":"/769.html"},{"title":"Python 三种遍历目录的方法，轻松帮你找出隐藏文件","text":"无论在 Windows 系统中还是 Linux 系统中，都存在着隐藏文件以及隐藏文件夹。隐藏文件夹一般是系统关键性目录，例如 Windows 系统中的 C 盘中的 Boot 文件夹、Windows 文件夹等。在 Linux 系统中，一些系统配置文件或软件配置文件会被隐藏起来，如：系统环境变量配置文件 .profile。有些时候，我们必须通过文件路径来遍历整个目录，然后找到隐藏文件。本文的主要内容是给大家分享 Python 三种遍历文件的方法。 简单暴力法-递归假设在 E 盘中，有个名为“Python”的文件夹；该文件夹中也有两个文件夹，分别是“A”和“B”；另外，在“A”文件夹中还有一个 “results.txt” 的文本文件。因此，“Python”文件夹的文件结构如下： 1234Python|--A| |--results.txt|--B 我们可以从上述看出，一个文件夹其实是一个树型的数据结构。遍历树的最简单、最暴力的办法就是递归。因此，遍历“Python”的文件夹的代码可以这么写。 123456789101112131415# -*- coding: UTF-8 -*-import os# 递归遍历目录def traversal_files(path): for dir in os.listdir(path): dir = os.path.join(path, dir) print(dir) # 判断当前目录是否为文件夹 if os.path.isdir(dir): traversal_files(dir)if __name__ == '__main__': path = '.' traversal_files(path) 运行脚本程序后，发现“Python”中的文件和文件夹都被打印出来。 这种遍历方法能否找出隐藏文件？答案是肯定能。让我们来验证一番。首先，我们在“Python”的文件夹中放入一个“config.txt”的隐藏文件。 然后再运行程序，结果发现“config.txt”被打印出来。 这种办法虽然写起来代码简洁，但是在文件夹的子目录层级过深的情况下，效率会比较低。 优雅的 os.walk()既然递归太暴力，那么使用 os.walk() 会让程序显得优雅。os.walk() 方法是一个简单易用的文件、目录遍历器，可以帮助我们高效的处理文件、目录方面的事情。这个方法适用于在 Linix 和 Windows。 os.walk() 一般用法是传入两个参数。第一个参数是 path， 即所要遍历的目录的地址。它返回的是一个三元组(root, dirs, files)。 root 所指的是当前正在遍历的这个文件夹的本身的地址 dirs 是一个 list ，内容是该文件夹中所有的目录的名字(不包括子目录) files 同样是 list , 内容是该文件夹中所有的文件(不包括子目录) 第二个参数是topdown，它是一个可选参数。当它的值为 True 时，则优先遍历 path 目录，否则优先遍历 top 的子目录(默认为开启)。 因此，使用 os.walk 遍历文件夹，找出隐藏文件的代码如下所示。 12345678910111213# -*- coding: UTF-8 -*-import osdef traversal_files(path): for root, dirs, files in os.walk(path, topdown=False): for name in files: print(os.path.join(root, name)) for name in dirs: print(os.path.join(root, name))if __name__ == '__main__': path = '.' traversal_files(path) 高效的 os.scandir()在 Python 3.5版本中，新添加了 os.scandir()方法，它是一个目录迭代方法。os.scandir() 的运行效率要比 os.walk 高。在 PEP 471 中，Python 官方也推荐我们使用 os.scandir() 来遍历目录。 按照前面的例子，遍历“Python”的文件夹中的隐藏文件的代码如下： 12345678910111213141516171819# -*- coding: UTF-8 -*-import osdirs = []files = []def traversal_files(path): for item in os.scandir(path): if item.is_dir(): dirs.append(item.path) elif item.is_file(): files.append(item.path) print('dirs:', dirs) print('files:', files)if __name__ == '__main__': path = '.' traversal_files(path) 运行结果，同样也是能找出隐藏文件。","link":"/1185.html"},{"title":"推荐几个免费数据源的网站（送书福利）","text":"说到数据分析，我们会很容易联想到 Python。因为我们可以拿 Python 对已有的数据做数据分析。那什么是数据分析？数据分析指用适当的统计分析方法对收集来的大量数据进行分析，提取有价值的信息。在实用中，数据分析可帮助人们作出判断，以便采取适当行动。 “啤酒与尿布”的故事就是最典型的数据分析案例。“啤酒”和“尿布”这两个商品看上去没有关联性。而沃尔玛将其摆放在一起进行销售、并获得了很好的销售收益。原因在于沃尔玛对购物篮分析，研究出“啤酒与尿布”之间存在一定关联性。这就是数据分析的意义。 再例如通过数据分析出北京的空气质量的走势；分析近 10 年来 NBA 球队战绩和夺冠率之间的关系等等。 说到这里，你也许有疑问。我没有数据，怎么做数据分析？别急，我推荐几个免费的数据源网站。 awesome-public-datasets这是一个 GitHub 仓库，其收集的数据所在领域比较广泛，收录政府、金融、计算机网络、软件、体育、气候天气等30个领域的数据。 这些数据集的质量都是比较高，大部分都是免费且能直接下载到的数据。 Github 仓库地址 FiveThirtyEightFiveThirtyEight，亦称作 538，是一个专注与民意调查分析，政治，经济与体育的博客。FiveThirtyEight 维护一个名为 data 的 GitHub 仓库，专门存储数据。这些数据都来源于 FiveThirtyEight 的官网 。另外，FiveThirtyEight 被ESPN收购后 扩展了其覆盖的内容，增加了更多诸如政治，体育，科学，经济与流行文化的栏目。所以我们看到的内容如下： 这些数据都保存在 CSV 文件，我们下载之后就能用来做数据分析。 点击☞☞☞Github 仓库地址 另外，我自费送出两本数据分析入门书籍。美国 Facebook 数据科学家 Clinton W. Brownley 博士出品的《 Python数据分析基础》。 这本书可以说是适合领编程经验的同学。因为这本书的学习路线是层层递减，从浅到深。学习路线是学习基础语法，创建并运行自己的Python脚本 - 读取和解析CSV文件 - 读取多个Excel工作表和工作簿 - 执行数据库操作 - 搜索特定记录、分组数据和解析文本文件 - 建立统计图并绘图 - 生成描述性统计量并估计回归模型和分类模型 。 参与方式：关注『极客猴』公众号，公众号后台回复【抽奖】。即可参与抽奖。抽奖将于周四晚上 10 点准时开奖。","link":"/1187.html"},{"title":"100 行代码爬取全国所有必胜客餐厅信息","text":"当我刚接触 Python 时，我已经被 Python 深深所吸引。Python 吸引我的地方不仅仅能用其编写网络爬虫，而且能用于数据分析。我能将大量的数据中以图形化方式呈现出来，更加直观的解读数据。 数据分析的前提是有数据可分析。如果没有数据怎么办？一是可以去一些数据网站下载相关的数据，不过数据内容可能不是自己想要的。二是自己爬取一些网站数据。 今天，我就爬取全国各地所有的必胜客餐厅信息，以便后续做数据分析。 爬取目标我们要爬取的目标是必胜客中国。打开必胜客中国首页，进入“餐厅查询”页面。 我们要爬取的数据内容有城市、餐厅名字、餐厅地址以及餐厅联系电话。因为我看到页面中有地图，所以页面一定有餐厅地址的经纬度。因此，餐厅的经纬度也是我们需要爬取的数据。 至于全国有必胜客餐厅的城市列表，我们可以通过页面的“切换城市”获取。 分析目标页面在编写爬虫程序之前，我都是先对页面进行简单分析，然后指定爬取思路。而且对页面结构进行分析往往会有一些意想不到的收获。 我们使用浏览器的开发者工具对页面结构进行简单分析。 我们在 StoreList 页面中能找到我们所需的数据。这个能确定数据提取的 Xpath 语法。 StoreList 页面的 Response 内容比较长。我们先不着急关闭页面，往下看看，找找看是否有其他可利用的内容。最后，我们找到调用获取餐厅列表信息的 JavaScript 函数代码。 我们接着搜索下GetStoreList函数，看看浏览器如何获取餐厅列表信息的。 从代码中，我们可以了解到页面使用 Ajax 方式来获取数据。页面以 POST 方式请求地址http://www.pizzahut.com.cn/StoreList/Index。同时，请求还携带参数 pageIndex 和 pageSize。 爬取思路经过一番页面结构分析之后，我们指定爬取思路。首先，我们先获取城市信息。然后将其作为参数，构建 HTTP 请求访问必胜客服务器来获取当前城市中所有餐厅数据。 为了方便数据爬取，我将所有城市全部写入到 cities.txt 中。等要爬取数据时，我们再从文件中读取城市信息。 爬取思路看起来没有错，但是还是有个难题没有搞定。我们每次打开必胜客的官网，页面每次都会自动定位到我们所在的城市。如果无法破解城市定位问题，我们只能抓取一个城市数据。 于是乎，我们再次浏览首页，看看能不能找到一些可用的信息。最终，我们发现页面的 cookies 中有个 iplocation 字段。 我将其进行 Url 解码，得到 深圳|0|0 这样的信息。 看到这信息，我恍然大悟。原来必胜客网站根据我们的 IP 地址来设置初始城市信息。如果我们能伪造出 iplocation 字段信息，那就可以随便修改城市了。 代码实现第一步是从文件中读取城市信息。 123456789# 全国有必胜客餐厅的城市, 我将城市放到文件中, 一共 380 个城市cities = []def get_cities(): \"\"\" 从文件中获取城市 \"\"\" file_name = 'cities.txt' with open(file_name, 'r', encoding='UTF-8-sig') as file: for line in file: city = line.replace('\\n', '') cities.append(city) 第二步是依次遍历 cities 列表，将每个城市作为参数，构造 Cookies 的 iplocation 字段。 123456# 依次遍历所有城市的餐厅for city in cities: restaurants = get_stores(city, count) results[city] = restaurants count += 1 time.sleep(2) 然后，我们再以 POST 方式携带 Cookie 去请求必胜客服务器。最后再对返回页面数据进行提取。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263def get_stores(city, count): \"\"\" 根据城市获取餐厅信息 \"\"\" session = requests.Session() # 对【城市|0|0】进行 Url 编码 city_urlencode = quote(city + '|0|0') # 用来存储首页的 cookies cookies = requests.cookies.RequestsCookieJar() headers = { 'User-agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 UBrowser/6.2.3964.2 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Host': 'www.pizzahut.com.cn', 'Cache-Control': 'max-age=0', 'Connection': 'keep-alive', } print('============第', count, '个城市:', city, '============') resp_from_index = session.get('http://www.pizzahut.com.cn/', headers=headers) # print(resp_from_index.cookies) # 然后将原来 cookies 的 iplocation 字段，设置自己想要抓取城市。 cookies.set('AlteonP', resp_from_index.cookies['AlteonP'], domain='www.pizzahut.com.cn') cookies.set('iplocation', city_urlencode, domain='www.pizzahut.com.cn') # print(cookies) page = 1 restaurants = [] while True: data = { 'pageIndex': page, 'pageSize': \"50\", } response = session.post('http://www.pizzahut.com.cn/StoreList/Index', headers=headers, data=data, cookies=cookies) html = etree.HTML(response.text) # 获取餐厅列表所在的 div 标签 divs = html.xpath(\"//div[@class='re_RNew']\") temp_items = [] for div in divs: item = {} content = div.xpath('./@onclick')[0] # ClickStore('22.538912,114.09803|城市广场|深南中路中信城市广场二楼|0755-25942012','GZH519') # 过滤掉括号和后面的内容 content = content.split('(\\'')[1].split(')')[0].split('\\',\\'')[0] if len(content.split('|')) == 4: item['coordinate'] = content.split('|')[0] item['restaurant_name'] = content.split('|')[1] + '餐厅' item['address'] = content.split('|')[2] item['phone'] = content.split('|')[3] else: item['restaurant_name'] = content.split('|')[0] + '餐厅' item['address'] = content.split('|')[1] item['phone'] = content.split('|')[2] print(item) temp_items.append(item) if not temp_items: break restaurants += temp_items page += 1 time.sleep(5) return restaurants 第三步是将城市以及城市所有餐厅信息等数据写到 Json 文件中。 12with open('results.json', 'w', encoding='UTF-8') as file: file.write(json.dumps(results, indent=4, ensure_ascii=False)) 爬取结果程序运行完之后, 就会在当前目录下生成一个名为「results.json」文件。","link":"/1186.html"},{"title":"爬虫必备工具 —— Chrome 开发者工具","text":"在《论语》中，孔子提倡“学而不思则罔，思而不学则殆”的学习方法。我们再往深层面挖掘，“思”究竟是在思考什么？个人理解是思考并总结出一些共性的东西，即“套路”。有套路了，我们学习或工作会更加有效率。 当我们爬取不同的网站是，每个网站页面的实现方式各不相同，我们需要对每个网站都进行分析。那是否有一些通用的分析方法？我分享下自己爬取分析的“套路”。在某个网站上，分析页面以及抓取数据，我用得最多的工具是 Chrome 开发者工具。 Chrome 开发者工具是一套内置于 Google Chrome 中的 Web 开发和调试工具，可用来对网站进行迭代、调试和分析。因为国内很多浏览器内核都是基于 Chrome 内核，所以国产浏览器也带有这个功能。例如：UC 浏览器、QQ 浏览器、360 浏览器等。 接下来，我们来看看 Chrome 开发者工具一些比较牛逼的功能。 元素面板通过元素（Element）面板，我们能查看到想抓取页面渲染内容所在的标签、使用什么 CSS 属性（例如：class=”middle”）等内容。例如我想要抓取我知乎主页中的动态标题，在网页页面所在处上右击鼠标，选择“检查”，可进入 Chrome 开发者工具的元素面板。 通过这种方法，我们能快速定位出页面某个DOM 节点，然后可以提取出相关的解析语句。鼠标移动到节点，然后右击鼠标，选择 “Copy”，能快速复制出 Xpath 、CSS elector 等内容解析库的解析语句。 网络面板网络（Network）面板记录页面上每个网络操作的相关信息，包括详细的耗时数据、HTTP 请求与响应标头和 Cookie，等等。这就是我们通常说的抓包。 其中的 Filters 窗格，我经常使用其来过滤出一些 HTTP 请求，例如过滤出使用 Ajax 发起的异步请求、图片、视频等。 最大的窗格叫 Requests Table，此表格会列出了检索的每一个 HTTP 请求。 默认情况下，此表格按时间顺序排序，最早的资源在顶部。点击资源的名称可以显示更多信息。 Headers 是显示 HTTP 请求的 Headers，我们通过这个能看到请求的方式，以及携带的请求参数等。 Preview 是请求结果的预览。一般用来查看请求到的图片，对于抓取图片网站比较给力。 Response 是请求返回的结果。一般的内容是整个网站的源代码。如果该请求是异步请求，返回的结果内容一般是 Json 文本数据。 Cookies 能看到请求携带的 Cookies 以及服务器返回的 Cookies。有时候是需要使用到 Response 的 Cookies。例如，某个页面必须登录才能看到。","link":"/1188.html"},{"title":"使用 Python 分析全国所有必胜客餐厅","text":"在之前的一篇文章中，我讲到如何爬取必胜客官网中全国各大城市餐厅的信息。虽然餐厅数据信息被抓取下来，但是数据一直在硬盘中“躺尸”。不曾记得，自己已经第 n 次这么做了。说到这里，要追溯到自己的大学时光。 自己从大学开始就接触 Python，当时是自己的好奇心很强烈。好奇为什么 Python 不需要浏览器就能抓取网站数据。内心感叹到，这简直是太妙了。自己为了体验这种抓取数据的乐趣，所以写了很多的爬虫程序。 随着自己知识面地拓展，自己了解到数据分析这领域。自己从而才知道爬取到的数据，原来背后还隐藏的一些信息。自己也是在学习这方面的相关知识。这篇文章算是数据分析的处女稿，主要内容是从数据中提取出必胜客餐厅的一些信息。 环境搭建百度前端技术部开源一个基于 Javascript 的数据可视化图表库。其名字为 ECharts。它算是前端数据可视化的利器，能提供直观，生动，可交互，可个性化定制的数据可视化图表。 国内有个大神突发奇想，这么好用的库如果能和 Python 结合起来就好了。于是乎，pyecharts 库就应运而生。因此，pyecharts 的作用是用于生成 Echarts 图表的类库。本文中的所有图标，自己都是利用 pyecharts 生成的。 安装该库也很简单，使用 pip 方式安装。 1pip install pyecharts 数据清洗数据清洗工作是数据分析必不可少的步骤。这一步是为了清洗一些脏数据。因为可能网站本身就有空数据，或者匹配抓取网站信息时，有些混乱的数据。这些都需要清除掉。 我之前是将数据写到一个 json 文件中，我先将数据读取出来。然后把 json 文本数据转化为字典类型。 12345678def get_datas(): \"\"\" 从文件中获取数据 \"\"\" file_name = 'results.json' with open(file_name, 'r', encoding='UTF-8') as file: content = file.read() data = json.loads(content, encoding='UTF-8') # print(data) return data 接着对字典进行遍历, 统计每个城市的餐厅总数。 1234567def count_restaurants_sum(data): \"\"\" 对字典进行遍历, 统计每个城市的餐厅总数 \"\"\" results = {} for key, value in data.items(): results[key] = len(value) # print(key, len(value)) return results 再将字典中的每个 key-value 转化为元组，然后根据 value 进行倒序排序。 1restaurants_sum = sorted(restaurants_sum.items(), key=lambda item: item[1], reverse=True) 最后根据显示结果，手动删除一些脏数据。 1234567891011def clean_datas(data): \"\"\" 清除脏数据。 经过分析发现 ('新区', 189), ('南区', 189), ('朝阳', 56) 是脏数据, 必胜客官网的地区选项中就有这三个名字 [('新区', 189), ('上海市', 189), ('南区', 189), ('北京市', 184), ('深圳', 95), ('广州', 86), ('杭州', 78), ('天津市', 69), ('朝阳', 56), ('苏州', 54)] \"\"\" data.remove(('新区', 189)) data.remove(('南区', 189)) data.remove(('朝阳', 56)) return data 到此，数据工作已经完成。 数据分析我们已经拿到了经过清洗的数据，我们简单对数据进行打印，然后绘制直方图。 123456789101112def render_top10(): \"\"\" 绘制直方图显示 全国必胜客餐厅总数 Top 10 的城市 根据清洗过后数据的结果, Top 城市如下 ('上海市', 189), ('北京市', 184), ('深圳', 95), ('广州', 86), ('杭州', 78), ('天津市', 69), ('苏州', 54), ('西安', 52), ('武汉', 51), ('成都', 48) \"\"\" attr = [\"上海\", \"北京\", \"深圳\", \"广州\", \"杭州\", \"天津\", \"苏州\", \"西安\", \"武汉\", \"成都\"] values = [189, 184, 95, 86, 78, 69, 54, 52, 51, 48] bar = Bar(\"全国各大城市必胜客餐厅数量排行榜\") bar.add(\"总数\", attr, values, is_stack=True, is_more_utils=True) bar.render(\"render_bar.html\") 绘制出来的结果如下： 不难看出，一线城市拥有必胜客的餐厅数比较多，省会城市拥有餐厅数要比非省会城市要多。 我们继续绘制饼状图，看看北上广深的餐厅数在全国的占比。 12345678910def render_top10_percent(): \"\"\" 绘制饼状图 显示北上广深餐厅数在全国中的比例 \"\"\" configure(global_theme='macarons') attr = [\"上海\", \"北京\", \"深圳\", \"广州\", \"其他城市\"] value = [189, 184, 95, 86, 1893] # 根据 count_other_sum() 计算出来的 pie = Pie(\"北上广深餐厅数的占比\") pie.add(\"\", attr, value, is_label_show=True, is_more_utils=True) pie.render(\"render_pie.html\") 绘制出来的结果如下： 从数据上看，北上广深的餐厅数占据全国餐厅数的 22.64%。其他二三线城市共占据 77.36%。说明必胜客餐厅不仅主打大城市路线，还往二三四线城市发展，扩展领域。","link":"/1189.html"},{"title":"阅读优秀代码是提升技术的最佳途径","text":"在之前的一篇文章中，我讲到如何爬取必胜客官网中全国各大城市餐厅的信息。虽然餐厅数据信息被抓取下来，但是数据一直在硬盘中“躺尸”。不曾记得，自己已经第 n 次这么做了。说到这里，要追溯到自己的大学时光。 在互联网行业，技术更新迭代太快了。我们身在行业中，要不断地学习提高自己的能力。有一种不错的方式来提高自己的技术实力。那就是阅读别人优秀的代码。 那也许你会有疑问，为何要阅读别人优秀的代码？而不是拿到代码就进行阅读？正如意大利作家伊塔洛·卡尔维诺的《为什么要读经典》中提到经典于对喜欢它的人构成一种宝贵的经验。 因此，阅读别人的优秀源代码有很多好处，收益也会非常大。通过大量阅读别人的代码，我们可以采用更先进的方法、风格和架构模式，让自己的技术能力和知识不断的增长。 我收集一些优秀的 Python Web 相关的优秀项目，分享给大家参考学习。 flask-adminflask-admin 是基于 flask 框架开发的 admin 管理系统。该库能基于现有的数据模型，快速创建管理界面。易用性很高，简单配置参数就能运行。运行结果如下： 另外，它还是微服务应用，提供很多 API。我们能从中学到微服务架构设计思路、数据(model)-视图(view) 绑定设计等。 Github 仓库地址 FlaskBBFlaskBB 基于 Flask 框架做的论坛，轻量级的论坛应用。虽然具备论坛帖子分类、成员/成员组地管理、搜索功能、插件等功能，但是功能还是有限，做得不够完善。值得庆幸的，我们可以在这个项目上进行二次开发，实现更加复杂的功能。 阅读该仓库代码，我们了解到论坛的设计和开发等各方面的知识。 Github 仓库地址 supersetsuperset 是一个 Apache 开源的现代的、企业级商业智能 web 应用程序。它基于 flask-appbuilder 框架开发的。superset 的可视化能力超强，我们可以用其来做数据分析、展示和探索。 这个项目实在太优秀了，如果能啃下了，能力绝对提升不止一个档次。 Github 仓库地址 django-blog-tutorialdjango-blog-tutorial 项目是基于 Django 1.10 编写的博客网站。另外，它还是一个教程仓库。它通过 26 篇教程一步步带我们使用 Django 从零开发一个个人博客系统。 Github 仓库地址 jumpserverJumpserver 是一款基于 Django 框架编写开源的跳板机(堡垒机)系统，实现了跳板机应有的功能。基于ssh 协议来管理，客户端无需安装 agent。该仓库很适合运维工程师阅读学习。 Github 仓库地址","link":"/1290.html"},{"title":"Python 代码性能优化技巧","text":"众所周知，程序的性能好坏影响着用户体验。所以性能是留住用户很重要的一环。Python 语言虽然能做很多事情，但是有一个不足之处，那就是执行效率和性能不够理想。 因此，更有必要进行一定的代码优化来提高 Python 程序的执行效率。本文章主要是输出自己在 Python 程序优化的经验。 尽量使用内置函数Python 的标准库中有很多内置函数，它们的运行效率都很高。因为很多标准库是使用 C 语言编写的。Pyhton 的内置函数有： 原图比较大，排版原因可能导致字体看不清。在公众号后台中，回复【内置函数】即可获取高清图片。 拼接字符串运算符 “+” 不仅能用于加法运算，还能做字符串连接。但是这种效率不是很高。在 Python 中，字符串变量在内存中是不可变的。如果使用 “+” 拼接字符串，内存会先创建一个新字符串，然后将两个旧字符串拼接，再复制到新字符串。推荐使用以下方法： 使用 “%” 运算符连接 这种方式有点像 C 语言中 printf 函数的功能，使用 “%s” 来表示字符串类型参数，再用 “%” 连接一个字符串和一组变量。 1234fir = 'hello'sec = 'monkey'result = '%s, %s' % (fir, sec)print(result) 使用 format() 格式化连接 这种格式化字符串函数是 Python 特有的，属于高级用法。因为它威力强大，不仅支持多种参数类型，还支持对数字格式化。 1234fir = 'hello'sec = 'monkey'result = '{}, {}'.format(fir, sec)print(result) 上述代码使用隐式的位置参数，format() 还能显式指定参数所对应变量的位置。 1234fir = 'hello'sec = 'monkey'result = '{1}, {0}'.format(fir, sec)print(result) 使用 join() 方式 这种算是技巧性办法。join() 方法通常是用于连接列表或元组中的元素。 123list = ['1', '2', '3']result = '+'.join(list)print(result) 使用 generatorgenerator 翻译成中文是生成器。生成器也是一种特殊迭代器。它其实是生成器函数返回生成器的迭代。生成器算是 Python 非常棒的特性。它的出现能帮助大大节省些内存空间。 假如我们要生成从 1 到 10 这 10 个数字，采用列表的方式定义，会占用 10 个地址空间。采用生成器，只会占用一个地址空间。因为生成器并没有把所有的值存在内存中，而是在运行时生成值。所以生成器只能访问一次。 12345# 创建一个从包含 1 到 10 的生成器gen = (i for i in range(10))print(gen)for i in gen: print(i) 死循环虽然使用While True和while 1都能实现死循环，但是while 1是单步运算，所以效率会高一点。 123456# 推荐while 1: # todo list while True: # todo list 巧用多重赋值交换将两个变量的值，我们会立马想到应用一个第三方变量的方法。 1234# 将 a 和 b 两个值互换temp = aa = bb = temp Python 素有优雅的名声，所以有一个更加优雅又快速的方法，那就是多重赋值。 12# 将 a 和 b 两个值互换a, b = b, a 列表的插入与排序Python 标准库中有个 bisect 模块是内置模块，它实现了一个算法用于插入元素到有序列表。在一些情况下，这比反复排序列表或构造一个大的列表再排序的效率更高。 12345678910111213141516171819202122import bisectL = [1,3,3,6,8,12,15]x = 3x_insert_point = bisect.bisect_left(L, x)# 在 L 中查找 x，x 存在时返回 x 左侧的位置，x 不存在返回应该插入的位置# 这是3存在于列表中，返回左侧位置１print(x_insert_point)x_insert_point = bisect.bisect_right(L, x)# 在 L 中查找 x，x 存在时返回x右侧的位置，x 不存在返回应该插入的位置# 这是3存在于列表中，返回右侧位置３print(x_insert_point)x_insort_left = bisect.insort_left(L, x)#将 x 插入到列表 L 中，x 存在时插入在左侧print(L)x_insort_rigth = bisect.insort_right(L, x)#将 x 插入到列表L中，x 存在时插入在右侧print(L) 尽量使用局部变量Python 检索局部变量比检索全局变量快。因此, 尽量少用 “global” 关键字。","link":"/1291.html"},{"title":"账号认证那些事","text":"QQ 承载着我们一代人的青春和记忆，一个账号和密码就能体验 QQ 各种功能。而微信作为一款国民级应用，是每个人手机必安装的软件，同样也需要一个账号登陆才能使用。纵观各种社交应用、网站，往往都是离不开账号体系。 账号体系中有个重要的功能是账号登录，账号登录涉及到身份认证方法。我们在模拟登录网站去进行数据采集时，经常需要跟网站的身份认证“斗智斗勇”。因此，让我们来扒一扒其中的一些原理。 基本身份验证我们写的爬虫都是采用 HTTP/HTTPS 协议。HTTP 协议中有种名为 Basic Authentication（基本身份验证）的验证方式。这种认证方式在 HTTP/1.0 就定义了。 它的原理是在请求的 Headers 中增加 Authorization 字段，该字段的值是将“用户名：密码”的经过 Base64 编码之后的字符串；然后将其发送给服务器端做校验。 我们进一步理解 HTTP 基本认证的过程： 客户端发送 Request 给服务端。 因为 Request 的 Headers 中没有包含 Authorization 字段，服务器会返回一个 401 错误给客户端。 客户端把用户名和密码用 base64 编码之后，放在 Authorization header 中发送给服务器进行验证。 服务端将 Authorization header 中的用户名和密码取出来，进行验证，如果验证通过将根据需求发送资源给客户端。 基于 Session 的认证因为 HTTP 协议是无状态的，所以有人提出 Session 机制来维护服务器和用户之间的状态。 如果需要验证身份，那么在服务端生成用户相关的 Session 数据，然后发给客户端 Session_id 存放到 Cookies 中。客户端下次请求时带上 Session_id 就可以验证服务器端是否存在 Session 数据，以此完成用户认证。这种 Session 认证是网站平常用的比较多。 这种认证方式，可以更好的在服务端对会话进行控制，安全性比较高(Session_id 随机），但是服务端需要存储 Session 数据(如内存或数据库）。 基于 Token 的认证基于 Token（令牌）的认证已经变得更加普遍最近随着 RESTful API 的应用，单页应用程序和微服务的兴起。怎么理解 token 呢？当发出HTTP请求时， token 是验证用户是否有资格访问资源的凭证。 基于令牌的认证流程如下： 用户输入账号和密码。 服务器验证信息是否正确；如果正确则返回已签名的 token。 token 储在客户端，最常见的是存储在 local storage 中，但也可以存储在 Session 或 Cookies 中。 后面每次 HTTP 请求都在 Header 中带上 token。 服务端收到 token 通过验证后即可确认用户身，并且如果令牌有效，则接受请求。 一旦用户注销，令牌将在客户端被销毁。 基于 token 的用户认证是一种服务端无状态的认证方式，服务端不用存放 token 数据或当前 Session 的记录。这种方式相对 Cookies 的认证方式就简单一些，服务端不用存储认证数据，易维护扩展性强， token 存在 localStorage 可避免 CSRF ， web 和 app 应用这用接口都比较简单。不过这种方式在加密或解密的时候会有一些性能开销。 说到基于 Token 的认证，就不得不说下 JWT。JWT 全称是“JSON Web Token”，它是一种基于 Token 的认证标准。 OAuthOAuth 是一种认证协议，允许用户对没有密码的服务器执行认证。 OAuth 存在很多版本，OAuth 1.0，OAuth 1.0a 和 OAuth 2.0。OAuth 认证也是很好理解，就是一些网站或者应用支持第三方登录（QQ、微信、微博等）。 OpenIDOpenId 是另一种不需要密码的身份验证协议（类似于OAuth）。它们两者的区别是 ： OpenID 强调 验证 Authentication例如使用 QQ 方式登录知乎。最开始是要请求认证，用户输入 QQ 号和密码，点击登录。腾讯会先进行验证该用户是否为腾讯 QQ 的用户；如果是我的用户，那么腾讯 QQ 会通知知乎说“刚才登录是我腾讯 QQ 的用户，验证没有问题，可以你知乎社区”，这个过程就是认证（Authentication）。 OAuth 强调 授权 Authorization微信一开始是支持 QQ 方式登录，登录时微信会乎会询问允许获取你的 QQ 昵称、头像、性别等信息。这个就是授权（Authorization）。","link":"/1292.html"},{"title":"Python 居然有这么奇葩的库--The Fuck","text":"GitHub ，这个被广大网友戏称为全球最大的程序员同性交友网站。在程序员的圈子中，可以说无人不知，无人不晓。因为这个网站托管着一些开源项目。全球知名的大公司如谷歌，微软，腾讯，阿里巴巴等也会在 Github 上开源一些项目。可以说 Github 上有很多优秀的项目。作为一名程序员，在成长过程中往往离不开阅读别人的优秀代码。 猴哥每周都会定期去逛逛 GitHub 网站，暮色一些优秀的仓库。然后利用业余时间去学习消化。自己今天看到一个非常奇葩但十分实用的 Python 库，顺便分享给大家。 这个库就是 The Fuck。 相信很多人看到 fuck 这个单词，很容易会联想到骂人的话。其实不然，这个单词在外国有很多种意思。它可以当做语气词来用，表示非常的意思。比如你刚好要出门，突然就下起暴雨。这个可以用 the fucking weathe 宣泄自己的不满，而这短语表示的是非常糟糕的天气, 该死的天气。fuck 还有很多层意思，感兴趣的东西可以自行搜索下。 在学习和工作中，我们会经常在终端上执行各种命令行。如果输入的命令行错误，我们虽然很气，但是还是默默地再输入一次命令行。 nvbn 大神觉得这处理方式很不爽。于是乎就有了 The Fuck 库。The Fuck 这个库的作用自动纠正输入错误的命令行。如果我们输入的命令行有错误，在控制台输入 fuck，终端会自动纠正错误的命令。 看完例子，相信大家一定是迫不及待地想去安装下，然后好好体验一番。这个库是用 Python 编写的，所以我们可以使用 pip 方式去安装。 1pip install thefuck 如果你想深入了解该库，阅读其源码，可以访问该库的 Github 地址。","link":"/1083.html"},{"title":"挥别2018，起航2019","text":"我的 2018 年总结。 流光容易把人抛，红了樱桃，绿了芭蕉。 2018 年已经落下帷幕。每当年末年初时，我都会对过去一年做下总结。总结下自己在逝去的一年究竟收获什么。这算是对过去一年的交代，也是对自己一个交代。 2018 的收获学习体系化我收获到第一个关键字是学习体系化。说到学习，自己一直以“不日进则日退”激励自己。因为自己内心一直很害怕失败，所以一直在学习，总是希望自己能做得再更好一点。而我自己的知识来源途径有书籍、博客、公众号等。简单来说，自己遇到不懂的知识，就利用搜索引擎去了解和学习。 但是，这种知识是比较碎片化的。通过这种方式，自己可能会对某个“点”的知识内容很清楚。如果知识内容上升到“线”的层面，自己会很茫然。这像一盘散乱的珠子。你每一次只能拎出“一个”即止，而不是连续的“一串”。 因此，体系化是学习的正道。学到的东西只有纳入自己的知识体系中，才算是为自己所用。不成体系的零碎知识是没有任何价值的。 现在自己接触到新的领域知识，会先画出该领域的思维导图，然后对每个细的分类进行学习。 深度思考职场上有一句很流行的话「不要用战术上的勤奋掩盖战略上的懒惰」。 当自己听到这句话后，细思恐极，发现自己的勤奋是一种低质量的勤奋。 自己上半年工作状态大概是有新的工作任务安排下，自己完成就完事了，然后就去解锁新的工作技能。而下半年最大的改变是自己会进行深度思考。自己不仅会思考如何完成任务，还是思考自己为什么这么做，思考如何做得更好，思考自己还能为团队多做些什么贡献。 对于每件事，我们深入思考之后，真的有很多东西会被挖掘出来。 写作公众号算是我生活中的支线。我将其作为提升写作能力，知识输出的平台。自己很享受沉浸在写作的状态。不知不觉，自己也写了 69 篇原创文章。 起航 2019持续学习还是那句话，「一天不读书，感觉像头猪」。无论是工作还是生活，自己会通过学习新的知识来填补短板,提高自己的综合竞争力。 持续输出自己公告号不仅持续技术文章，还会输出自己一些所思所想。 最后，感谢你还在这里~","link":"/193.html"},{"title":"如何发现并参与开源项目","text":"去年 6 月份，一个被广大程序员戏称为“全球最大同性交友网站”的 Github 网站，被微软收购了。在当时 IT 界，这算是轰动一时的大事件。 因为 Github 不仅仅是一个编程工具，一个仓库托管网站，更是开源社区的核心。据微软称，GitHub 拥有 2800 万用户，8500 万个代码库。现在，开发工程师越来越喜欢在 Github 上写作，仓库的主题也不限于代码，例如购房踩坑记录、科技资讯周报、面试指南、书籍博客资源翻译、个人网站等。 Github 可以是个知识宝库。那本文主要内容是介绍如何上手 Github 网站。 什么是 Github ?Github 网站上有很多开源项目，每个项目都是一个 Git 仓库，而 Github 就是一个托管 Git 仓库的网站。毫不夸张地说，Github 迄今为止世界上公开的托管开源项目最多、代码最全、涵盖技术生态最全面、聚集牛人最多的平台。 我们为什么需要 Github？Github 本质上是个分享社区。所以我们可以把它当做个写作平台，记录自己编写的代码、记录自己学习的总结等。如果输出的内容很有价值，会引起别人的关注并 start，自己会更有动力去创作，这是个正反馈的过程。 另外，自己在一些知名度比较高的开源项目上贡献过代码，或者自己有一些高质量高赞的项目，是面试的加分项。如果只是单纯注册个账号，想在面试的时候“秀下肌肉”，最好打消这个念头。 版本管理个人独立开发软件时代已经不复存在，现在软件开发都是团队协作。在团队中，每个人可能负责某个或多个模块。如果没有引入版本管理，某些特定代码改动导致项目出现异常，那排查问题就需要彻底审查整个项目，耗时耗力。版本管理的好处是经常性地保存着项目的改动，方便跟踪项目的改动。 版本管理有一些专门的版本控制系统，例如 SVN、Git 等。SVN 是老牌的版本管理系统。有些公司选择它是因为 SVN 有自带权限管理，能对不同的用户设置不同的权限。如测试同学只有查看权限，开发同学拥有提交、查看等权限。但它有个缺点，它的需要保证代码服务器正常运行，一旦服务器挂了或数据丢失，则无法正常访问。这是 SVN 本身是集中式版本控制系统所致。 Github 网站的版本管理使用的 Git，它是一个分布式版本控制系统。既然是分布式管理系统，每个终端既是服务端又是客户端。那么理论上一台电脑拥有 Git 仓库，其他人都能随意拉取和推送。但实际情况，很少在两人之间的电脑上推送版本库的修改。所以，Git 通常也有一台充当“中央服务器”的电脑，但这个服务器的作用仅仅是用来方便“交换”大家的修改。 入门 GithubGithub 网站的代码仓库都是远程 Git 仓库，所以需要先熟悉 Git 相关的知识。如果你不熟悉 Git，可以去看廖雪峰大神的 Git 教程。 我们平时最频繁地操作也只分为两种，一种是从 Github 网站下载代码仓库地址到本地，另一种是对本地仓库进行修改，然后提交修改，最后推送到 Github 上。 克隆仓库到本地的命令 1git clone https://github.com/monkey-soft/SchweizerMesser.git 如果你没有远程仓库，可以到 Github 网站上可视化创建下。当然，克隆的仓库可以是自己的，也可以是别人的。如果想在别人的项目中共享代码，需要在 Github 上 fork 别人的仓库，相当于自己基于当前版本新建一个分支。 提交修改并推送到远程仓库 如果远程仓库的拥有者是自己，可以先增加文件，然后再提交修改到本地仓库中； 12git add test.pygit commmit -m &quot;add test.py&quot; 最后才能推送到远程仓库 12345678# 先关联远程仓库git remote add origin https://github.com/monkey-soft/SchweizerMesser.git# 关联后，第一次推送需要增加 -ugit push -u origin master# 后续的修改，直接推送即可git push origin master 请求合并 如果远程仓库的拥有者是其他人，自己推送只是更新自己 fork 分支。如果想别人采纳自己共享代码，这时就需要使用 pull 命令。 1git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; 现在的 IDE 工具都内置了 Git，如 Android Studio、Pycharm 等。当熟悉 Git 的工作流程之后，可以使用可视化操作。另外，Github 官网也推出了桌面应用 GitHub Desktop，同样也是可视化操作。 发现有趣的项目Github 上项目成百上千，想在上面寻找自己需要的项目还真需要点技巧。 第一种办法是使用 Github 的探索功能，点击“Explore”按钮。在这之后，你会看到 Github 会根据自己的喜好等推送有一些项目。 第二种办法是关注一些活跃的 Up 主。如果你向机器方向发展，可以关注这方面的活跃用户。因为他们会经常发现一些高赞的项目，我们可以通过好友动态去了解。","link":"/297.html"},{"title":"图表类型，你选对了吗？","text":"2019 年已悄然过去一个星期，不知你是否有新的收获？ 而自己在新年的头一周，一方面忙于工作，试着挑战一些更高难度的工作；另一方面在积极“充电”。自然而然公众号就拖更了。不过值得庆幸的是，今天更新了。今天给大家分享的内容是如何准确选择图表类型。 虽然文字能很生动形象地描述出一件事或一个人，但是它在数据面前则显得有点吃力。因为文字描述一些数据时，需要人们去理解，在大脑中做对比。而图表具有集中、概括、便于分析和比较的特点，能给人一种直观、清晰的感觉；因此，在数据表示方面，图表比文字更适合。 图表类型微软公司在数据图表显示这方面可以算是行家。Excel 作为 Office 三剑客之一，它提供着一些丰富的图表类型。常见的图表大概能分为 8 种，分别是柱形图、折线图、饼图、条形图、面积图、X Y（散点图）、曲面图、雷达图。 如果将上述图表进行细分，还是划分出很多子类型图表。如： 当然，万变不离其宗。不管子类型图表怎么演变，还是属于上述 8 种图表。大概变化规律有四点：1）二维图形变成三维图形。2）横纵坐标表示值发生改变，如从具体数值变成百分比。3）图形的叠加，如堆积柱形图、簇状条形图。4）增加特殊标记，如直线、特殊点的值等。 上述的分类是按照图表的形状来分类，还有一种分类是按照数据呈现的关系（或者说功能性）来分类。大概能分为以下几类： 1）趋势图趋势图是最基础的图表，包括折线图、柱状图、堆积图等多种形式。 2）占比图占比图反应出不同部分占据总体的百分比。这类图形有饼图、环形图、百分比堆积条形图等。 3）对比图对比图反馈是两种或多种事物之间的差距。常见图形有柱形图、条形图等。 4）关联图关联图能呈现出维度之间的联系。散点图反馈两个变量之间存在某种关联。雷达图反馈是多个维度数据之间的关系。 图表作用我们根据亿图提供的图表来详细了解各个图表的作用。 柱状图柱形图适于比较数据之间的多少。另外，柱状图可以用来观察某一事件的变化趋势。如果将整体拆分可以做成堆积图，同时观察到部分所占比重及变化趋势。 条形图条形图显示各个项目之间的比较情况，和柱状图类似的作用。柱状图是纵向显示，条形图是横向显示。 折线图折线图可以观察一个或者多个数据指标连续变化的趋势，也可以根据需要与之前的周期进行同比分析。 饼图饼图主要是用来了解不同部分占总体的比例。 环形图环形图是用来显示部分与整体关系的图。 面积图面积图可以用来展示变化的幅度。 散点图散点图主要作用是判断两个变量（XY）之间的是否存在关系或者度量关系强弱。 雷达图雷达图可以用来表现一个周期数值的变化，也可以用来多个对象/维度之间的关系。 气泡图气泡图判断三个变量之间是否存在某种关系。它跟散点图有点类型，只不过气泡图以气泡大小作为新的维度。 词云词云可以用来显示词频，可以用来做一些用户画像、用户标签的工作； 漏斗图漏斗图用来表示逐层筛选的过程，主要用于转化过程。 地理图地理图呈现出跟地理相关的图。可以用来显示气温和地理的关系等。常见有区域地图、散点地图、热力地图等。","link":"/194.html"},{"title":"该死的拖延症","text":"月初，自己给公众号指定出新的目标。在不影响技术文章的输出情况下，输出一些自己所思所想。 本文是第一篇自己思考的文章，可能写得不够好，请见谅，毕竟是新的尝试。但这也算是自己的输出。在之前，我只是将重点以笔记的形式记录下来自己的思考。 第一篇文章，让我们来聊聊拖延症。 我在知乎看到过一个关于拖延的精彩回答，说得非常好，我就直接引用。 拖延，是心理上的一种逃避，当人们做的事情有困难，不经济，短期看不到反馈，那么人们就会想尽办法去拖延，知道 deadline 前的一段时间内，才匆匆忙忙地区完成整个任务，这件事情。 不可否认，人都有拖延行为，只不过程度的深浅。比如: 寒风刺骨的冬天，本该到了起床时间，自己跟自己说再睡 10 分钟就起床。再如: 周末，自己本来计划在家写篇工作报告，但被一部精彩的电视剧所吸引，自己一直跟自己说看完这集就去写报告，结果一拖再拖，周末就过去了，报告也没有写完。 拖延症其实慢性毒药。因为拖延症会慢慢腐蚀高效的状态。 举我自己亲身的例子。我每天到公司第一件事，就是把自己需要做的事情罗列出来，然后设定优先级，再开始工作。而我的工作模式是”单线程”模式，每次做完一个任务之后，才开始做第二任务。 正因为这样，当前正在做的事情被打断或者有新的任务来临时，自己只能放下手头工作，想处理紧急的事；或把任务安排到最后。如果频繁被打断，那么会造成恶性循环，有些事情会越拖越久。 之前团队来了几个新同学， leader 让我写篇关于《自家产品的新人指南》的资料，需要把一些基础知识，产品的一些原理性东西总结出来。但因自己手头上事情比较多，内心有点小抵触，所以这个任务一直拖着。等到最后 leader 来催，我才匆忙把资料写完。 结果可想而知，资料写得太捞，自己被 leader 批了一顿，然后要求重写。那时候快到了国庆假期，自己只能利用整个假期来重写。 之后，自己复盘这件事，总结自己没有做好的原因。表面上看是自己拖延导致，其实是自己对这件事不上心。这个就是自己拖延的根本原因。因此，想要克服拖延症，自己必须要对所做的事多花点时间和心思。 PS：在工作中，“单线程”的工作模式其实很难受，同时也是很难专注做一件事，因随时都有突发情况。自己也是往“多线程模式”靠拢，先对当前的任务花费一个小时，然后暂停当前工作，切换到另外的任务中。","link":"/196.html"},{"title":"Vlog 是短视频发展的新催化剂？","text":"上周末，我逛下 B站，偶然发现 B 站打出“在 B 站，用 vlog 记录不平凡”的活动。这引起我浓厚的兴趣，于是乎，就有了今天这篇思考文章。 这两年来，短视频行业发展可以是爆发增长。这期间诞生很多成功的产品，例快手、抖音等。 快手诞生比较早，它凭借 9 年时间的运营积累，现成为用户量最多的短视频应用。而风头正劲, 非抖音莫属。抖音利用视频结合背景音乐作为爆点，实现用户量指数增长。这也逐渐形成“北快手，南抖音”的格局。 但是在 2018 年下半年，抖音虽然用户量在增长，但是增长速度变缓慢，有趋于稳定的趋势。不过这也符合一款产品的发展趋势。任何一个产品都有其生命周期，一款产品发展趋势大致划分为 4 个阶段。 第一阶段：启动期——功能、内容不全，用户极其稀少； 第二阶段：扩展期——内容和用户开始迅速增长； 第三阶段：稳定期——内容生态规则基本完善，内容仍在增长，但内容消费频次降低，用户增长放缓； 第四阶段：衰退期——内容和用户都开始减少。 另外，当产品处于稳定期时，如果产品还能找到新的增长点，还能迎来新的扩张期。 纵观全局，短视频行业市场、用户增长增速变缓。我从中国产业信息网站了解到，截止到 2018 年，短视频的用户规模已经达到 3.53 亿人，预计 2020 年达到 6.67 亿。 市场资本规模在 2018 年突破百亿大关，预计 2020 年能达到 350 亿。 近来刮起全民“vlog风”又是什么鬼？这里要先了解什么是 vlog。 vlog 全称是 Video Blog，即视频博客。它是一种以视频形式记录生活、学习、工作的点点滴滴。vlog 着重记录的是创作者自己的生活，以创作者为中心输出视频内容。它的内容形式可以是一场旅行、一场表演、一次烹饪、一个开箱秀等等。 我为什么说 vlog 是短视频发展的催化剂？ vlog 的时长没有很明确的界定。而短视频顾名思义是短小的视频，视频时长从 15 秒到 60 秒不等，最长也在 60 秒左右。因此，vlog 时长把控会更加灵活，能根据创作者场景的需要，制作适当时长。目前我关注到的 vlogger 的大多数视频时长是 3 到 5 分钟。这个也跟网传抖音正在内测长达 5 分钟的长视频相呼应。 vlog 是以视频为表现手法。所以创作者离不开视频拍摄，视频剪辑。手机具备现场拍摄、现场编辑、现场发布等优势，另外有抖音、快手、VUE 等现成应用工具，这大大降低普通人成为一名 vlogger 的门槛。 现在是自媒体时代，一个普通人可以借助自媒体平台来展示自己，打造个人 IP，成为一名网络达人。传统的表现手段是文字，视频方式则比较新颖，也鲜为人用。 同时，vlog 也算是的一种原创内容输出。现在一些平台对原创内容的博主扶持很大，例如 B 站。一共 3 个扶持计划，有“创作激励计划”，当博主满足申请条件（1000 粉丝或 10 万播放量时），以后每 1000 个播放量，B 站都会给予一定的收益；有充电计划，粉丝可以给博主进行充电，类似微信的赞赏功能；有悬赏计划，博主加入悬赏计划后，即可在视频播放页加入一些广告来收取广告曝光费用。当然，博主也可以通过接广告形式赚取收入。 既然 vlog 是现在的热点，我能该如何成为一名 vlogger？ 你只需要一个手机，一个平板等这样的拍摄设备，几个专业视频编辑软件，然后寻找自己擅长的领域，最后专注输出视频内容。","link":"/398.html"},{"title":"说说近期自己的感悟","text":"四月中旬，我更新一篇自己的总结，之后就没心没肺的断更了两个月。我估计能等到今天还没把我删掉的读者应该都是真爱了。用了一段时间来寻觅到真爱，我倒也不觉得亏本。本文主要是自己近期的总结心得，希望能对你有所帮助。 抓住事情本质，做个明白人工作和生活总是不停给我们创造问题，而我们需不停地解决问题。 解决方案纵然有千万种，但解决核心问题才是关键。如果开始就没能找到真正的问题！后面的分析再精彩、解决技能再高超只是枉然罢了！ 好莱坞经典电影《教父》里有一句话说得很好：“花半秒钟就看透事物本质的人，和花一辈子看不清事物本质的人，注定是截然不同的命运。” 而我身边刚好有这样的同事，他不仅个人能力超强，而且情商非常高。他有着一眼见底的洞察力，能把每件事看得很透彻，分析出问题的关键点，涉及的那些人，以及如何扫清障碍。 自己前段时间看到一本书，书名为《遥远的救世主》，后被改编为电视剧《天道》，感兴趣的小伙伴可以去看看。自己看完深有体会，不禁感叹人与人最大的差别在于思维。 重要的事情不过二三近来一段时间，工作和生活上的事越来越多，我渐渐忙不过来。自己分身乏术，恨不到将 1 小时掰成 2 个小时使用。因太想把每件事做好，最后每件事都不能办得让自己感到满意。每天自己都是拖着疲惫的身体奔波于公司家两点一线，反反复复。自己想着每天事情很多，都搞得自己很累。 于是乎，我琢磨着能不能给自己“减负”。 我翻阅近一个月的日报，整理自己的每天所做的事情，分别对每件事进行两点思考。一是这件是价值点在哪里？二是这件事今天能完成，如果延迟做又有什么影响？最后发现，每天堆集成山的事情中，重要的事情不会超过三件！因为搞定三件事对我的价值最大的。 在网上搜索一番，了解到时间管理有一个著名的“二八原则”。所谓“二八原则”，是 19 世纪末20 世纪初，意大利的经济学家帕累托发现的：在任何一组事物中，最重要的只占其中一小部分，约 20%，其余 80% 尽管是多数，却是次要的。 为了将事情的价值最大化，我们应将 80% 的精力放在 20% 的重要的事情上。 知其然，更要知其所以然这句话来源于“知其然，知其所以然”。大概意思是只知道事物的表面现象，也知道事物的本质以及产生的原因。但我更加重视思考事物的本质会让产生原因。这也人们常说的凡事多问个为什么？这有助于我们递进地深入思考。 举个栗子，TCP 协议是三次握手已熟记于我们大脑中。但多少人知道为什么是三次握手？怎么不是两次握手，四次握手？这会趋势我们深入去研究，会发现 TCP 协议建立连接有状态转换的过程。再经过一番研究，这又引发出新的思考， TCP 状态变化对应 Socket 连接过程是怎么样？随着思考越深，慢慢地会发现自己对某个事物理解很透彻了。","link":"/6101.html"},{"title":"“快速”掌握一门新的技能","text":"Hi, 老朋友们。 几个星期不见，甚是想念。距离我上次发文，已经过去四个星期。我在三月初发文宣布 come back，但因团队接手新的项目，对于我来说是全新的技术栈，所以我只能牺牲个人业余生活，把大量空闲时间用于学习新的技术。借着这次总结机会，分享自己如何“快速”掌握一门新的技术。 思维转变我们每个人都经历学生时代。学生时代的目标很明确，就是单纯地搞好学习，考个好成绩。同时，业余时间也比较充裕，比较集中，特别是大学的生活。如果你还在求学阶段，好好珍惜现在的时光。当参加工作之后，这一切都改变。时间会被碎片化，而且工作更强调我们的产出。没有一个公司会养闲人。公司花一定的薪水雇佣我们，目的是要我们给公司创造价值，创造收益。能力强弱决定我们的职场能走多远。我认为最重要的能力是快速学习。 理清脉络当公司或团队因业务扩展，可能会使用新的技术栈，但这技术跟平时我们接触的不太一样。这个时候是最考验学习能力的时候。我的学习路线是以目标为导向，理清脉络，再扣细节。首先把最终学习成就定下来，再把新的技术涉及的方方面面内容都列举出来，然后逐个击破。这就是为什么书籍、教学视频都会有提纲的原因。 例如，我们想利用业余时间来学习 Python 做后台。我们先假设学习目标是 Python 后台支持高并发的 RESTFUL 架构后台。接下来的工作就需要理清知识点。 没有接触过 Python 语言，需要先把基础知识先过一遍。 接着再考虑业务后台逻辑，分析业务重点和实现卡点。然后了解各大主流框架的特性，最后选择最优方案。 指定计划指定计划的目的是为了拆分各个学习任务，安排每天适当的学习量，从而保证任务如期完成。同时，这种全局观念有助于我们了解自己当前学习进度，是进度超前还是进度落后，以便我们做出相应的调整。再者，人是有惰性的，都想着能轻松点。这种方法还能克制下偷懒的心。 我习惯使用 Eexcl 表格来跟踪记录。我每天睡觉前花 10 分钟回忆下，然后更新表格。这样自己知道自己的情况，做到对自己知根知底。 大量时间古人云：冰冻三尺非一日之寒。学习也是逃不出这真理。如果真的想把一门技能吃透，真的需要靠大量时间的堆积。所谓的“快速掌握”只不过是在我们在搭建知识体系。所以不要被“快速”懵逼双眼。技术很注重细节，细节要深究，深究就需要花费时间成本。 以上内容是我自己关于学习的一些思考和总结。如果你觉得不错，可以素质三连一波。","link":"/3100.html"},{"title":"如何快速上手熟悉业务？","text":"也许你会有这样的经历，刚跳槽到新公司，开始接手新的项目，熟悉业务。当你发现代码易读性不高，注释也是简单几句；这确实是一个不小的挑战。硬着头皮去啃代码，然后来熟悉业务。这方法看似乎可行，但缺会消耗大量的时间和精力，故此策是下策。那何为上策？上述的方式是以点线去摸索整个面。那我们可以换种思考方式，从整体的角度去看到业务，再逐步去熟知各个部分细节。 先以产品功能角度分析产品从面向客户群体可以分为 ToC 产品和 ToB 产品。ToB 产品，B 是表示 Business，即 ToB 产品则是面向企业的产品。ToC 产品是 To Consumer 的缩写，即面向消费者的产品。但无论是 ToC 还是 ToB 产品，都是给客户使用的。 ToC 产品的范畴有类似 12306 购票网站、类似微信 APP 等。这类产品功能比较具体化，所以给用户感觉很直观，容易理解。而 ToB 产品更多是以接口、解决方案等方式提供，会比较抽象，如微信开放平台、淘宝卖家平台、网络安全解决方案、电商业务解决方案。 拆分业务最直接的方式是体验。以使用者的角度入手，遍历产品个各个功能项，然后进行分类工作和绘制思维导图。 分类可以从功能关联性、用户使用率、时序逻辑等不同角度来进行。 功能关联性指功能之间存在某种联系。如购票网站的注册功能和登录功能存在依赖关系，如果是新用户就需要先注册后才能去登录。 用户使用率可以通过统计使用功能的频率。 如果时序逻辑角度入手，重点关注是整个流程。如：分享一篇微信公众号文章到朋友圈，流程是浏览文章 -&gt; 点击分享 -&gt; 填写分享内容（可省略）-&gt; 确认分享。 最后一步就是绘制产品功能项的思维导图，绘制产品功能的思维导图，导图的目的不是为了单纯列举功能项，是为了找到产品的核心竞争力。一个产品如果各项功能之间都是分散的，只能叫工具箱。绘制思维导图按照产品四个功能维度去划分。 核心功能 必不可少的核心痛点功能，缺了就不能构成产品的基本能力。例如：社交软件的用户沟通功能，12306 网站的购买车票，文本编辑器能处理文本。 附加功能 比不可少的痒点功能，但不能带来收益。例如：闹钟支持更换响铃，浏览器附带下载管理。 增值功能 痒点功能，能够带来收益的功能。例如：腾讯视频VIP，百度网盘会员，MS Office 365订阅 非必须功能 并不是必然要实现的功能（有不有都无所谓的功能）。例如：iOS 低电量警告音，车载中控的游戏功能，短信骚扰拦截后通知。 再看技术实现通过将产品拆分为各个功能模块，我们已经对其从整体上的有了大概认识。但这些还远达不到拆分业务的要求，需要我们从技术实现角度往深入去剖析。技术实现层面可以分为产品架构、技术层级两个方面。 一款产品在一开始设计架构时，往往是花费时间。产品的架构相当于整座高楼大厦的钢筋骨骼。架构设计好的话，产品稳定，可扩展性强，可维护性高。产品架构分析是描述各个业务模块之间的关系。因此，同样也通过绘制产品的架构图加深理解。我在谷歌官网找到 Android 系统整体架构图。 技术层级是逻辑概念，指将业务进行分层。有句流行语挺有意思的。在软件行业中，如果分层无法搞定，那就再分一层。这也是我们常说的设计模式。例如常见的 MVC、MVP、MVVM 设计模式。目的是解决界面 UI 呈现和业务逻辑代码分离，所以拆分为视图、业务处理层、数据存储层。 最后研究技术方案技术方案是各个抽象出来的需求和功能转为化一行行代码的指令。也可以简单理解为实现方式。举个栗子，有个业务功能需要客户端向服务器获取数据，采用 HTTP 短链接方式不停去请求服务器？还是 WebSocket 长链接去轮询？","link":"/7102.html"},{"title":"开发一个操作系统有多难？","text":"今年上半年，谷歌因为遵循美国政府命令，需要中断与华为的业务往来，不再给华为提供 Android 服务。这也意味着未来华为仅限于使用公开、开源版本的Android系统。如果撤销 Android 系统使用许可，将意味着华为会立即失去接收包括安全更新在内的 Android 系统更新的资格，Google Play商店、Gmail、地图、YouTube等相关应用将不能使用。虽然国内的用户不受影响，但是这会给海外用户致命的打击。特别是欧洲市场，毕竟华为是欧洲市场上第二大手机供应商。 面对这种困境，华为以“华为一直在开发操作系统，名为鸿蒙”面对。但鸿蒙一直处于“犹抱琵琶半遮面”的状态，直到上周末。华为才正式在华为 2019 开发者大会上揭盖鸿蒙的面纱。 华为借着 5G 刚起步的契机发布自主研发系统，在业务造就一方声势。我个人保持支持的态度，希望鸿蒙在下一次能给我们带来一些新的东西。接下来，本文是介绍自己对操作系统的一些理解。 操作系统是什么？我在读大学的时候，有一门《操作系统》的课程，授课老师是一位老教授。他姓龚，人和蔼可亲，我们都称他为“老龚”。因为老师人好、授课方式让人很容易接受，所以我这门课学得比较好。我现在还记得，当时老龚对操作系统的定义。 操作系统是控制和管理计算机硬件和软件资源、合理地组织计算机工作流程以及方便用户使用计算机的一个大型程序。简单来是，操作系统是一个大型软件，它内部会有很多设备的驱动程序。这些程序会将各种硬件设备（CPU、内存条、硬盘、外接设备等）运行起来；另外它还给用户以图形化界面或命令行终端提供交互方式。 操作系统都是有一个核心，我们称之为内核。内核负责系统的进程调度、作业调度、文件管理、设备管理、用户接口管理。内核分类常见有宏内核和微内核。华为的鸿蒙系统是微内核，虽然微内核已经不是新鲜事了，但还是有别于 Android 系统。辨别微内核和宏内核的办法是否将设备驱动写到核心中。Minix、Windows、MacOS 都是微内核，而 Linux 系统则是宏内核。这种内核差别，我们在用户层面也是能感受到的。例如Windows 系统能让用户自主安装，卸载，更新设备驱动；Windows 10 上能运行多个 Linux 系统，这对内核来说都自是子系统而已；说到 Linux 系统，很多人会吐槽 Linux 系统安装显卡驱动贼难。原因是有些厂商（例如：NVIDIA）不愿意提供开源驱动 。 操作系统是分为内核态和用户态。 内核态也叫做管态、核心态，或者超级用户模式。它拥有所有硬件的完全访问权限，可以执行任何机器指令 ，并且可以访问系统中存储器的位置。如果不了解存储器概念的同学，可以去搜索下计算机原理。因为我们生活和学习用到的计算机都是基于冯诺依曼架构实现的。这里就不多做介绍。用户态是只使用机器指令的一个子集，一般应用程序（游戏客户端等）都是运行用户态。用户态是禁止运行影响及其的控制或可进行 I/O 操作的指令。 难在编译工具链这里需要先了解下本地编译和交叉编译。本地编译是指在当前编译平台下，编译出来的程序只能放到当前平台下运行。例如在 Windows （x86 架构）上编译 EXE 的应用程序，这个 EXE 也只能跑着 Windows 系统上。交叉编译则是在当前编译平台下，编译出来的程序能运行在系统结构不同的目标平台上。例如在Windows（x86 架构）上编译出 Android 系统（ARM 架构）的应用程序。 了解交叉编译，来看看什么是交叉编译链。交叉编译链是为了实现编译跨平台架构的程序代码所构成的工具集。交叉编译链主要由 Binutils、GCC、GLibc 三个部分组成。工具集有免费版和付费版。目前免费版本主要由三大主流工具商提供，第一是 GNC（以源码形式提供、需要我们手动编译制作），第二是 Codesourcery，第三是 Linora。付费版有 ARM 原厂提供的 armcc、IAR 提供的编译器等。 因此，制作一个操作系统的难点之一是如何搞定交叉编译工具链接。再者，编译器的用户群体是应用开发者，会有使用不同平台去开发应用程序。华为推出方舟编译器，称能通过多终端 IDE 开发环境来支撑鸿蒙系统。 难在驱动操作系统需要各种硬件驱动程序来使硬件完成正常的工作，例如让摄像头拍照、声卡播放声音等。如果驱动程序不够稳定，那会引发各种不兼容情况。如显卡驱动不能完全设配显卡，可能会出现玩游戏时画面卡顿、播放视频出现花屏或绿屏等。 应用软件难操作系统最终是面向与消费者。如果一个操作系统没有数量足够多且稳定可用的应用软件，那也是无尽于是。试想一下，如果一个手机只能打电话和发短信，不能安装微信、不能追剧看视频、不能安装游戏，谁还会去买这个手机呢？","link":"/8103.html"},{"title":"学会时间管理，让自己少些焦虑","text":"什么是时间管理？就是利用时间宝石查看久远的过去或者遥远的未来所发生的事，甚至还能操纵时间，实现时间循环。 嗯？不对呀，这走错片场了。你没走错，刚才是个玩笑。 说到时间管理，本质上不是对时间本身进行管理，而是在有限时间内，优先选择完成有价值的事。 大多数人对时间管理的最大误解是提高工作效率，增加对时间的利用率。单纯的增加效率，只会让事情越来越多，工作越来越忙。 我一开始也是这么认为，虽然将番茄工作法运用起来，每天早上安排每天需要处理的事，但感觉每天还是很忙；而且工作还经常被各种其他被打断，感觉自己工作状态不是处于打断中，就是在处在被打断的路上。 当自己专心处理一件事时，突然有紧急的事情需要去处理。虽然我们在处理另外一件事，但是大脑还会想着以前处理的事。如果频繁有事情打断目前专注做的事，我们大脑会在不同的事情上来回切换，这会让我们大脑越来越疲劳，导致结果是重要的事情都完成不了，这也间接产生了焦虑。 《中国合伙人》里有个场景，成东青被学校开除，在肯德基里开英语辅导班，艰难的创业之路刚刚开始，他邀请好友王阳合伙。王阳说了一句卡内基的话，成东青惊奇地问，你不是说卡内基是个骗子吗？王阳说到其实最大的骗子是我们自己。因为我们总是想改变别人，而拒绝改变自己。 因此，自己想着如何改变这种工作状态，然后自己就了解时间管理的知识。 在《高效能人士的七个习惯》一书中，作者史蒂芬•柯维提出，“重要事”和“紧急事”的差别是人们浪费时间的最大理由之一。因为人的惯性是先做最紧急的事，但这么做会导致一些重要的事被荒废掉。 李开复老师曾经也分享过时间管理的经验。处理每件事件之前，先将事情放到时间管理的四象限图中进行划分（紧急程度当做横轴，把重要性当做纵轴），然后按照第一象限 &gt; 第二象限 &gt; 第三象限 &gt; 第四象限的顺序处理待办事项。这种办法能很大程序上保护自己的时间，尤其需要有足够的时间做那些“重要事”。 最后，我分析自己总结的时间管理的方法。 1.每天上班前先制定每天需要工作的内容，这部分工作主要是重要非紧急的事情，因为这些是做这些事是对自己有提高且会带来价值的事。2.每天上班到公司，9点到10点这段时间大家都还没有进入到工作状态，这段时间相对比较完整。所以我会利用这段时间处理一些每天要花费比较长的时间和精力的事情。3.剩余时间使用番茄工作法工作，每两三个番茄处于非紧急但重要的事中穿插一个番茄钟，专门用来处理各种紧急且重要的事情。4.每天下班前，复盘总结今天工作完成情况。","link":"/8105.html"},{"title":"一篇文章彻底了解HTTP发展史","text":"HTTP 协议可以算是在人们日常生活、工作用得比较多的协议。我们使用浏览器访问网页，就是通过 HTTP 来传递数据；客户端跟服务器交互，大部分会使用到 HTTP 协议。对于我们做数据采集的人来说，也是再正常不过。Requests 和 Scrapy 都是对 HTTP 进行封装的支持自定义配置的库。互联网工程任务组（IETF）在去年提议将 HTTP-over-QUIC 重命名为 HTTP/3。我们是做技术的，需要保持一定敏感度。一旦 HTTP/3 标准被定下来，各大产商会相继支持，那会给我们带来什么影响？需要我们回顾下 HTTP 的发展史。 HTTP 0.9我们把时间拨回到 1991年， 万维网协会（World Wide Web Consortium，W3C）和互联网工程任务组（IETF）制定了 HTTP 0.9 标准。因为那个年代互联网还在普及，加上网速带宽低，所以 HTTP 0.9 只支持 GET 请求。 HTTP 1.0时间来到1996 年 5 月，HTTP/1.0 版本发布，HTTP 协议新增很多内容。首先是请求方式的多样化，从单一的 GET 请求，增加了 POST 命令和 HEAD 命令。除此之外，还支持发送任何格式的内容。这两项新增内容，不仅使得互联网不仅可以传输文字、传输图像、视频、二进制文件，还丰富了浏览器与服务器的互动方式。这为互联网的大发展奠定了基础。再次，HTTP请求和回应的格式也变了。除了数据部分，每次通信都必须包括头信息（HTTP header），用来描述一些元数据。其他的新增功能还包括状态码（status code）、多字符集支持、多部分发送（multi-part type）、权限（authorization）、缓存（cache）、内容编码（content encoding）等。 但 HTTP/1.0 还是存在缺点： 第一点是：连接无法复用。HTTP 1.0 规定浏览器与服务器只保持短暂的连接，浏览器的每次请求都需要与服务器建立一个TCP连接，服务器完成请求处理后立即断开TCP连接，服务器不跟踪每个客户也不记录过去的请求。如果还要请求其他资源，就必须再新建一个连接。 第二点是：Head-Of-Line Blocking（HOLB，队头阻塞）。HOLB 是指一系列包（package）因为第一个包被阻塞；当页面中需要请求很多资源的时候，HOLB 会导致在达到最大请求数量时，剩余的资源需要等待其它资源请求完成后才能发起请求。这会导致带宽无法被充分利用，以及后续健康请求被阻塞。 HTTP 1.1W3C 组织为了解决 HTTP 1.0 遗留的问题，在 1997 年 1 月，发布 HTTP/1.1 版本，只比 1.0 版本晚了半年。它进一步完善了 HTTP 协议，一直用到了20年后的今天，直到现在还是最流行的版本。具体优化点： 缓存处理。在 HTTP 1.0 中主要使用 header 里的 If-Modified-Since,Expires 来做为缓存判断的标准，HTTP 1.1则引入了更多的缓存控制策略例如 Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用。针对网络开销大的问题，HTTP 1.1 在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。 错误通知的管理。在HTTP1.1中新增了24个错误状态响应码。4 . 长链接。HTTP/1.1 加入 Connection：keep-alive 可以复用一部分连接，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟。 随着网络的发展，HTTP 1.1 还是暴露出一些局限性。 虽然加入 keep-alive 可以复用一部分连接，但域名分片等情况下仍然需要建立多个 connection，耗费资源，给服务器带来性能压力。 pipeling 只部分解决了 HOLB。HTTP 1.1 尝试使用 pipeling 来解决队头阻塞问题，即浏览器可以一次性发出多个请求（同个域名、同一条 TCP 链接）。但 pipeling 要求返回是按序的，那么前一个请求如果很耗时（比如处理大图片），那么后面的请求即使服务器已经处理完，仍会等待前面的请求处理完才开始按序返回。 协议开销大。HTTP/1 在使用时，header 里携带的内容过大，在一定程度上增加了传输的成本，并且每次请求 header 基本不怎么变化，尤其在移动端增加用户流量。 HTTP 2.0互联网的发展还是受到网络速度的限制。有个调侃的话，永远不要忽略一辆载满磁带的在高速公路上飞驰的卡车的带宽。大概意思说数据量大到一定程度时，物理运输无论是速度、安全性、便捷性都比网络传输好。 谷歌是业务首先提出云计算的概念；加上谷歌的公司文化特点是自己内部信息公开、透明，每个人都能了解到其他任何人当前的工作计划、代码等。谷歌为了解决内部系统传输数据慢的问题，自行研发的 SPDY 协议，目的是以最小化网络延迟，提升网络速度，解决 HTTP/1.1 效率不高的问题。 SPDY 协议是在 TCP 协议之上。相比 HTTP/1 的文本格式，HTTP/2 采用二进制格式传输数据，解析起来更高效。同时，还支持对 Header 压缩，减少头部的包体积大小。 HTTP 2.0 还引入了多路复用技术。多路复用很好地解决了浏览器限制同一个域名下的请求数量的问题，同时也更容易实现全速传输，毕竟新开一个 TCP 连接都需要慢慢提升传输速度。 谷歌于2009 年公开了 SPDY 协议，W3C 组织协议不错。于是乎，W3C 将 SPDY 协议引入到 HTTP 协议中，在 2012 年发布 HTTP 2.0。 HTTP 3.0不得不说谷歌的技术确实牛逼，TCP 协议虽然能保证不丢包，但还是存在一些局限性。谷歌为了提高Web联网的速度决定推倒重来，吸收 TCP 快速打开的技术，缓存当前会话的上下文等优点，基于 UDP 协议研发一种名为QUIC （全称是“快速UDP互联网连接”）的实验性网络协议，并且使用运用在 Chrome 浏览器上。 我们可以在 Chrome 浏览器地址栏上输入 chrome://flags/ 来体验 QUIC。 身兼 IETF 旗下 HTTP 工作组组长和 QUIC 工作组组长的马克•诺丁汉（Mark Nottingham）提议，将 HTTP-over-QUIC 实验性协议将被重命名为 HTTP/3，并有望成为 HTTP 协议的第三个正式版本。","link":"/9106.html"},{"title":"假期的所见所闻","text":"国庆假期是难得的休息时间，也是难得拥有完整的时间块。前几天，我自己好好放松自己，跟朋友跟亲戚去嗨。后面几天则是利用时间去研究下行业数据。本文分享下自己的一些发现，充当一回“事后诸葛”。 在 8 月份的时候，有句话很流行，“70后炒股，80后炒房，90后炒币，00后炒鞋”。我对比下几双鞋子的官方发行价和二级市场的现价，结果吓我一跳。这差价是好几倍啊！真的是太疯狂。 我从 StockX 上搞到 2018 年全年各大品牌上市鞋款款式、销量、销售额等在二级市场的数据。 从销量上看，Jordan、Nike、adidas 分别占据前三甲，这三者的销量总和已经占据整个二级市场的 94% 份额。 那为什么鞋子价格能飞天？我说下自己的一些看法。 1.存量少 商家推出一些限量款、纪念款等鞋子，外加采用“饥饿销售”策略，线上摇号中签方式购买，而且中签率相当低。 2.市场庄家炒作 普通消费者在官方的售卖渠道无法买到，只能通过第三方平台渠道。同时。消费者担心买到假鞋所以选择比较知名的交易平台，一旦有了买卖交易，会衍生出“庄家”和“散户”。炒鞋这分为两类：一类是通过官方渠道抢鞋，并在市场售卖赚利差的散户；另一类是通过大量渠道抢购新鞋，提前大量囤货的左右市场价格的庄家。 我自己也很好奇，为什么只有二级市场的品牌主要是耐克、阿迪达斯？其他品牌就在走下坡路？后来我也研究下其他品牌的销售数据，比如国内的安踏、李宁，欧洲的 PUMA（彪马），美国的锐步等。这些品牌在市场上还是很火的，特别是 PUMA（彪马）。 近期，PUMA 的市场热度不断攀升，存在感也越来越强，在各大商场、电视都能看到其身影。我也索性去看下 PUMA 的财报，可以用“惊艳”来形容。 Puma 2019 年第一季度销售额为 13.1 亿欧元，同比增长 15.3%，净利润增长 40.1% 至 9440 万欧元。从地区分布来看，所有地区都实现了持续增长。亚太地区为本季度的总销售额贡献了 30.3% 的营收，增速高达 32.5%。财报中提到，中国和美国成为业绩增长最快的市场，增幅均达到了双位数。 Puma 2019 年第二季度销售额同比增长 16.9% 至 12.27 亿欧元，营业利润同比增长 39% 至8030万欧元，销售额与营业利润均高于分析师预期。其中美洲市场第二季度销售额同比增长22.7%至4.63亿欧元；亚太市场第二季度销售额同比增长23.0%至3.15亿欧元。 第二季度在耐克、阿迪达斯这种销售放慢的情况下，PUMA 依旧一 16.9% 逆势增长。这真值得研究，我分享自己研究的一些结果。 1.找对大众市场口味，利用明星效应。 虽然 PUMA 是做足球鞋出身，但足球在美国、中国的大众市场都不流行。美国和中国最近流行风格是时尚潮流、篮球风。在美国市场，签约了 Rihanna，让其担任创意总监兼全球形象大使；另外还签下 instagram 上拥有1.4亿 粉丝的歌手 Selena Gomez。篮球市场方面，将传奇嘻哈歌手 Jay-Z 纳入麾下， 任其为 Puma 篮球部门的创意顾问。针对中国市场，时尚圈签下“大表姐”刘雯、“国民男神”李现；篮球方面签下了赵继伟。 2.跟风口，重设计 现在年轻人的倾向于时尚复古鞋，可以理解为各鞋款的经典鞋款、日常通勤鞋，还有老爹鞋。选购鞋子，在满足基本需求前期下，优先考虑鞋子是否能体现出个性。不管哪种鞋，PUMA 都优先考虑潮流感、设计感。这些款式的鞋子跑鞋不像跑鞋、篮球鞋不像篮球鞋，就是给人一种很酷的感觉。","link":"/10107.html"},{"title":"如何将 Pycharm 打造得更称手","text":"截止至 2019 年 9 月，在 PYPL 编程语言榜单上，Python 因近几年受欢迎程不断提高而继续霸榜。微软开源的 VS Code 编辑器对 Python 支持力度越来越好，近期也推出了 Python 插件，支持了 Jupyter Notebook。 俗话说“萝卜青菜，各有所爱”，各大编辑器都是挺不错的，因每个人的使用习惯不同，也会选择各自的喜欢的编辑器。 在没有 Pycharm 的年代，我看中 Sublime Text2 编辑器的炫酷的个性，将其作为主力生成工具。后来 JetBrains 推出的 Pycharm IDE 工具，自己也逐渐习惯使用 Pycharm 工具。我分享下自己 Pycharm 使用小技巧。 版本选择？Pycharm 分为 Community (社区版) 和 Professional(专业版)，两个版本对比如下： 社区版虽然能免费使用，但是功能比专业版少了一些。主要少了一些 Web 开发相关组件、数据库支持等。另外，其中的 Scientific tools 功能，其实是 Pycharm 里面预装了Matplotlib 和 NumPy 这两个库，以及内置支持 Jupiter Notebook。 作为 Python 开发者，我更推荐使用专业版。 界面美观我们在 Pycharm 官网看到的编辑界面，是那么炫酷，是那么有极客范。 然而，我们在安装 Pycharm 之后，会发现事情没有那么简单，有种被欺骗的感觉。原因是 Pycharm 默认的主题(Theme)，其他自带的主题效果也一般。这需要可以根据个人喜好风格来配置颜色，也可以通过安装一些主题。 这里推荐个人常用的 UI 主题， Material Theme UI。可以通过 Pycharm 工具 -&gt; Plugins 中，选择 “Browse Repositories” 方式安装。 效果如下： 使用 Material Theme UI 主题之后，看到这效果是不是有种写代码的冲动？ 如果你想要其他的一个风格，可以到 color-themes 网站下载，里面有很多模仿其他编辑器的风格，例如 Sublime Text2、Monokai 等。 高效地插件我是乐于折腾，以“不折腾，死星人”折腾很多 Pycharm 插件，这些插件能给我们带来更好的编程体验。 CodeGlance当文件代码行数过多时，使用鼠标滑鼠来预览代码太费劲了，定位代码段也是不太方便。有了这个代码预览插件，你会有一种“一览众山小”的感觉，代码尽收眼底。 Markdown Navigator代码注释很有必要，方便后续理解和阅读。对于一些流程操作、使用说明，可能需要用 README.md 文件来说明。所以需要一款能即时预览 Markdown 语言效果的插件，Markdown Navigator 是一款不错的预览插件，是 JetBrains 官方出品的。","link":"/10108.html"},{"title":"Python 创建二维数组的正确姿势","text":"List （列表）是 Python 中最基本的数据结构。在用法上，它有点类似数组，因为每个列表都有一个下标，下标从 0 开始。因此，我们可以使用 list[1] 来获取下标对应的值。如果我们深入下列表的底层原理，会发现列表是基于 PyListObject 实现的。PyListObject 是一个变长对象，所以列表的长度是随着元素多少动态改变的。同时它还支持插入和删除等操作，所以它还是一个可变对象。 可以简单理解为，Python 的列表是长度可变的数组。一般而已，我们用于列表创建都是一维数组。那么问题来，我们如果创建多维数组呢？ 列表能创建多维数组？列表是支持操作符，如果一个列表与 ‘ * ’ 号结合使用，能达到重复列表的效果。比如 123456789list_one = [0]list_two = [0] * 3print(list_one)print(list_two)&gt;&gt;&gt; 运行结果：[0][0, 0, 0] 那么利用这个重复特性，我们是否可以来创建一个二维数组呢？于是乎，我进行一顿猛操作，结果就被我折腾出来了。 12345678list_one = [0]list_two = [[0] * 3] * 3print(list_one)print(list_two)&gt;&gt;&gt; 运行结果：[[0, 0, 0], [0, 0, 0], [0, 0, 0]] 看起来很完美的操作，但是如果进行一些列表更新操作，问题就显露出来了。比如我对 list_two 的更换中间位置的值，即对 list_two[1][1] 进行更换值。 12345list_two = [[0] * 3] * 3print(list_two)list_two[1][1] = 2print(list_two) 不难发现，运行结果有点不对劲，列表中有三个位置的值也改变了。 12[[0, 0, 0], [0, 0, 0], [0, 0, 0]][[0, 2, 0], [0, 2, 0], [0, 2, 0]] 为什么会出现在这种情况呢？原因是浅拷贝，我们以这种方式创建的列表，list_two 里面的三个列表的内存是指向同一块，不管我们修改哪个列表，其他两个列表也会跟着改变。 如果要使用列表创建一个二维数组，可以使用生成器来辅助实现。 1234list_three = [[0 for i in range(3)] for j in range(3)]print(list_three)list_three[1][1] = 3print(list_three) 我们对 list_three 进行更新操作，这次就能正常更新了。 12[[0, 0, 0], [0, 0, 0], [0, 0, 0]][[0, 0, 0], [0, 3, 0], [0, 0, 0]] 除了以上的方式，还有一种更加简洁方便的方式，就是使用 NumPy 模块。 相比 List，NumPy 数组的优势NumPy 全称为 Numerical Python，是 Python 的一个以矩阵为主的用于科学计算的基础软件包。NumPy 和 Pandas、Matpotlib 经常结合一起使用，所以被人们合称为数据分析三剑客。Numpy 中有功能强大的 ndarray 对象，能创建 N 维的数组，另外还提供很多通用函数，支持对数组的元素进行操作、支持对数组进行算法运算以及提供常用的统计函数。 相比 List 对象，NumPy 数组有以下优势：1.这是因为列表 list 的元素在系统内存中是分散存储的，而 NumPy 数组存储在一个均匀连续的内存块中。这样数组计算遍历所有元素，不像列表 list 还需要对内存地址进行查找，从而节省了计算资源。2.Numpy数组能够运用向量化运算来处理整个数组，速度较快；而 Python 的列表则通常需要借助循环语句遍历列表，运行效率相对来说要差。3.NumPy 中的矩阵计算可以采用多线程的方式，充分利用多核 CPU 计算资源，大大提升了计算效率。4.Numpy 使用了优化过的 C API，运算速度较快。 创建数组前面说到 NumPy 的主要对面是 ndarray 对象，它其实是一系列同类型数据的集合。因为 ndarray 支持创建多维数组，所以就有两个行和列的概念。 创建 ndarray 的第一种方式是利用 array 方式。 123456789101112131415161718import numpy as np# 创建一维数组nd_one = np.array([1, 2, 3])# 创建二维数组nd_two = np.array([[1, 2, 3], [4, 5, 6]])print(nd_one)print(nd_two)print('nd_two.shape =', nd_one.shape)print('nd_two.shape =', nd_two.shape)&gt;&gt;&gt; 运行结果：[1 2 3][[1 2 3] [4 5 6]]nd_two.shape = (3,)nd_two.shape = (2, 3) 其中 shape 是数组的一个属性，表示获取数组大小(有多少行，有多少列)，如果是一维数组，则只显示（行，）。代码中打印出 nd_two 的形状，输出为（2，3），表示数组中有 2 行 3 列。 第二种办法则使用 Numpy 的内置函数 使用arange 或 linspace 创建连续数组。1234567891011121314151617181920212223242526import numpy as np# arange() 类似Python内置函数的 range()# arange(初始值, 终值, 步长) 不包含终值x0 = np.arange(1, 11, 2)print(x0)# 创建一个 5x3 的数组x1 = np.arange(15).reshape((5, 3))print(x1)# linspace()线性等分向量# linspace(初始值, 终值, 元素个数) 包含终值x2 = np.linspace(1, 11, 6)print(x2)&gt;&gt;&gt; 运行结果：[1 3 5 7 9][[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11] [12 13 14]][ 1. 3. 5. 7. 9. 11.] 虽然 np.arange 和 np.linspace 起到的作用是一样的，都是创建等差数组，但是创建的方式是不同的。 使用 zeros()，ones()，full() 创建数组123456789101112131415161718192021222324import numpy as np# 创建一个 3x4 的数组且所有值全为 0x3 = np.zeros((3, 4), dtype=int)print(x3)# 创建一个 3x4 的数组且所有元素值全为 1x4 = np.ones((3, 4), dtype=int)print(x4)# 创建一个 3x4 的数组，然后将所有元素的值填充为 2x5 = np.full((3, 4), 2, dtype=int)print(x5)&gt;&gt;&gt; 运行结果：[[0 0 0 0] [0 0 0 0] [0 0 0 0]][[1 1 1 1] [1 1 1 1] [1 1 1 1]][[2 2 2 2] [2 2 2 2] [2 2 2 2]] 使用 eye() 创建单位矩阵eye() 创建的数组特点是行数和列数都是一样。因为它创建出来的是单位矩阵，单位矩阵是正形矩阵，对角线的值均为 1，其他位置的值为 0。 12345678910import numpy as np# 创建 3x3 的单位矩阵x6 = np.eye(3, dtype=int)print(x6)&gt;&gt;&gt; 运行结果：[[1 0 0] [0 1 0] [0 0 1]] 使用 diag() 创建对角矩阵diag() 是创建一个 NxN 的对角矩阵，对角矩阵是对角线上的主对角线之外的元素皆为 0 的矩阵。 123456789import numpy as npx7 = np.diag([1, 2, 3])print(x7)&gt;&gt;&gt; 运行结果：[[1 0 0] [0 2 0] [0 0 3]] 使用 random 创建随机数组numpy 中的 random 中有很多内置函数，能简单介绍其中的几种。 1234567891011121314151617import numpy as np# 创建 2x2 数组且所有值是随机填充x9 = np.random.random((2, 2))print(x9)# 创建一个值在 [0, 10) 区间的 3x3 的随机整数x10 = np.random.randint(0, 10, (3, 3))print(x10)&gt;&gt;&gt; 运行结果：[[ 0.77233522 0.41516417] [ 0.22350126 0.31611254]][[0 6 5] [7 6 4] [5 5 9]]","link":"/10109.html"},{"title":"个人博客如何选型？","text":"当看到别人比自己优秀的时候，有时候会自嘲一句：“同九年，汝何秀”。别人之所以优秀，自然是有过人的能力。身边有优秀的人，是一件很幸福的事，我们能偷而学之。 但学习也要注重一些学习方法，让学习效率最大化，满脑子死啃有时候会走很多弯路。我推荐一种非常有用的学习方式，同样我自己也一直在使用，那就是分享。 教别人是最好的学。如果我们无法用自己的语言表述学到的东西，说明我们还没有掌握该知识。另外，我们教别人的时候，不但能加深印象，还能发现自己的不足之处。 分享的形式有很多，有以文字的形式，比如写作；有以视频的形式，比如短视频，演讲等。相比视频的形式，我更推荐写作的方式。这个也是最适合大多数人的，不需要克服面对镜头的紧张。 写作有什么用处？写作能锻炼自己的表述能力。 写作的过程就是表达自己的想法的过程。在这过程中，我们不仅让别人理解自己的想法，也让自己理解自己的想法。 况且，文章写出来和别人能不能看懂是两回事。就好比我们看网络小说，能一下子看很多章；而看一些经典名著，我们会发现很生涩，有点读不动的感觉。在要排除书籍的深度、个人阅读能力等因素之外，一般人能更能接受通俗易懂的白话文文章。因此，能将复杂的东西通俗化是一门艺术，是个人表述能力的体现。 写作能锻炼自己的思考能力。 写作是思维的进阶之路。这种改变是潜移默化的。跟别人交流时，自己表述思路非常清晰，别人听着会非常舒服。我自己写文章不知不觉也有两年多了，这中间我深有体会。 去哪里写作呢？现在很多网络社区众多，如知乎，简书以及微信公众号等。搭建个人博客网站也是一个平台。其实平台无所谓，在于自己有没有这份热情和冲劲。 如何选型？前面聊了这么多，主要是自己一些心得体会吧。有些人喜欢或者想要搭建个人博客网站，我把博客网站方案分类两大类，大家根据自己喜好去选择。 1、动态网站 动态网站指文章内容等数据存储到数据库中。这种可玩性比较高，适合喜欢折腾，喜欢 DIY 的小伙伴。WordPress 算是搭建动态网站比较成熟的方案，它是一个经典的博客平台。而且生态圈比较完善，有大量的插件以及博客主题给我们选择，自由度很高。 说到数据库，这里又分为两种解决方案。一种是自己购买云主机，安装各种运行环境，比如 PHP、MySQL 等，然后再部署我们的网站。另一种相对比较简化，购买一些云厂商提供的虚拟空间（已经帮我们安装好的运行环境的主机），我们直接上传网站文件到空间，再配置数据库就完事。 2、静态网站 这种方案适用于专注内容写作的小伙伴。这里推荐使用 Github Pages 方案。GitHub Pages 是 GitHub提供的一个网页寄存服务，可以用于存放静态网页，包括博客、项目文档甚至整本书。我们使用这种方案，只需一个小时就能搞定一个博客网站。 Github Pages 支持一些模板系统，方便我们进行一些定制化，比如更换主题等。方案有以下几种。 Ruby 编写的 Jekyll Node.js 编写的 Hexo Go 编写的 Hugo Python 编写的 Pelican 以及更人性化的 Gridea Hexo 和 Jekyll 方案是使用人数比较多，后面会出一些教程详细讲解如何使用这两种方案进行搭建。","link":"/10110.html"},{"title":"利用 Github+Hexo 搭建个人博客网站","text":"我之前写过一篇文章《利用Github+Jeklly搭建个人博客网站》，利用 Github 仓库提供的 github pages 来搭建个人博客。Jekyll 和 Hexo 可以说是最受欢迎、用户都非常多的两个静态博客生成系统。本文给大家介绍利用 Hexo 结合 Github pages 来搭建个人网站。 什么是 Hexo？Hexo 是一个基于 node.js 制作的快速、简洁且高效的博客框架。Hexo 可以将我们撰写的 Markdown 文档解析渲染成静态的 HTML 网页。 Hexo VS Jeklly 本地环境 Jeklly 是由 Ruby 语言编写，需要到官网下载并安装 RubyInstaller。Hexo 则需要安装 Node.js 环境。网上经常看到很多人吐槽安装 Jekyll 经常碰到各种问题。我分别在 Windows 下安装过这两个环境。整个流程跑下来还是挺顺利的。 速度 说是比较 Hexo 和 Jeklly 这两个框架，其实要比较 Ruby 和 Node.js 的运行速度。Node.js 是一个 Javascrip t运行环境(Runtime)。实际上它是对 Google V8 引擎进行了封装。众所周知，Google JS Runtime 速度非常快，性能非常好。在本地预览上，Jekyll 是生成了页面然后进行预览，而 Hexo 是没有在根目录生成文件的，速度也快不少。因此，Hexo 在性能和速度上面更胜一筹。 部署 Jeklly 是将整个工程源码上传到 Github 仓库，然后 Github 会自动生成静态文件。而 Hexo 需要事先在本地生成整个站点页面，再将 Html 文件、资源文件等上传到 Github 上。 主题 Jekyll 使用 Liquid；它是有 Ruby 语言编写的开源模板语言。Hexo 使用的是 EJS；EJS 是 JavaScript 模板库，用来从 JSON 数据中生成 HTML 字符串。EJS 相对比较复杂，所以可实现的功能更加的多。从开发一个主题难度上看，Hexo 实现起来更方便、更简单些。 生态 一个产品能不能快速上手，要看其生态圈是否完善。在文档说明配套方面，Hexo 更加完善。我们能在 Hexo 官网上找到中文的文档说明。Jeklly 的中文文档还是广大网友自发组织翻译的。从主体方面上看，Hexo 官网就有主题跳转链接，目前一共有 280 个主题；这 Jeklly 是无法比拟的。再者，相比 Ruby，Node.js 使用的人群更广, 火爆程度更好。因此，我们通过搜索引擎搜索问题时，搜到 Hexo 的资料量会大于 Jeklly，这也是造成网友吐槽 Jeklly 的原因。 创建 Github 项目我们是将网页托管到 Github Pages 上，这部分就不再花篇幅去详细介绍，具体可以参考《利用Github+Jeklly搭建个人博客网站》的内容。简单来说，就是创建一个 Github 项目，项目名称命名格式是 username.GitHub.io。 搭建本地 Hexo 环境首先我们需要到 Node.js 官网，根据自己系统平台以及系统版本下载 Node.js 安装包，安装的时候记得将 node.js 添加到系统变量中。 安装后，分别以下命令来检测 Node.js 和 npm 是否安装成功 能看到版本号，说明 Node.js 环境安装成功。然后我们使用 npm 来安装 Hexo。 1$ npm install -g hexo-cli 构建本地项目在本地磁盘中，创建一个名为 Github_blog 的文件夹，用来保存网站项目的文件。接着，通过终端执行命令来初始化 Hexo 工程。 123$ hexo init monkey-soft.github.io # 创建工程名，随意命名$ cd monkey-soft.github.io$ npm install # 安装 node 所需模块 执行完命令之后，我们能看到 Github_blog 多出一个 monkey-soft.github.io 的子文件夹，里面还有 Hexo 默认生成的文件。 deploy_gitHexo 借助 Git 推送网站信息到 Github 仓库所生成的文体。 public执行生成站点文件的命令，即hexo g，Hexo 将自动生成静态网页和资源文件都保存到这里 scaffolds模版 文件夹。当我们新建文章时，Hexo 会根据 scaffold 来建立 Markdown 文件。 source里面有个 _posts 文件夹，存放着我们以 Markdown 语法创造内容的文件。 themesHexo 主题存放的文件夹 _config.ymlHexo 项目的配置文件，配置网站的基本信息、网页 URL 路径、时间格式、文章分页、扩展插件等信息。这里的 Site 配置建议都修改下。 主题选择当我们创建 Hexo 项目之后，项目中默认会有一个名为 landscape 的主题。如果你觉得默认主题不够美观，可以自行到 Hexo 官网选择适合自己的主题。为了方便大家选择，猴哥推荐 10 大明星主题。排名根据目前 Github 上 Star 数依次降序排列。 【iissnan / hexo-theme-next】 Stars：14.9K【litten / hexo-theme-yilia】 Stars：7.1K【theme-next / hexo-theme-next】 Stars：4.6k【viosey / hexo-theme-material】 Stars：3.8K【tufu9441 / maupassant-hexo】 Stars：2.1K【LouisBarranqueiro / hexo-theme-tranquilpeak】 Stars：1.6K【blinkfox / hexo-theme-matery】 Stars：1.4K【ahonn / hexo-theme-even】 Stars：1.1K【forsigner / fexo】 Stars：1K【fi3ework / hexo-theme-archer】 Stars：942 萝卜青菜各有所爱，猴哥选择一款名为 icarus 的小众主题。如果你也想使用同款主题，在公众号回复关键字【hexo】即可获取。 将从 Github 上下载的主题，复制到 theme 文件夹中。 最后需要在 _config.yml 指定 Heox 主题。 开始写作在source\\_posts目录下，新建一个 .md 文件。默认的命名方式是 year-:month-:day-:title.md。如果你想修改这个命名规则，可以在 Hexo 目录下的 _config.yml 文件中，找到 # Writing 字样，然后进行修改。 不过我不建议修改这命名规则，这格式能一目了然。知道我们创作文章是什么时候？文章的标题是什么？ 创建 md 文件之后，需要在文件头部指定一些信息。 1234567---layout: posttitle: &quot;文章标题&quot;date: 2019-12-16 22:35:13categories: [文章分类]tags: [文章标签1, 文章标签2]--- SEO 优化关于固定链接，还是“三不二建”的套路。1.日期不要出现在固定链接中。会让搜索引擎爬虫认为是过期内容，导致不再爬取2.链接的层次不要太深。默认的固定链接是 /年/月/日/文章名。这种层次过深，不方便搜索引擎爬虫的抓取，对搜索引擎的收录不太友好。3.链接中不要出现中文。4.建议使用 post_id 形式来设置固定链接。5.建议使用英文缩写或英文来设置固定链接。 不得不说， Hexo 很人性化，有考虑到网站开发者会做 SEO 优化工作，在模板语言的可选变量中就有 id 选项。因此，设置 post_id 的链接方法有两种。 这里直接利用 :id（文章ID） 变量能实现效果。针对以上两种建议优化，我给出一个兼容两者的方案。Hexo 目录下的 _config.yml 文件中，修改永久链接的显示方式。 这里 :urlname 变量可以显示为数字或者英文内容。我为了实现 post_id 形式，同时保证 post_id 有一定的跳跃性，在 id 后面拼接个月号数。 接着，在 md 文件的头部信息中，需要增加一个 urlname 字段， 设置文章的 id 号。 1urlname: 1 如果文章的名字是 2019-12-16-titie.md，那么显示的永久链接是 121.html 网站有说法是百度搜索对 Hexo 站点搜索不友好，原因是百度搜索屏蔽了 Github pages。但其实也是有方法来规避这种规则。 用户一般是通过关键字从搜索引擎搜索我们网页内容，所以我们需要给页面设置关键字和页面描述，能命中用户搜索的关键字。当然前提是要被搜索引擎收录。 首先，在 md 文件中，增加两个字段。 12keywords: [关键字1, 关键字2, 关键字3]description: 页面描述(一句话总结文章在讲什么？) 然后分别到百度搜索引擎提交入口和谷歌搜索引起提交入口，提交自己的域名地址。在这里之前，建议先购买一个域名，然后在 Github 设置域名重定向。 百度域名提交地址：传送门 谷歌域名提交地址：传送门 我们这么做是化被动为主动。自己网站会被搜索引擎爬虫收录，但不知道等到猴年马月。不如我们主动出击，自己主动我们网站地址，让搜索引擎知道我们网站，会加速收录速度。 个性化优化Hexo 很多主题可玩性很高， 如果你感兴趣可以感觉个人喜好对主题内容进行一些定制化修改。这里涉及主题比较多，就是一一讲解。大家可以通过 Github 主题说明文档以及搜索引擎来了解相关信息。","link":"/11112.html"},{"title":"利用 Github+Jeklly 搭建个人博客网站","text":"在上篇文章《个人博客如何选型？》中讲到，可以利用 Github Pages 来搭建个人博客网站，本文主要讲解其中的各种细节。 评估 Github Pages 方案前面说到几种博客方案，我都玩过，可以说是各有各的特点与优势。在你考虑选择是否 Github Pages 方案之前，可以先了解其优缺点，然后根据自己情况加一判定。 优点： 完全免费。因为它本质上是一个 Github 仓库，只不过 Github 官方提供网页寄存服务。 简单省心。无须自己购买云服务，也不需要关系环境搭建、系统维护等，我们只需专注写作。 带宽够用。在使用的时候项目和网站的大小不要超过 1GB，也不要过于频繁的更新网站的内容（每小时不超过 10 个版本），每个月的也要注意带宽使用上限为 100GB。这些对于个人网站其实是够用。 缺点： 它是静态网站，无法支持数据库，涉及一些数据存储的操作需要自己找其他解决方案。例如页面浏览的阅读人数统计、点赞数等。 访问速度比较慢。我们的网站是寄托在 Github 网站上，Github 服务器节点是在美国，所以访问速度没有国内主机快。 以上是我对于 Github Pages 方案的理解。如果你已经确认选择 Github Pages 方案，继续跟着猴哥的思路，一步步搭建属于自己的博客。 创建简单页面首先需要注册一个 GitHub 账号，然后到个人主界面里面，创建一个新的 Repository。 进入创建新仓库页面后，在 Repository name 的位置填写域名，格式是 username.GitHub.io。我的 Github 的 id 是 monkey-soft，所以域名如下图所示。 创建成功后，点击 settings 栏目。 接着找到 GitHub Pages，然后随便选择 Github 提供的默认主题。 最后，我们通过浏览器访问仓库的域名（monkey.github.io）就能看到系统创建的默认网页。 Jekyll 主题选择上文讲到选择默认的主题，这里都是 Jekyll 主题。那什么是 Jeklly呢？它是一个简单静态站点生成器，能将纯文本的(一般是 Markdown 文件)转化为静态页面。有趣的是，Jeklly 是由 GitHub 的联合创始人 Tom Preston-Werner 基于 Ruby 语言编写的。因此，GitHub 官方默认采用 Jeklly 来生成网页内容。 原生的 Jeklly 主题过于简单，如果自己想进行改造，工作量还蛮大的。通常比较方便的做法是利用其他作者制作的主题。猴哥找到三个 Jeklly 主题网站，分别是： http://jekyllthemes.org https://jekyllthemes.io http://themes.jekyllrc.org 你可以根据个人喜好选择适合的主题。我找到一款名为 MatJek 的主题，将主题压缩包下载到本地。 主题下载之后，我们需要把主题文件同步到我们的网站，也就是远程的 Github 仓库。如果你熟悉 Git 用法，可以使用 clone 命令将 Github 仓库下载到本地。 如果你没有任何 Git 的基础，也不想进行一些繁琐的配置，那么推荐使用桌面客户端的形式进行管理。我们需要下载 GitHub 桌面客户端，然后登录自己 Github 账号，接着将仓库拉取到本地。注意的是，存储路径不能有中文。 找到仓库存放的文件夹，将之前所有文件全部删除，把刚才下载的主题文件复制到当前文件夹中。 我们还需要修改配置文件。_config.yml 是 Jekyll 的全局配置文件。里面记录着网站的名字，网站的域名，网站的链接格式等等。 我对于原来的 MatJek 主题进行一些定制化需求，整合起来更像是一个博客网站。这里我以自己修改主题的配置文件为例。如果你使用其他主题，根据作者的要求，修改 _config.yml 的内容即可。 最后一步，我们将刚才修改的内容同步到 Github 远程仓库。 完成以上操作，我们可以打开浏览器，输入我们仓库地址访问我们的网站。 该主题是猴哥基于 MatJek 主题进行修改，同时也修复几个缺陷。如果你想获取猴哥同款主题，在公众号后台回复『主题』即可获取。 Jekyll 本地环境搭建如果我们想对主题界面进行修改，每次预览界面需要将文件同步到 Github 仓库，这样操作不太方便。因此，我们可以本地搭建个 Jeklly 环境，方便我们调试。 因为 Jeklly 是基于 Ruby 语言编写的，所以我们需要安装 Ruby 环境。我以 Windows 环境下安装为例，Mac 环境比较简单，可以执行搜索搞定。 首先，我们到 Ruby 下载安装包，最好下载带有包管理工具 devkit，方便后续安装各种组件。 安装 Ruby 之后，会弹出提示安装 MSYS。MSYS 是 Windows 上模拟 GUN 环境的组件。这里选择*选项 3 *进行安装。 进入到本地项目文件中，启动终端，依次执行以下命令。 123~ $ gem install jekyll bundler~ $ bundle install~ $ bundle exec jekyll serve # 启动本地服务器 然后打开浏览器，访问 http://localhost:4000 就能预览本地网站的界面效果。 SEO 优化该主题已经集成用于 SEO 优化的 jekyll-seo-tag 插件，我们简单修改 _config.yml 里面的这几个字段。 title（博客主题） subtitle（博客副标题） description（网站描述，尽量增加跟网站内容的关键字） 接着是优化网站文章的 URL 链接。默认 Jeklly 的 URL 的 Path 路径是 /:year/:month/:day/:title，显示效果如/2019/11/12/我的第一篇文章.html。这种固定链接不太理想。 猴哥总结 URL 地址的 SEO 优化三个原则： 1.日期需要出现在固定链接中。这基于两个方面的考虑。一是如果数字出现在固定链接里面，等于提醒搜索引擎，这是很旧的内容了，没必要再爬一遍了。另外一个原因是，假如你要修改文章的日期重新发布的话，链接地址就变了，也就是意味着你的反向链接，PR 等等都没有了。 2.链接的层次不要太深默认的固定链接是 /年/月/日/文章名。这种层次过深，不方便搜索引擎爬虫的抓取，对搜索引擎的收录不太友好。 3.链接中不要出现中文虽然现在的搜索引擎已经能识别URL地址里面的中文字符，但无论是从美观上，以及中文字符会被转义的角度上看，都是非常差的。 猴哥推荐两种固定链接方案。一种是 /postname/，基于文章的英文单词翻译；另一种是 /post_id/，基于文章发布的 ID 号。 因为是静态网站，所以必须按照 Jeklly 的规范来设置 URL，第一种方式也就被排除，只能选择第二种方案。Jeklly 提供的路径变量只有几个，不够灵活。最后我花费九牛二虎之力，翻阅 Jeklly 中文网站，找到一个伪 postid 的方案。Path 路径设置为 /:short_year:i_month:i_day.html, 即利用日期的部分字段来拼凑。 这里我进行讲解下。比如一篇文章于 2017-06-25 发布的，Jeklly 会将年前面两位去掉；月份如果是10月之前去掉数字 0，没有则直接保留月份；日跟月份一样会去掉数字 0；因此，最后的 Path 路径是 17625.html。 发布文章网站主题搭建工作完成之后，我们就能往博客上填充内容。文章一般是用 Markdown 语法编写的，存放在 _posts 文件中。 文件的命名规则是：年-月-日-文章标题.md。 在 md 文件中，必须带上头部信息才能被识别出来，其中信息有文章标题、编写时间、分类、标签等。 我的设想是在首先展示文章时会显示封面图片，我在文件中创建一个名为 img 文件夹来存放封面图片，图片命名须方式是以日期的形式。 当一切工作完成就绪，我们就可以使用 Github 客户端将内容推送到远程仓库。 剧透一下，下篇分享 Github Pages 结合 Hexo 搭建博客网站方案。","link":"/11111.html"},{"title":"爱自己多一点，对自己健康负责","text":"近期关于身体健康的热点新闻颇多的，前有“网易裁员，让保安把身患绝症的我感触公司。我在网易亲身尽力的噩梦！”，后有“35岁高以翔意外猝死”。每次看到这样的新闻，都是惊讶和不安，怎么好端端的人突然就没有了。 有个段子说得挺好的，反馈出当前社会年轻人的现状。前程四紧：眉头紧、手头紧、衣服紧、时间紧。每个人都有自己的理想和人生追求目标，想为自己奋斗一把。虽说可以热爱工作，可以趁年轻的时候多努力，但也别拿宝贵的生命来交换。自己所追求的东西，只有健康的身体才能够真正看到。 其实这些道理自己都懂，但我就是做不到！其实我自己也是这样，我这两年也是晚上经常熬夜，但有次体验让我惊出一身冷汗。检查报告显示身体一些指标异常，然后自己各种去医院检查。最后医生一开口就说：小伙子，是不是天天熬夜？熬夜对身体伤害很大，你现在还年轻没什么，等过几年你就知道后果。不要再熬夜了，多运动吧。 这件事确实给我敲响警钟，我告诉自己：人生漫漫，爱自己多一点，自己自己健康负责。现在自己也调整自己的生活规律，我也希望大家也能像对待自己的朋友一样关心爱护身体。以下是我自己的一些建议：1.每周进行至少三次合理运动。目的是让自己出汗，有助于身体新陈代谢。但也不要一上来就进行高强度运动，身体需要有个适应的过程。2.注意饮食规律。早餐吃好、午餐吃饱，晚餐吃少。3.少熬夜，少熬夜，少熬夜。重要的话说三遍。4.给自己买份保险。给自己一个心理安慰，也给自己一道最后的健康保障。","link":"/11113.html"},{"title":"国外大神制作的超棒 Pandas 可视化教程","text":"如果读者们计划学习数据分析、机器学习、或者用 Python 做数据科学的研究，你会经常接触到 Pandas 库。Pandas 是一个开源、能用于数据操作和分析的 Python 库。 加载数据加载数据最方便、最简单的办法是我们能一次性把表格(CSV 文件或者 EXCEL 文件)导入。然后我们能用多种方式对它们进行切片和裁剪。 Pandas 可以说是我们加载数据的完美选择。Pandas 不仅允许我们加载电子表格，而且支持对加载内容进行预处理。 Pandas 有个核心类型叫 DataFrame。DataFrame 是表格型的数据结构。因此，我们可以将其当做表格。DataFrame 是以表格类似展示，而且还包含行标签、列标签。另外，每列可以是不同的值类型(数值、字符串、布尔型等)。 我们可以使用 read_csv() 来加载 CSV 文件。 12# 加载音乐流媒体服务的 CSV 文件df = pandas.read_csv('music.csv') 其中变量 DF 是 Pandas 的 DataFrame 类型。 Pandas 同样支持操作 Excel 文件，使用 pd.read_excel() 接口能从 EXCEL 文件中读取数据。 1df = pandas.read_csv('music.xls') 选择数据我们能使用列标签来选择列数据。比如，我们想获取 Artist 所在的整列数据, 可以将 artists 当做下标来获取。 同样，我们可以使用行标签来获取一列或者多列数据。表格中的下标是数字，比如我们想获取第 1、2 行数据，可以使用 df[1:3] 来拿到数据。 Pandas 的利器之一是索引和数据选择器。我们可以随意搭配列标签和行标签来进行切片，从而得到我们所需要的数据。比如，我们想得到第 1, 2, 3 行的 Artist 列数据。 1234import pandas as pddf.loc[1:3, ['Artist']]# loc(这里会包含两个边界的行号所在的值) 过滤数据过滤数据是最有趣的操作。我们可以通过使用特定行的值轻松筛选出行。比如我们想获取音乐类型(Genre)为值为 Jazz 行。 再比如获取超过 180万听众的 艺术家。 处理空值数据集来源渠道不同，可能会出现空值的情况。我们需要数据集进行预处理时。 如果想看下数据集有哪些值是空值，可以使用 isnull() 函数来判断 1234import pandas as pddf = pd.read_csv('music.csv')print(df.isnull()) 假设我们之前的音乐数据集中 有空值(NaN)的行。 我们对之前的音乐.csv 文件进行判断，得到结果如下: 如果我想知道哪列存在空值，可以使用 df.isnull().any() 1234import pandas as pddf = pd.read_csv('music.csv')print(df.isnull().any()) 结果如下： 处理空值，Pandas 库提供很多方式。最简单的办法就是删除空值的行。 除此之外，还可以使用取其他数值的平均值，使用出现频率高的值进行填充缺失值。 1234import pandas as pd# 将值填充为 0pd.fillna(0) 5.分组我们使用特定条件进行分组并聚它们的数据，也是很有意思的操作。比如，我们需要将数据集以音乐类型进行分组，以便我们能更加方便、清晰了解每个音乐类型有多少听众和播放量。 上述代码的的执行过程是：Pandas 会将 Jazz 音乐类型的两行数据聚合一组；我们调用了 sum() 函数，Pandas 还会将这两行数据端的 Listeners(听众)和 Plays (播放量) 相加在一起，然后组合在 Jazz 列中显示总和。 这也是 Pandas 库强大之处，能将多个操作进行组合，然后显示最终结果。 6.从现有列中创建新列通常在数据分析过程中，我们发现自己需要从现有列中创建新列，使用 Pandas 也是能轻而易举搞定。 附：原文地址","link":"/1114.html"},{"title":"GitHub 标星10k+，新型冠状病毒（2019-nCoV）最全的数据集在这里！","text":"2020 年的春节是不平凡的。 新型冠状病毒（2019-nCoV）突然降临武汉，随着春运大潮，逐渐扩散到全国各省份。 这让原本应该是热热闹闹的春节，一下子气氛冷到冰点，感觉空气中都带着恐怖的气息。我们国家在疫情面前表现出强大一面，采取封城措施，举国禁足，共同对抗疫情。经过一个月多的努力，目前疫情在国内算是得到控制。 但海外情况却不容乐观，欧洲发达国家，英国、瑞典宣布放弃抵抗新冠病毒。容许我大声喊一句：中国牛逼！我为自己身为中国人而感到骄傲！ 我自己是学技术出身，想利用优势来获取全球疫情的第一手数据。几经寻觅之后，我发现 GitHub 有个开源项目 COVID-19。这是美国约翰·霍普金斯大学系统科学与工程中心（JHU CSSE）收集各国卫生机构公开 2019 年新型冠状病毒的数据存储项目。项目是处于教育和学术研究目的而开源，还提供了可视仪表板 Web 界面。不得不说是一个很良心的项目。 截止文章发布前半个小时，我们通过仪表盘数据可知，全球有 18 万人确诊，7155 人死亡，79433 人治愈。 意大利也是重灾区，伊朗、西班牙、韩国、德国、法国等也有超过 6 千人确证。 通过全球疫情总览图，我们可以知道新型冠状病毒已经席卷全球。 但可喜的是，国内的疫情已经得到控制。橙色的线条可以看出，2月14号开始确诊人数出现平缓趋势；2月28号以后就显示为平滑的曲线，说明疫情暂时得到控制。 关于这个项目，一共有三个数据集，分别是 archived_data、csse_cvoid_19_data、who_covid_19_situation_reposts。 archived_data 是2020年1月21日至2月14日有关冠状病毒COVID-19（以前称为2019-nCoV）的仪表板案例报告。其中 archived_time_series 目录是经过清洗之后的数据。 csse_cvoid_19_data 是每日病例数据数据集，每隔24小时更新一次。其中** csse_covid_19_daily_reports 目录下有从1月22号至今每天的病例数据，而csse_covid_19_time_series 是经过清洗的数据集。如果我们想利用 Python 对疫情数据做可视化处理，可以直接选用该数据集。** who_covid_19_situation_reports 则是 世卫组织 COVID-19 状况报告。 最后附上数据可视化仪表盘地址：PC 端：PC版可视化仪表板盘 手机端：手机版可视化仪表 ☞☞☞项目仓库地址：COVID-19","link":"/3115.html"},{"title":"说说 Django 如何优雅地对接 Mongodb","text":"大家好，我是猴哥。近来在研究 Django 对接 MongoDB 数据库，遇到一些坑，自己随便做下总结。 前言Django 更新迭代速度真的是快，现在最新版本都 3.0.5 了。如果有留意 Django 在 GitHub 上的仓库，不难发现几乎每天都有人在提交。 不得不能说 Django 官方团队好积极呀。 软件的版本更新快，其实也算是一件好事。如果我们在使用过程中发现问题和缺陷，提交给团队，能很快得到修复。但这不意味着我们要跟着版本更新的节奏走，建议跟进自身情况来定。个人研究和学习，使用最新版本倒不是问题。如果是公司或者团队要用于商业，特别是已经上线的，最好是求稳。选择官方有长期支持版本，或保持跟最新版本有两~三个版本差。 目前，Django 团队对各个版本的支持情况。 不难看出，Django 3.0 版本也算是一个过渡版本。3.0.5 在今年 8 月份就停止主流维护支持，到明年 4 月份就停止维护了。具体是什么意思？说人话。3.0.5 版本从现在到 2020 年 8 月份，再这段时间内版本更新迭代会比较快，既有实现新需求，又要修复遗留的重大缺陷。8 月份一过，就不做新需求了，偶尔修修 bug 而已，版本更新截止放慢了。 因此，个人学习选择长期支持的 2.2 版本或尝鲜的 3.0 版本都行。如果要开发并用于商用，推荐选择长期支持的 2.2 版本。同时，Django 2.2 已经不再支持 Python 2.x 和 3.x 版本，最低要求 Python 版本是 3.5。 选型Django 本身已经有 ORM 框架。ORM 是对象关系映射(Object Relational Mapping)的缩写，由于程序设计者更多采用面向对象的思想，而数据库则以关系作为其基础。ORM 的作用使得我们可以采用面向对象的思路来设计数据库，使数据库设计更加简单。 但是 Django 框架的数据库引擎中没有 MongoDB 引擎，配置文件 setting.py 中的 ENGINE 字段只支持常见几种关系型数据库。 12345django.db.backends.sqlite3django.db.backends.mysqldjango.db.backends.oracledjango.db.backends.postgresqldjango.db.backends.postgresql_psycopg2 如果没有数据引擎支持，我们会多做很多造轮子的活，比如实现数据库连接、封装数据库 DAO 接口等。我在 Django 官网 WIKI 文档中了解到，Django 也是支持非关系型数据库，不过需要使用第三方支持库。官方解释到，如果数据库使用 MongoDB，推荐使用 Djongo这个库来做数据库引擎。我们不用当心 Djongo 不够完善，该库已经有在超过 1 百万人从 pypi 上下载并使用。再者，Djongo 没有大刀阔斧地修改，保留 Django ORM 框架，这也算是比较稳。 优雅地使用安装使用 pip 安装 python 第三方库是最方便的。 1pip install djongo 修改配置在项目的 setting.py 中，修改数据库引擎、数据库名、主机号等信息。Djongo 关于数据库完整的配置： 12345678910111213141516171819202122232425# setting.pyDATABASES = { 'default': { 'ENGINE': 'djongo', 'ENFORCE_SCHEMA': True, 'LOGGING': { 'version': 1, 'loggers': { 'djongo': { 'level': 'DEBUG', 'propogate': False, } }, }, 'NAME': 'your-db-name', 'CLIENT': { 'host': 'host-name or ip address', 'port': port_number, 'username': 'db-username', 'password': 'password', 'authSource': 'db-name', 'authMechanism': 'SCRAM-SHA-1' } }} 如果是本地安装 MongoDB 数据的话，可以不用配置这么多参数。 1234567891011# setting.pyDATABASES = { 'default': { 'ENGINE': 'djongo', 'ENFORCE_SCHEMA': True, 'NAME': '数据库名称', 'CLIENT': { 'host': '127.0.0.1', } }} MongoDB 数据库默认访问端口是 27017。如果你修改访问端口，需要显示指定端口号。用户名和密码也不需要填写，MongoDB 默认没有开启用户验证。如果你想开启用户校验或者线上数据库，需要在数据库安装目录下，找到 mongod.cfg 文件，然后开启登录校验。 123# mongod.cfg 文件中找到以下字段#security:authorization: enabled 配置好文件之后需要重启数据库，后面链接数据库就需要账号和密码了。","link":"/4116.html"},{"title":"不懂代码也能爬取数据？试试这几个工具","text":"前天，有个同学加我微信来咨询我：“猴哥，我想抓取近期 5000 条新闻数据，但我是文科生，不会写代码，请问该怎么办？” 猴哥有问必答，对于这位同学的问题，我给安排上。 先说说获取数据的方式：一是利用现成的工具，我们只需懂得如何使用工具就能获取数据，不需要关心工具是怎么实现。打个比方，假如我们在岸上，要去海上某个小岛，岸边有一艘船，我们第一想法是选择坐船过去，而不会想着自己来造一艘船再过去。第二种是自己针对场景需求做些定制化工具，这就需要有点编程基础。举个例子，我们还是要到海上某个小岛，同时还要求在 30 分钟内将 1 顿货物送到岛上。 因此，前期只是单纯想获取数据，没有什么其他要求的话，优先选择现有工具。可能是 Python 近来年很火，加上我们会经常看到别人用 Python 来制作网络爬虫抓取数据。从而有一些同学有这样的误区，想从网络上抓取数据就一定要学 Python，一定要去写代码。 其实不然，猴哥介绍几个能快速获取网上数据的工具。 Microsoft Excel你没有看错，就是 Office 三剑客之一的 Excel。Excel 是一个强大的工具，能抓取数据就是它的功能之一。我以耳机作为关键字，抓取京东的商品列表。 等待几秒后，Excel 会将页面上所有的文字信息抓取到表格中。这种方式确实能抓取到数据，但也会引入一些我们不需要的数据。如果你有更高的需求，可以选择后面几个工具。 火车头采集器 火车头是爬虫界的老品牌了，是目前使用人数最多的互联网数据抓取、处理、分析，挖掘软件。它的优势是采集不限网页，不限内容，同时还是分布式采集，效率会高一些。缺点是对小白用户不是很友好，有一定的知识门槛（了解如网页知识、HTTP 协议等方面知识），还需要花些时间熟悉工具操作。 因为有学习门槛，掌握该工具之后，采集数据上限会很高。有时间和精力的同学可以去折腾折腾。 官网地址：http://www.locoy.com/ 八爪鱼采集器 八爪鱼采集器是一款非常适合新手的采集器。它具有简单易用的特点，让你能几分钟中就快手上手。八爪鱼提供一些常见抓取网站的模板，使用模板就能快速抓取数据。如果想抓取没有模板的网站，官网也提供非常详细的图文教程和视频教程。 八爪鱼是基于浏览器内核实现可视化抓取数据，所以存在卡顿、采集数据慢的特点。但这瑕不掩瑜，能基本满足新手在短时间抓取数据的场景，比如翻页查询，Ajax 动态加载数据等。 网站：https://www.bazhuayu.com/ GooSeeker 集搜客 集搜客也是一款容易上手的可视化采集数据工具。同样能抓取动态网页，也支持可以抓取手机网站上的数据，还支持抓取在指数图表上悬浮显示的数据。集搜客是以浏览器插件形式抓取数据。虽然具有前面所述的有点，但缺点也有，无法多线程采集数据，出现浏览器卡顿也在所难免。 网站：https://www.gooseeker.com/ Scrapinghub 如果你想抓取国外的网站数据，可以考虑 Scrapinghub。Scrapinghub 是一个基于Python 的 Scrapy 框架的云爬虫平台。Scrapehub 算是市场上非常复杂和强大的网络抓取平台，提供数据抓取的解决方案商。 地址：https://scrapinghub.com/ WebScraper WebScraper 是一款优秀国外的浏览器插件。同样也是一款适合新手抓取数据的可视化工具。我们通过简单设置一些抓取规则，剩下的就交给浏览器去工作。 地址：https://webscraper.io/","link":"/4117.html"},{"title":"170 行代码爬取《白蛇：缘起》短评数据","text":"在我的童年记忆中，电视台播放的动画片大多都是从日本、美国引进的。很多动画片算是银幕上的经典，例如：《变形金刚》系列、《猛兽侠》、《蜘蛛侠》、《七龙珠》、《名侦探柯南》、《灌篮高手》、《数码宝贝》等。 但是国产的精品动画篇确认寥寥无几，可能是当时我国动漫产业还处在起步阶段。一晃几十年过去了，现在的国产动漫算是强势崛起，这也涌现出《斗破苍穹》、《秦时明月》、《天行九歌》等优秀的动画片。 2019年1月11日，一部国产动画电影《白蛇：缘起》在全国热映，一经上映便是好评如潮。这部电影凭借惊艳的花屏，出色的配音取得猫眼 9.4 分、豆瓣 8.0 分的高分成绩。 既然是难得一见的精品，那么我去猫眼上爬爬网友的短评，看看网友们的观点。 分析页面估计很多人经常光顾猫眼电影网，猫眼的反爬机制越来越严格，手段也越来越多。如果选择“刚正面”，爬取 PC 端的页面，可能总体收益不高。况且，PC 端的页面只有精彩短评，没有全部的网页评论数据。 因此，我选择转移战场，从手机页面入手，看看是否有收获。将浏览器选择以手机模式浏览器，结果发现手机网页有全部的短评数据。点击“查看全部讨论”，继续抓包分析。 随着鼠标滚动，自己浏览过几页短评数据之后，最终找到规律。 页面请求的地址是： 1http://m.maoyan.com/review/v2/comments.json? 后面携带一些参数： 123456movieId=1235560&amp; #电影iduserId=-1&amp; #默认用户idoffset=0&amp; #分片页数limit=15&amp; #每个分片显示具体数值ts=0&amp; #当前时间type=3 然后 offset 的值以 15 间隔递增。 我格式化请求结果，确认能获取到短评内容、点赞数等。但是这个无法满足我的需求，我是想获取城市信息，后面想绘制地理热力图。而目前这个接口没有城市信息。 因此，我选择逛逛各大搜索引擎，试下碰碰运气。最后幸运女神帮了我一把，我找到别人已经挖掘到的猫眼短评接口。 1http://m.maoyan.com/mmdb/comments/movie/1235560.json?_v_=yes&amp;offset=1 其中 1235560 表示电影的 id，offset 代表页数。 爬虫制作因为短评数据量可能会比较多，所以我选择用数据库来存储数据。后面方便进行数据导出、数据去重等。 自己从 json 数据结果中提取想要的数据，然后设计数据表并创建。 1234567891011121314151617181920212223242526def create_database(self): create_table_sql = ( \"CREATE TABLE IF NOT EXISTS {} (\" \"`id` VARCHAR(12) NOT NULL,\" \"`nickName` VARCHAR(30),\" \"`userId` VARCHAR(12),\" \"`userLevel` INT(3),\" \"`cityName` VARCHAR(10),\" \"`gender` tinyint(1),\" \"`score` FLOAT(2,1),\" \"`startTime` VARCHAR(30),\" \"`filmView` BOOLEAN,\" \"`supportComment` BOOLEAN,\" \"`supportLike` BOOLEAN,\" \"`sureViewed` INT(2),\" \"`avatarurl` VARCHAR(200),\" \"`content` TEXT\" \") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\".format(self.__table) ) try: self.cursor.execute(create_table_sql) self.conn.commit() except Exception as e: self.close_connection() print(e) 然后构造 Session 会话、请求头 headers 和 url 地址。 123456789101112131415\"\"\" 构造会话 Session 抓取短评 \"\"\"session = requests.Session()# 电影短评地址movie_url = 'http://m.maoyan.com/mmdb/comments/movie/1235560.json?_v_=yes&amp;offset={}'headers = { 'User-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Mobile Safari/537.36', 'Accept-Encoding': 'gzip, deflate', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Host': 'm.maoyan.com', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Cache-Control': 'max-age=0', 'Connection': 'keep-alive', 'Upgrade-Insecure-Requests': '1',} 接着请求 url 地址并解析返回的 Json 数据。 1234567891011121314151617181920212223242526272829303132333435offset = 1while 1: print('============抓取第', offset, '页短评============') print('============&gt;&gt;&gt;', movie_url.format(offset)) response = session.get(movie_url.format(offset), headers=headers) if response.status_code == 200: \"\"\" 解析短评 \"\"\" data_list = [] data = {} for comment in json.loads(response.text)['cmts']: data['id'] = comment.get('id') data['nickName'] = comment.get('nickName') data['userId'] = comment.get('userId') data['userLevel'] = comment.get('userLevel') data['cityName'] = comment.get('cityName') data['gender'] = comment.get('gender') data['score'] = comment.get('score') data['startTime'] = comment.get('startTime') data['filmView'] = comment.get('filmView') data['supportComment'] = comment.get('supportComment') data['supportLike'] = comment.get('supportLike') data['sureViewed'] = comment.get('sureViewed') data['avatarurl'] = comment.get('avatarurl') data['content'] = comment.get('content') print(data) data_list.append(data) data = {} print('============解析到', len(data_list), '条短评数据============') self.insert_comments(data_list) else: # 抓取失败就先暂停抓取, 标记抓取页数, 过段时间再抓取 print('&gt;=== 抓取第 ', offset, ' 失败, 错误码为 ' + response.status_code) break offset += 1 time.sleep(random.randint(10, 20)) 完成解析数据工作之后，最后一步工作就是将数据插入到数据库中。 1234567891011121314151617181920212223242526272829303132def insert_comments(self, datalist): \"\"\" 往数据库表中插入数据 \"\"\" insert_sql = ( \"insert into \" \"{} (id, nickName, userId, userLevel, cityName, gender, score, \" \"startTime, filmView, supportComment, supportLike, sureViewed, avatarurl, content)\" \"values(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\".format(self.__table)) try: templist = [] for comment in datalist: if comment.get('gender') is None: comment['gender'] = -1 data = (comment.get('id'), comment.get('nickName'), comment.get('userId'), comment.get('userLevel'), comment.get('cityName'), comment.get('gender'), comment.get('score'), comment.get('startTime'), comment.get('filmView'), comment.get('supportComment'), comment.get('supportLike'), comment.get('sureViewed'), comment.get('avatarurl'), comment.get('content')) templist.append(data) self.cursor.executemany(insert_sql, templist) self.conn.commit() except Exception as e: print('===== insert exception --&gt;&gt;&gt; %s', e) 我因控制爬虫速率，暂时还没有完成爬取工作。至于爬取结果，详情见下篇文章关于电影短评的数据分析。 源码如果你想获取项目的源码, 点击☞☞☞Github 仓库地址","link":"/195.html"},{"title":"技术人如何突破能力边界？","text":"近来一段时间，我在跟一些朋友交流过程中，又接触到新的领域，又学到新的知识。 于是乎，我每天花大量时间恶补这方面的知识，忙到忘记我还有一个公众号。 正好趁着这次回归机会，谈谈技术人如何突破自己的能力边界。在职场上，一个人所具备的能力可以分为两种，一种是通用技能，另一种是专业技能。 通用技能通用技能是不区分岗位，普遍人都具备的能力 沟通能力工作是讲究协同合作，这离不开跟别人打交道。比如向领导汇报工作，跟同事一起讨论技术方案、跟客户合作等，那么沟通能力就显得格外重要。沟通能力不仅是表达自己想法，而且聆听别人的意见和看法。 听就安安静静地听，不轻易打断别人，等别人说完再发言。说，其实是让别人跟着自己思路走，核心是自己条理清晰。沟通能力归根到底是跟人打交道，想提高可以学习一些沟通技巧。 独力思考能力你身边一定有这样的人，他们学习时习惯被动接受知识；他们遇到问题了，求助于周围人和网络，觉得都有道理却很难取舍，如果得到的几个建议有冲突就更迷茫了。 你身边也一定有这样的人：他们看问题总有自己的见解，并且独到深刻； 这两种人的差距是独立思考能力。 分享自己一个练习独立思考的方法：反向思考。 举个例子，我们每天工作，从正面角度去思考是为了拿到一份薪水来生存下去，为下一次跳槽做准备，为以后自己发展积累资源。 那么反向思考，我们为什么要工作？不工作的话，我们拿什么养活自己？自己手头上有什么资源？现在有什么本领？…… 等后面总结，我们会发现自己已经从不同角度、方式来思考问题了 。 专业技能专业技能可以理解为谋生的技能。比如，我们是 Python 工程师，那 Python 就是我们的专业技能。专业技能得到掌握程度是直接薪水上。同时，它也是我们再找工作时，决定岗位要求的匹配程度。想提交专业技能需要花大量时间、精力去学习。 突破能力边界做技术的，大多数的目标应该是进入大厂。 这不香吗？确实香，客观的薪水、优厚的福利、跟一群优秀的人共同做事。 岗位就那么多，总有些人会与其擦肩而过。 其实，技术不是全部。 如果自己懂技术，再补充产品，市场营销等知识，会有新的发现。 如何突破？ 1、加入一些其他领域的圈子，跟优秀的人交流，目的是扩宽自己的思维方式。 2、在工作中，可以去承担一些自己想努力方式的额外工作，跟着别人去学。平时，我除了完成工作之外，因对产品方向感兴趣，主动去承担一些产品工作。我个人觉得产品经理岗位在公司或者团队中，对产品发展起到决定性的作用。","link":"/5118.html"},{"title":"发展副业的正确路线","text":"近两年来，有个很流行的名词，有个叫“斜杠青年”。斜杠青年指年轻人们，不满足于本职工作所带来的收入，利用业余时间搞副业来获取收入。 他们变换不同的职业身份，用不同的职业技能来赚钱。有的平时是普通的程序员，晚上以及周末则是靠写作赚钱。 本文是自己对于副业的思考。先要理解两个概念，爱好和副业。副业不等于爱好，但副业可以由爱好发展而来。副业是指除了在单位上班获得薪水之外，还能通过自己其他途径获得收入。 重要的说三遍:副业是能带来收入的~副业是能带来收入的~副业是能带来收入的~ 现在流行知识付费，如果你有主动思考的话，我们会发现知识付费本质上是这样。很多人把自己长期积累的知识、归纳总结出来的东西进行包装，然后进行售卖。 回到本次主题内容，想利用业余时间去副业，怎么做？ 1.对于少数人来说，可以利用自己早期优势。如果你从小有学习一门乐器(钢琴、吉他等)或者舞蹈(街舞、拉丁舞、交际舞等)，那么你可以利用业余时间去教别人获取收入。 2.对于大多数人来说，可能没有小时候的积累，也别慌别气馁，那就从现在开始积累。先把本质工作做好，做好极致。这既简单又困难。简单是有付出就有收获。难就难在坚持，坚持去研究，坚持去思考。这也可能会占用自己的业余时间。当你积累到一定量时，就可以考虑产出，利用自己的积累去帮助别人。因为每个行业都是有后继者，如果我们把这个行业，这个领域的内容都了解足够深。那我们的优势就体现出来了，我们可以利用这些优势去帮助后继者。 说说我自己的经验，我公众号一开始是写 Python 网络爬虫和 Web 开发。这些技能都是我在大学的时候已经研究过。两年前流行公众号，我也跟着潮流开通公众号，一开始不知道输出什么原创内容。我对自己做个全方位的了解。发现自己对爬虫和 Web 开发比较熟悉，所以就输出这方面的内容。 然后我因公众接触到产品、项目管理等知识。我也想将在公众号输出这方面的内容。发现太难了，一篇文章写几个小时也不一定写完。而以前写技术文章一个小时内就搞定，还能把文章给排版好。原因产品领域对我来说是一个知识盲区，我自己需要花时间积累，才能有所产生。 没有输入，哪来的输出？","link":"/8104.html"},{"title":"用数据告诉你，哪位导演是漫威影片中的票房收割机？","text":"3 月 8 号是国际劳动妇女节，漫威在这天“搞事”了。上映《惊奇队长》，这部是漫威电影宇宙的第一部女性超级英雄电影，也算是给妇女节献礼。身为漫威粉丝的我，当时不会错过这部电影，趁着周末去观影。 现如今，漫威电影成为一种潮流文化，各系列电影基本是很卖座，导演的作用是功不可没。本文主要通过数据来分析，哪位导演是漫威电影的票房收割机。 回首十年2008 年，濒临破产的漫威，砸锅卖铁地向美林证券申请 5 亿美金的贷款，准备将重生希望寄托于电影事业。 漫威转型成为独立电影制片公司后，推出第一部电影《钢铁侠》，虽然主演是三线污点演员的唐尼，不到 2 亿美金的制作成本，但确在全球狂揽 5.8 亿票房。算是一部很成功的商业电影，这不仅为漫威续命，而且也为漫威 10 年计划打个响炮。 经过 10 年发展，漫威从一个小小的工作室逐渐发展为拥有多个超级 IP 的影视帝国。让我们用一张图来回顾漫威近 10 年的上映的 20 部电影。 数据收集电影的票房、导演、上映时间等信息都能在一些网站找到。我通过豆瓣、谷歌、IMDb等网站收集到漫威目前 20 部电影的信息，经过详细的整合匹对，整理一个数据表格。 数据分析究竟谁才是票房收割机，当然需要一个评定标准。考虑到有些导演会指导多部电影，如下图所示。其中排行榜首的罗素兄弟（乔·罗素与安东尼·罗素的合称）一共指导三部电影。 我指定的标准是：1）看单部影片的收益比，即看看谁导演的影片制作成本最少，票房收益最多。2）看谁指导电影的总票房最高，如果有多部电影则看平均票房。 先根据收益比计算公式，投资收益率＝投资收益／投资成本×100％，计算出收益比；然后将数据生成对应的图表。 单部电影收益比： 看完图表的第一印象是：我的天！拍出一部好电影收益真的是惊人。从单部电影收益 看，影片排名前三甲的导演是：罗素兄弟、乔斯·惠登、瑞恩·库格勒。 影片制作成本/票房： 罗素兄弟指导的《复仇者联盟3》票房很带劲，看似他们像是票房收割机。 影片总票房（平均）： 但是从总票房来看，前三名分别是：乔斯·惠登、瑞恩·库格勒、罗素兄弟，罗素兄弟排到探花。 真的是难以抉择，不得不说。这几位导演乔斯·惠登、瑞恩·库格勒、罗素兄弟，都是拍摄电影的好手，能指导出成功的商业片。 如果硬要要评选出最强票房收割期，那我只能算是电影的评分。我根据豆瓣上各部电影的评分，计算出各位导演拍摄电影的平均得分，最后生成柱形图。 拍摄影片口碑排行前三名是詹姆斯·古恩、罗素兄弟、乔恩·法夫罗。 最后综合对比三个条件（电影口碑好，收益高，票房高）可知，罗素兄弟是票房收割机。","link":"/399.html"}],"tags":[{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"HTTP","slug":"HTTP","link":"/tags/HTTP/"},{"name":"思念","slug":"思念","link":"/tags/%E6%80%9D%E5%BF%B5/"},{"name":"网络爬虫","slug":"网络爬虫","link":"/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"name":"beautifulSoup","slug":"beautifulSoup","link":"/tags/beautifulSoup/"},{"name":"正则表达式","slug":"正则表达式","link":"/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"爬虫实战","slug":"爬虫实战","link":"/tags/%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98/"},{"name":"项目实战","slug":"项目实战","link":"/tags/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"},{"name":"当当","slug":"当当","link":"/tags/%E5%BD%93%E5%BD%93/"},{"name":"鸡汤","slug":"鸡汤","link":"/tags/%E9%B8%A1%E6%B1%A4/"},{"name":"Requests","slug":"Requests","link":"/tags/Requests/"},{"name":"Xpath","slug":"Xpath","link":"/tags/Xpath/"},{"name":"lxml","slug":"lxml","link":"/tags/lxml/"},{"name":"阅读","slug":"阅读","link":"/tags/%E9%98%85%E8%AF%BB/"},{"name":"方法论","slug":"方法论","link":"/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"},{"name":"认知","slug":"认知","link":"/tags/%E8%AE%A4%E7%9F%A5/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Scrapy","slug":"Scrapy","link":"/tags/Scrapy/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"多进程","slug":"多进程","link":"/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"},{"name":"电影","slug":"电影","link":"/tags/%E7%94%B5%E5%BD%B1/"},{"name":"反爬虫","slug":"反爬虫","link":"/tags/%E5%8F%8D%E7%88%AC%E8%99%AB/"},{"name":"数据分析","slug":"数据分析","link":"/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"generator","slug":"generator","link":"/tags/generator/"},{"name":"Iterator","slug":"Iterator","link":"/tags/Iterator/"},{"name":"Iterable","slug":"Iterable","link":"/tags/Iterable/"},{"name":"OCR","slug":"OCR","link":"/tags/OCR/"},{"name":"验证码","slug":"验证码","link":"/tags/%E9%AA%8C%E8%AF%81%E7%A0%81/"},{"name":"定时任务","slug":"定时任务","link":"/tags/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/"},{"name":"APScheduler","slug":"APScheduler","link":"/tags/APScheduler/"},{"name":"sched","slug":"sched","link":"/tags/sched/"},{"name":"pustil","slug":"pustil","link":"/tags/pustil/"},{"name":"IP代理池","slug":"IP代理池","link":"/tags/IP%E4%BB%A3%E7%90%86%E6%B1%A0/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"Django","slug":"Django","link":"/tags/Django/"},{"name":"技巧","slug":"技巧","link":"/tags/%E6%8A%80%E5%B7%A7/"},{"name":"datetime","slug":"datetime","link":"/tags/datetime/"},{"name":"time","slug":"time","link":"/tags/time/"},{"name":"社工库","slug":"社工库","link":"/tags/%E7%A4%BE%E5%B7%A5%E5%BA%93/"},{"name":"自学","slug":"自学","link":"/tags/%E8%87%AA%E5%AD%A6/"},{"name":"编码规范","slug":"编码规范","link":"/tags/%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83/"},{"name":"PEP","slug":"PEP","link":"/tags/PEP/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"后台","slug":"后台","link":"/tags/%E5%90%8E%E5%8F%B0/"},{"name":"session","slug":"session","link":"/tags/session/"},{"name":"豆瓣","slug":"豆瓣","link":"/tags/%E8%B1%86%E7%93%A3/"},{"name":"Mysql","slug":"Mysql","link":"/tags/Mysql/"},{"name":"MangoDB","slug":"MangoDB","link":"/tags/MangoDB/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"操作系统","slug":"操作系统","link":"/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/tags/Ubuntu/"},{"name":"Debian","slug":"Debian","link":"/tags/Debian/"},{"name":"Unicode","slug":"Unicode","link":"/tags/Unicode/"},{"name":"csv","slug":"csv","link":"/tags/csv/"},{"name":"codecs","slug":"codecs","link":"/tags/codecs/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"Selenium","slug":"Selenium","link":"/tags/Selenium/"},{"name":"网易云音乐","slug":"网易云音乐","link":"/tags/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90/"},{"name":"Nosqlclient","slug":"Nosqlclient","link":"/tags/Nosqlclient/"},{"name":"Mongo Plugin","slug":"Mongo-Plugin","link":"/tags/Mongo-Plugin/"},{"name":"unsplash","slug":"unsplash","link":"/tags/unsplash/"},{"name":"os.path","slug":"os-path","link":"/tags/os-path/"},{"name":"标准库","slug":"标准库","link":"/tags/%E6%A0%87%E5%87%86%E5%BA%93/"},{"name":"文本操作","slug":"文本操作","link":"/tags/%E6%96%87%E6%9C%AC%E6%93%8D%E4%BD%9C/"},{"name":"抓包","slug":"抓包","link":"/tags/%E6%8A%93%E5%8C%85/"},{"name":"抓包工具","slug":"抓包工具","link":"/tags/%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7/"},{"name":"编程入门","slug":"编程入门","link":"/tags/%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/"},{"name":"开发进阶","slug":"开发进阶","link":"/tags/%E5%BC%80%E5%8F%91%E8%BF%9B%E9%98%B6/"},{"name":"书籍","slug":"书籍","link":"/tags/%E4%B9%A6%E7%B1%8D/"},{"name":"aiohttp","slug":"aiohttp","link":"/tags/aiohttp/"},{"name":"二维码","slug":"二维码","link":"/tags/%E4%BA%8C%E7%BB%B4%E7%A0%81/"},{"name":"七夕","slug":"七夕","link":"/tags/%E4%B8%83%E5%A4%95/"},{"name":"面试","slug":"面试","link":"/tags/%E9%9D%A2%E8%AF%95/"},{"name":"代理","slug":"代理","link":"/tags/%E4%BB%A3%E7%90%86/"},{"name":"高并发","slug":"高并发","link":"/tags/%E9%AB%98%E5%B9%B6%E5%8F%91/"},{"name":"后台架构","slug":"后台架构","link":"/tags/%E5%90%8E%E5%8F%B0%E6%9E%B6%E6%9E%84/"},{"name":"中间键","slug":"中间键","link":"/tags/%E4%B8%AD%E9%97%B4%E9%94%AE/"},{"name":"总结","slug":"总结","link":"/tags/%E6%80%BB%E7%BB%93/"},{"name":"IG","slug":"IG","link":"/tags/IG/"},{"name":"产品","slug":"产品","link":"/tags/%E4%BA%A7%E5%93%81/"},{"name":"竞品分析","slug":"竞品分析","link":"/tags/%E7%AB%9E%E5%93%81%E5%88%86%E6%9E%90/"},{"name":"懂点系列","slug":"懂点系列","link":"/tags/%E6%87%82%E7%82%B9%E7%B3%BB%E5%88%97/"},{"name":"TCP","slug":"TCP","link":"/tags/TCP/"},{"name":"文件遍历","slug":"文件遍历","link":"/tags/%E6%96%87%E4%BB%B6%E9%81%8D%E5%8E%86/"},{"name":"数据集","slug":"数据集","link":"/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/"},{"name":"必胜客","slug":"必胜客","link":"/tags/%E5%BF%85%E8%83%9C%E5%AE%A2/"},{"name":"Chrome","slug":"Chrome","link":"/tags/Chrome/"},{"name":"pyecharts","slug":"pyecharts","link":"/tags/pyecharts/"},{"name":"Flask","slug":"Flask","link":"/tags/Flask/"},{"name":"web","slug":"web","link":"/tags/web/"},{"name":"源码","slug":"源码","link":"/tags/%E6%BA%90%E7%A0%81/"},{"name":"Github","slug":"Github","link":"/tags/Github/"},{"name":"性能优化","slug":"性能优化","link":"/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"账号认证","slug":"账号认证","link":"/tags/%E8%B4%A6%E5%8F%B7%E8%AE%A4%E8%AF%81/"},{"name":"Request","slug":"Request","link":"/tags/Request/"},{"name":"Session","slug":"Session","link":"/tags/Session/"},{"name":"Token","slug":"Token","link":"/tags/Token/"},{"name":"OAuth","slug":"OAuth","link":"/tags/OAuth/"},{"name":"the fuck","slug":"the-fuck","link":"/tags/the-fuck/"},{"name":"年总结","slug":"年总结","link":"/tags/%E5%B9%B4%E6%80%BB%E7%BB%93/"},{"name":"开源项目","slug":"开源项目","link":"/tags/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/"},{"name":"图表","slug":"图表","link":"/tags/%E5%9B%BE%E8%A1%A8/"},{"name":"拖延症","slug":"拖延症","link":"/tags/%E6%8B%96%E5%BB%B6%E7%97%87/"},{"name":"Vlog","slug":"Vlog","link":"/tags/Vlog/"},{"name":"短视频","slug":"短视频","link":"/tags/%E7%9F%AD%E8%A7%86%E9%A2%91/"},{"name":"商业","slug":"商业","link":"/tags/%E5%95%86%E4%B8%9A/"},{"name":"感悟","slug":"感悟","link":"/tags/%E6%84%9F%E6%82%9F/"},{"name":"时间管理","slug":"时间管理","link":"/tags/%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/"},{"name":"Pycharm","slug":"Pycharm","link":"/tags/Pycharm/"},{"name":"数组","slug":"数组","link":"/tags/%E6%95%B0%E7%BB%84/"},{"name":"博客","slug":"博客","link":"/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Jekyll","slug":"Jekyll","link":"/tags/Jekyll/"},{"name":"WordPress","slug":"WordPress","link":"/tags/WordPress/"},{"name":"SEO 优化","slug":"SEO-优化","link":"/tags/SEO-%E4%BC%98%E5%8C%96/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"可视化","slug":"可视化","link":"/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"Djongo","slug":"Djongo","link":"/tags/Djongo/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"数据采集器","slug":"数据采集器","link":"/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%99%A8/"},{"name":"白蛇缘起","slug":"白蛇缘起","link":"/tags/%E7%99%BD%E8%9B%87%E7%BC%98%E8%B5%B7/"},{"name":"副业","slug":"副业","link":"/tags/%E5%89%AF%E4%B8%9A/"},{"name":"漫威","slug":"漫威","link":"/tags/%E6%BC%AB%E5%A8%81/"}],"categories":[{"name":"Python 必知必会","slug":"Python-必知必会","link":"/categories/Python-%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/"},{"name":"计算机网络","slug":"计算机网络","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"网络爬虫","slug":"网络爬虫","link":"/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"},{"name":"思维与认知","slug":"思维与认知","link":"/categories/%E6%80%9D%E7%BB%B4%E4%B8%8E%E8%AE%A4%E7%9F%A5/"},{"name":"数据分析","slug":"数据分析","link":"/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"Django 学习笔记","slug":"Django-学习笔记","link":"/categories/Django-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"建站与优化","slug":"建站与优化","link":"/categories/%E5%BB%BA%E7%AB%99%E4%B8%8E%E4%BC%98%E5%8C%96/"},{"name":"认知","slug":"认知","link":"/categories/%E8%AE%A4%E7%9F%A5/"}]}