{"pages":[],"posts":[{"title":"每逢佳节倍思亲","text":"回忆童年每逢冬至来临外面寒风凛凛屋内热气腾腾全家人围在一起吃汤圆团团圆圆 如今身为游子在外漂流更思念家更思念父母亲只能发QQ说说、微博、朋友圈给远方家人送上祝福？不！没有比给家人打一电话来得更加直接来的更加亲切拿起你手中的手机给家里打一通电话为爸妈送上节日祝福！祝：冬至快乐~","link":"/124.html"},{"title":"深入理解HTTP","text":"HTTP是什么 HTTP全称是HyperText Transfer Protocal，即：超文本传输协议。它主要规定了客户端和服务器之间的通信格式。HTTP还是一个基于请求/响应模式的、无状态的协议；即我们通常所说的Request/Response。 HTTP与TCP的关系TCP协议是位于TCP/IP参考模型中的网络互连层，而HTTP协议属于应用层。因此，HTTP协议是基于TCP协议。 HTTP请求(HTTP Request)HTTP请求由三部分组成，分别是： 请求行 HTTP头 请求体 下面是请求示例： 123456789GET /?tn=90058352_hao_pg HTTP/1.1Host: www.hao123.comConnection: keep-aliveCache-Control: max-age=0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8Upgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/5.7.16400.16 Safari/537.36Accept-Encoding: gzip, deflateAccept-Language: zh-CN,zh;q=0.8 请求行同样也是由请求方法（POST/GET）方式、请求的主机、协议版本号三部分组成。下面为请求行的示例：GET /?tn=90058352_hao_pg HTTP/1.1 HTTP头HTTP头又细分为请求头(request header)、普通头(general header)、实体头(entity header)而HTTP头主要关注点是其字段 Accept作用: 浏览器可以接受的媒体类型例如： Accept: text/html 代表浏览器可以接受服务器回发的类型为 text/html 也就是我们常说的html文档通配符 * 代表任意类型例如： Accept: */* 代表浏览器可以处理所有类型,(一般浏览器发给服务器都是发这个) Accept-Language作用： 浏览器申明自己接收的语言。语言跟字符集的区别：中文是语言，中文有多种字符集，比如big5，gb2312，gbk等等；例如： Accept-Language: zh-CN,zh Accept-Encoding作用： 浏览器申明自己接收的编码方法，通常指定压缩方法（gzip，deflate）例如：Accept-Encoding: gzip, Accept-Encoding: deflate User-Agent作用： 告诉HTTP服务器， 客户端使用的操作系统的名称和版本以及浏览器的名称和版本.例如： User-Agent: Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 UBrowser/5.7.16400.16 Safari/537.36 Content-Type作用： 告诉服务器，请求的内容的类型常见的字段： 假设使用POST方式请求 text/xml [请求体为文本] application/json [请求体为JSON数据] application/xml [请求体为xml数据] image/jpeg [请求体为jpeg图片] multipart/form-data [请求体为表单] Cookie作用： 最重要的header，将cookie的值发送给HTTP服务器 Connection例如： Connection: keep-alive 当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接例如： Connection: close 代表一个Request完成后，客户端和服务器之间用于传输HTTP数据的TCP连接会关闭， 当客户端再次发送Request，需要重新建立TCP连接。 Content-Length作用：发送给HTTP服务器数据的长度。例如： Content-Length: 18 Referer:作用： 提供了Request的上下文信息的服务器，告诉服务器我是从哪个链接过来的。 请求体这个只有post方式请求才有，get方式请求没有。 HTTP响应(HTTP Response)HTTP Response的结构跟Request的结构基本一样。同样分为三部分： 响应行 响应头 响应体 下面是响应示例： 1234567891011HTTP/1.1 200 OKCache-Control: max-age=0Content-Encoding: gzipContent-Length: 156474Content-Type: text/html;charset=UTF-8Cxy_all: 90058352_hao_pg+d4fa7f28cefb9b120f868558e440bafaDate: Sun, 20 Nov 2016 05:09:51 GMTExpires: Sun, 20 Nov 2016 05:09:51 GMTLfy: nj02.11Server: BWS/1.0Set-Cookie: __bsi=11619936655404239050_00_60_N_R_126_0303_c02f_Y; max-age=3600; domain=www.hao123.com; path=/ 响应行响应行由协议版本、响应状态构成下面为响应行的示例：HTTP/1.1 200 OK 响应头响应头关注点是字段，常见的字段如下： Cache-Control作用: 非常重要的规则。 这个用来指定Response-Request遵循的缓存机制。例如：Cache-Control:Public 可以被任何缓存所缓存Cache-Control:Private 内容只缓存到私有缓存中Cache-Control:no-cache 所有内容都不会被缓存 Content-Type作用：服务器告诉浏览器，自己响应的对象的类型和字符集例如:Content-Type: text/html; charset=utf-8Content-Type: image/jpeg Expires作用: 浏览器会在指定过期时间内使用本地缓存例如: Expires:Sun, 20 Nov 2016 05:09:51 GMT Connection跟HTTP头中的Connection是同样的原理 Content-Encoding跟HTTP中头的Content-Encoding是同样的原理 Content-Length作用：指明实体正文的长度，以字节方式存储的十进制数字来表示。例如: Content-Length: 156474 Date作用: 生成消息的具体时间和日期例如: Date: Sun, 20 Nov 2016 05:09:51 GMT 响应体响应体包含的内容是网页的内容信息，主要是html代码等","link":"/111.html"},{"title":"常用Python标准库","text":"众所周知，Python有庞大的库资源，有官方标准库以及第三方的扩展库。每个库都一把利器，能帮助我们快速处理某方面的问题。作为一名python的初学者，当把基本的语法、列表和元组、字典、迭代器、异常处理、I/O操作、抽象等知识点学完之后。我建议把官方常用的标准库也随便学下来。讲真的，你知道这些库之后，你会有种相见恨晚的感觉。 接下来带大家走进python标准库的世界。PS： 使用Python的版本为Python3 字符串 re: 正则表达式。用来判断字符串是否是你指定的特定字符串。在爬虫项目中，经常能捕获到它的身影。 StringIO: 提供以文件为保存形式来读和写字符串。还有个性能更加好的cStringIO版本 struct: 以二进制字节序列来解释字符串。可以通过格式化参数，指定类型、长度、字节序（大小端）、内存对齐等。 数据类型 bisect: 数组二分算法。提供支持按顺序对列表进行排序，而不必每次在列表中插入后再去排序。 heapq: 堆队列算法。最小堆：完全平衡二叉树， 所有节点都小于字节点。 datetime: 提供操作日期和时间的类。其中有两种日期和时间类型： naive和aware collections: 高性能容器数据类型。实现了Python的通用内置容器、字典、列表、集合，和元组专门的数据类型提供替代品 pprint: 提供”整洁低打印”任意Python数据结构的能力。 数学运算 random: 各种分布的伪随机数的生成器 math: 数学函数。提供了由C标准的数学函数访问。该库的函数不适用于复数。 cmath: 为复数提供的数学函数。 operator: 提供了重载操作符 文件和目录 os.path: 常用路径名操作。提供了操作路径名的常用的函数。 filecmp: 文件和目录的比较。提供了比较文件和目录的函数。 shutil: 高级的文件操作。提供了许多文件和文件集上的操作操作。尤其是提供支持文件复制和删除的函数。 数据存储 serialization: Python专用的序列化算法，通常不建议用来存储自定义数据。 pickle: Python对象序列化。提供了一个基本但功能强大的Python对象序列化和反序列化算法。 cPickle: 比pickle快1000倍的对象序列化库， 和pickle可互相替换。 shevle: 将对象pickle序列化，然后保存到anydbm格式文件。anydbm是KV结构的数据库，可以保存多个序列化对象。 sqlite3: SQLite数据库DB-API 2.0接口。 数据压缩 zipfile: 提供了ZIP文件个创建、读取、写入、最佳和列出zip文件的函数。 tarfile: 提供了tar文件的压缩和解压的函数。 文件格式 csv: 提供对CSV文件的读取和写入的函数。 加密 hashlib: 安全哈希和消息摘要。实现了一个通用的接口来实现多个不同的安全哈希和消息摘要算法。包括 FIPS 安全哈希算法 SHA1、SHA224、SHA256、SHA384和 SHA512（定义在 FIPS 180-2），以及 RSA 的 MD5 算法（在互联网 RFC 1321中定义)。 hmac: 用于消息认证的加密哈希算法。实现了RFC 2104 中描述的HMAC 算法。 md5: 实现了MD5加密算法。 sha: 实现了sha1加密算法。 操作系统 time: 时间获取和转换。提供了各种与时间相关的函数。 argparse: 命令行选项、参数和子命令的解析器。使用该库使得编码用户友好的命令行接口非常容易。取代了之前的optparse io: 提供接口处理IO流。 logging: Python的日志工具。提供了日志记录的API。 logging.config: Python日志配置。用于配置日志模块的API。 os: 提供丰富的雨MAC，NT，Posix等操作系统进行交互的能力。这个模块允许程序独立的于操作系统环境。文件系统，用户数据库和权限进行交互。 _thread: 多线程控制。提供了一个底层、原始的操作 —— 多个控制线程共享全局数据空间。 threading: 高级线程接口。是基于_thread模块的，但是比_thread更加容易使用、更高层次的线程API。 sys: 提供访问和维护python解释器的能力。这包括了提示信息，版本，整数的最大值，可用模块，路径钩子，标准错误，标准输入输出的定位和解释器调用的命令行参数。 进程通信 subprocess: 管理子进程。允许用户产生新的进程，然后连接他们的输入/输出/错误/管道，并获取返回值。 socket: 底层网络接口。 signal: 设置异步时间处理handlers。信号是软中断，提供了一种异步事件通知机制。 网络数据处理 json: JSON格式的编码器和解码器。 base64: 提供依据RFC 3548的规定（Base16, Base32, Base64 ）进行数据编码和解码。 htmllib: 提供了一个HTML语法解析器。 mimetypes: 提供了判断给定的URL的MIME类型。 操作因特网网络协议 urllib: 提供了用于获取万维网数据的高层接口。这个是Python2.7版本的，Python3已经将其拆分成多个模块urllib.request，urllib.parse和urllib.error。 urlparse: 提供了用于处理URL的函数，可以在URL和平台特定的文件名间相互转换。 http.client: HTTP协议客户端。 telnetlib: 提供了实现Telnet协议的Telnet类。 poplib: POP3协议客户端。 ftplib: FTP协议客户端。 smtplib: SMTP协议客户端。 webbrowser: 提供控制浏览器行为的函数。","link":"/123.html"},{"title":"你为何要那么拼命？","text":"很多人往往有这样的状态，当完成一个目标之后，就守着这收获的成果沾沾自喜。你觉得考上了大学，就可以整天逃课沉迷于游戏？你觉得找到工作了，就可以准时下班走人，天天潇潇洒洒？答案是否定的。如果你是现在身处这状态，说明你对自己未来人生没有什么规划，是对自己极其不负责任的表示。你试问你自己，是否有在为自己拼命？ 01.何炅, 这个名字已经家喻户晓了。大家都知道他是大名鼎鼎的湖南电视台主持人。平时我们都在享受何老师给我们带来快乐，可知背后辛酸的汗水呢？何老师在读大学三年级时，每天需要应对高难度的阿拉伯语的学习，还担任着学生会的工作，兼职文艺部和宣传部的“要职”。除此之外，他还在央视担任支持，平日要撰写台本以及录影，有时还要出差去外地录制。每天他都很晚才回到学校，同学们可能已经下了晚自习，甚至都已经入睡了。而他只能先在学生会里将自己学生干部的事情都做完后，再回到宿舍开始预习第二天上课要准备的内容。 02.彭宇年轻的时还是街头一个小混混。当他树立人生中第一个梦想————进入电视台工作，生活从此跟之前是天壤之别。他时常对着电视机练习如何应对突发，还报名参加骗子的演员培训班。到了后来，他听说北京机会多，毅然决定北漂，每天在北京电影学院门口等着接活，做群众演员。 为何现在要拼命？只为自己，只为自己生活得更好，只为青春无悔。 你是否感到很震撼？看下你目前的生活状态，如果整天这么懒散。那么你该制定短期目标，并为之拼命一把。","link":"/15.html"},{"title":"学爬虫之道","text":"近来在阅读 《轻量级 Django》,虽然还没有读完，但我已经收益颇多。我不得不称赞 Django 框架的开发人员，他们把 Web 开发降低门槛。Django 让我从对 Web 开发是一无所知到现在可以编写小型 web 应用，这很舒服。 Django 已经算是入门，所以自己把学习目标转到爬虫。自己接下来会利用三个月的时间来专攻 Python 爬虫。这几天，我使用“主题阅读方法”阅读 Python 爬虫入门的文档。制定 Python 爬虫的学习路线。 第一阶段：夯实入门要就是在打基础，所以要从最基础的库学起。下面是几个库是入门最经典的库 urllib它属于 Python 标准库。该库的作用是请求网页并下载数据。在学习该库之前，最好把 HTTP 协议了解下。这会大大提高后面的学习效率。 先学会如何使用 urllib 请求到数据，再学习一些高级用法。例如： 设置 Headers: 某些网站反感爬虫的到访，于是对爬虫一律拒绝请求。设置 Headers 可以把请求伪装成浏览器访问网站。 Proxy 的设置: 某些站点做了反倒链的设置，会将高频繁访问的 IP 地址封掉。所以我们需要用到代理池。 错误解析：根据 URLError 与 HTTPError 返回的错误码进行解析。 Cookie 的使用：可以模拟网站登录，需要结合 cookielib 一起使用。 rere 是正则表达式库。同时也是 Python 标准库之一。它的作用是匹配我们需要爬取的内容。所以我们需要掌握正则表达式常用符号以及常用方法的用法。 BeautifulSoupBeautifulSoup 是解析网页的一款神器。它可以从 HTML 或者 XML 文件中提取数据。配合 urllib 可以编写出各种小巧精干的爬虫脚本。 第二阶段：进阶当把基础打牢固之后，我们需要更进一步学习。使用更加完善的库来提高爬取效率 使用多线程使用多线程抓取数据，提高爬取数据效率。 学习 RequestsRequests 作为 urlilb 的替代品。它是更加人性化、更加成熟的第三方库。使用 Requests 来处理各种类型的请求，重复抓取问题、cookies 跟随问题、多线程多进程、多节点抓取、抓取调度、资源压缩等一系列问题。 学习 XpathXpath 也算是一款神器。它是一款高效的、表达清晰简单的分析语言。掌握它以后介意弃用正则表达式了。一般是使用浏览器的开发者工具 加 lxml 库。 学习 Selenium使用 Selenium，模拟浏览器提交类似用户的操作，处理js动态产生的网页。因为一些网站的数据是动态加载的。类似这样的网站，当你使用鼠标往下滚动时，会自动加载新的网站。 第三阶段：突破学习 ScrapyScrapy 是一个功能非常强大的分布式爬虫框架。我们学会它，就可以不用重复造轮子。 数据存储如果爬取的数据条数较多，我们可以考虑将其存储到数据库中。因此，我们需要学会 MySqlMongoDB、SqlLite的用法。更加深入的，可以学习数据库的查询优化。 第四阶段：为我所用当爬虫完成工作，我们已经拿到数据。我们可以利用这些数据做数据分析、数据可视化、做创业项目原始启动数据等。我们可以学习 NumPy、Pandas、 Matplotlib 这三个库。 NumPy ：它是高性能科学计算和数据分析的基础包。 Pandas : 基于 NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。它可以算得上作弊工具。 Matplotlib：Python中最著名的绘图系统Python中最著名的绘图系统。它可以制作出散点图，折线图，条形图，直方图，饼状图，箱形图散点图，折线图，条形图，直方图，饼状图，箱形图等。","link":"/62.html"}],"tags":[{"name":"思念","slug":"思念","link":"/tags/%E6%80%9D%E5%BF%B5/"},{"name":"HTTP","slug":"HTTP","link":"/tags/HTTP/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"鸡汤","slug":"鸡汤","link":"/tags/%E9%B8%A1%E6%B1%A4/"},{"name":"网络爬虫","slug":"网络爬虫","link":"/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"}],"categories":[{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"计算机网络","slug":"计算机网络","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"Python必知必会","slug":"Python必知必会","link":"/categories/Python%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/"},{"name":"认知","slug":"认知","link":"/categories/%E8%AE%A4%E7%9F%A5/"},{"name":"网络爬虫系列","slug":"网络爬虫系列","link":"/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97/"}]}